{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQDnK-mVD52o",
        "outputId": "88da58ab-c117-4da6-e754-78abaffcd6ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/rDanielNutt/Qsim.git\n",
            "  Cloning https://github.com/rDanielNutt/Qsim.git to /tmp/pip-req-build-owfz63z8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/rDanielNutt/Qsim.git /tmp/pip-req-build-owfz63z8\n",
            "  Resolved https://github.com/rDanielNutt/Qsim.git to commit e26b6ba308cb853a0246d1663b8d0c53fcc9c662\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from qsim==0.0.1) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from qsim==0.0.1) (1.22.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->qsim==0.0.1) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->qsim==0.0.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->qsim==0.0.1) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->qsim==0.0.1) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->qsim==0.0.1) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->qsim==0.0.1) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->qsim==0.0.1) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->qsim==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->qsim==0.0.1) (1.16.0)\n",
            "Building wheels for collected packages: qsim\n",
            "  Building wheel for qsim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qsim: filename=qsim-0.0.1-py3-none-any.whl size=7092 sha256=e25167063e6bb4bc98ddefa2674f75e70f4c471cf776ca1b260809076c926681\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cwsj82gy/wheels/ae/e7/a5/494f414ec450b291077ca7696b8f9c1e649eb6d217434b880c\n",
            "Successfully built qsim\n",
            "Installing collected packages: qsim\n",
            "Successfully installed qsim-0.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting npy-append-array\n",
            "  Downloading npy_append_array-0.9.16-py3-none-any.whl (9.0 kB)\n",
            "Installing collected packages: npy-append-array\n",
            "Successfully installed npy-append-array-0.9.16\n"
          ]
        }
      ],
      "source": [
        "%pip install git+https://github.com/rDanielNutt/Qsim.git\n",
        "%pip install npy-append-array"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.random import randint\n",
        "import cupy as cp\n",
        "from cupyx.scipy.signal import fftconvolve\n",
        "from cupyx.scipy import fft\n",
        "from npy_append_array import NpyAppendArray as npy\n",
        "\n",
        "from qsim.models import SchroModel\n",
        "from qsim.schrosim import SchroSim\n",
        "\n",
        "import math\n",
        "import os"
      ],
      "metadata": {
        "id": "9YjMG2raEMId"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions"
      ],
      "metadata": {
        "id": "LiJeYgiIFmgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    e = cp.exp(x)\n",
        "    a = 1 / (1 + (1/e))\n",
        "    da = e / cp.square(e + 1)\n",
        "    return a, da\n",
        "\n",
        "def tanh(x):\n",
        "    a = cp.tanh(x)\n",
        "    da = 1 / cp.square(cp.cosh(x))\n",
        "    return a, da\n",
        "\n",
        "def softmax(x):\n",
        "    e = cp.exp(x)\n",
        "    esum = cp.sum(e, axis=1, keepdims=True)\n",
        "    a = e / esum\n",
        "    da = ((esum - e) * e) / cp.square(esum)\n",
        "    return a, da\n",
        "\n",
        "def relu(x):\n",
        "    da = cp.where(x > 0, 1, 0)\n",
        "    a = x * da\n",
        "    return a, da\n",
        "\n",
        "def comp_norm(x):\n",
        "    asum = cp.sqrt(cp.nansum(cp.square(cp.abs(x)), axis=(1,2,3), keepdims=True) * 1e-4)\n",
        "    a = x / asum\n",
        "    da = ((1+1j) / asum) - (((1e-4 + 1e-4j) * cp.square(x)) / cp.square(asum))\n",
        "    return a, da\n",
        "\n",
        "def none(x):\n",
        "    return x, 1\n",
        "\n",
        "activation_funcs = {\n",
        "    'sigmoid': sigmoid,\n",
        "    'tanh': tanh,\n",
        "    'softmax': softmax,\n",
        "    'relu': relu,\n",
        "    'norm': comp_norm,\n",
        "    'none': none,\n",
        "}\n"
      ],
      "metadata": {
        "id": "qoH-UL4GFAis"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Functions"
      ],
      "metadata": {
        "id": "r54Mq3uOFrb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(true, pred):\n",
        "    loss = cp.mean(cp.abs(pred - true))\n",
        "    dloss = pred - true\n",
        "    return loss, dloss\n",
        "\n",
        "def cat_crossentropy(true, pred):\n",
        "    loss = cp.mean(-(true * cp.log(pred)) + ((1 - true) * cp.log(1 - pred)))\n",
        "    dloss = - (pred - true) / (cp.square(pred) - pred)\n",
        "    return loss, dloss\n",
        "\n",
        "def wave_loss(true, pred):\n",
        "    loss = cp.mean(cp.abs(true) * cp.abs(pred - true))\n",
        "    dloss = cp.abs(true) * (pred - true)\n",
        "    return loss, dloss\n",
        "\n",
        "loss_funcs = {\n",
        "    'mse': mse,\n",
        "    'catcrossentropy': cat_crossentropy,\n",
        "    'wave': wave_loss,\n",
        "}"
      ],
      "metadata": {
        "id": "jTnrtO2FFG_y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer Classes"
      ],
      "metadata": {
        "id": "onbvDioBFt79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BaseConv2D:\n",
        "    \n",
        "    def __init__(self, n_kernels, kernel_size, padding='valid', padding_mode='constant', activation='sigmoid', input_size=(), lr=1e-2, dtype=cp.float64, **kwargs):\n",
        "        \n",
        "        self.pad_type = padding\n",
        "        self.pad_mode = padding_mode\n",
        "        self.activate = activation_funcs[activation]\n",
        "        self.lr = lr\n",
        "        self.n_kernels = n_kernels\n",
        "\n",
        "        if isinstance(input_size, int):\n",
        "            self.input_size = (input_size, input_size, 1)\n",
        "        elif isinstance(input_size, tuple):\n",
        "            self.input_size = (*input_size, *([1]*(3-len(input_size))))\n",
        "\n",
        "        if isinstance(kernel_size, int):\n",
        "            self.kernel_size = (kernel_size, kernel_size, self.input_size[2])\n",
        "        elif len(kernel_size) == 1:\n",
        "            self.kernel_size = (kernel_size[0], 1, self.input_size[2])\n",
        "        elif len(kernel_size) == 2:\n",
        "            self.kernel_size = (*kernel_size, self.input_size[2])\n",
        "\n",
        "        self.weights = cp.random.rand(1, *self.kernel_size, self.n_kernels).astype(dtype) / 100\n",
        "        self.bias = cp.zeros([1, 1, 1, self.n_kernels]).astype(dtype)\n",
        "\n",
        "        if self.pad_type == 'same':\n",
        "            pre0 = math.ceil((self.kernel_size[0] - 1) / 2)\n",
        "            post0 = math.floor((self.kernel_size[0] - 1) / 2)\n",
        "            pre1 = math.ceil((self.kernel_size[1] - 1) / 2)\n",
        "            post1 = math.floor((self.kernel_size[1] - 1) / 2)\n",
        "        elif self.pad_type == 'full':\n",
        "            pre0 = post0 = self.kernel_size[0] - 1\n",
        "            pre1 = post1 = self.kernel_size[1] - 1\n",
        "        else:\n",
        "            pre0 = post0 = pre1 = post1 = 0\n",
        "\n",
        "        self.pad = ((0,0), (pre0, post0), (pre1, post1), (0,0))\n",
        "        self.output_size = (self.input_size[0] - self.kernel_size[0] + pre0+post0+1, self.input_size[1] - self.kernel_size[1] + pre1+post1+1, self.n_kernels)\n",
        "        self.dact = cp.empty([])\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.pad_mode == 'wrap':\n",
        "            self.x = cp.pad(x, self.pad, 'wrap')[:,:,:,:,cp.newaxis]\n",
        "        else:\n",
        "            self.x = cp.pad(x, self.pad, 'constant', constant_values=0)[:,:,:,:,cp.newaxis]\n",
        "        act, self.dact = self.activate(cp.sum(fftconvolve(self.x, self.weights, mode='valid', axes=(1, 2)), axis=3) + self.bias)\n",
        "        return act\n",
        "    \n",
        "    def backprop(self, grad):\n",
        "        grad = (self.dact * grad)[:,:,:,cp.newaxis]\n",
        "        grad = cp.where(cp.isfinite(grad), grad, 0)\n",
        "        wgrad = cp.sum(fftconvolve(cp.flip(self.x, axis=(1,2)), grad, mode='valid', axes=(1, 2)), axis=0, keepdims=True)\n",
        "        bgrad = cp.sum(grad, axis=(0, 1, 2))\n",
        "\n",
        "        p0 = self.pad[1]\n",
        "        p1 = self.pad[2]\n",
        "        grad = cp.sum(fftconvolve(grad, cp.flip(self.weights, axis=(1, 2)), mode='full', axes=(1, 2)), axis=4)\n",
        "\n",
        "        self.bias -= (self.lr * bgrad)\n",
        "        self.weights -= (self.lr * wgrad)\n",
        "\n",
        "        return grad[:, p0[0]:grad.shape[1]-p0[1], p1[0]:grad.shape[2]-p1[1], :]\n",
        "\n",
        "\n",
        "class BaseFlatten:\n",
        "    def __init__(self, input_size, **kwargs):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = np.prod(input_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return x.reshape([-1, self.output_size])\n",
        "\n",
        "    def backprop(self, grad):\n",
        "        return grad.reshape([-1, *self.input_size])\n",
        "\n",
        "\n",
        "class BaseDense:\n",
        "    def __init__(self, n_neurons, input_size, lr, activation, dtype):\n",
        "        \n",
        "        self.weights = cp.random.rand(input_size, n_neurons).astype(dtype) / 10\n",
        "        self.bias = cp.zeros([1, n_neurons]).astype(dtype)\n",
        "        \n",
        "        self.lr = lr\n",
        "        self.activate = activation_funcs[activation]\n",
        "        self.output_size = n_neurons\n",
        "        self.x = cp.empty([])\n",
        "        self.dact = cp.empty([])\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x.copy()\n",
        "        act, self.dact = self.activate(cp.dot(self.x, self.weights) + self.bias)\n",
        "        return act\n",
        "    \n",
        "    def backprop(self, grad):\n",
        "        grad = self.dact * grad\n",
        "        grad = cp.where(cp.isfinite(grad), grad, 0)\n",
        "        wgrad = cp.dot(self.x.T, grad)\n",
        "        bgrad = cp.sum(grad, axis=0, keepdims=True)\n",
        "\n",
        "        grad = cp.dot(grad, self.weights.T)\n",
        "\n",
        "        self.weights -= (self.lr * wgrad)\n",
        "        self.bias -= (self.lr * bgrad)\n",
        "\n",
        "        return grad\n",
        "\n",
        "def Flatten(input_size=()):\n",
        "    class Flatten(BaseFlatten):\n",
        "        def __init__(self, input_size=input_size, **kwargs):\n",
        "            super().__init__(input_size, **kwargs)\n",
        "    \n",
        "    return Flatten\n",
        "\n",
        "def Dense(n_neurons=0, input_size=(), lr=1e-2, activation='sigmoid'):\n",
        "    class Dense(BaseDense):\n",
        "        def __init__(self, n_neurons=n_neurons, input_size=input_size, lr=lr, activation=activation):\n",
        "            super().__init__(n_neurons, input_size, lr, activation)\n",
        "    \n",
        "    return Dense\n",
        "\n",
        "def Conv2D(n_kernels=0, kernel_size=0, padding='valid', padding_mode='constant', activation='sigmoid', input_size=()):\n",
        "    class Conv2D(BaseConv2D):\n",
        "        def __init__(self, n_kernels=n_kernels, kernel_size=kernel_size, padding=padding, padding_mode=padding_mode, activation=activation, input_size=input_size, **kwargs):\n",
        "            super().__init__(n_kernels, kernel_size, padding, padding_mode, activation, input_size, **kwargs)\n",
        "\n",
        "    return Conv2D\n",
        "\n",
        "layer_funcs = {\n",
        "    'Flatten': Flatten,\n",
        "    'Dense': Dense,\n",
        "    'Conv2D': Conv2D,\n",
        "}"
      ],
      "metadata": {
        "id": "l2hvZYjHFaVt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential Model Class"
      ],
      "metadata": {
        "id": "XQGg7mERF0Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequential:\n",
        "    def __init__(self, layers=[], loss='mse', lr=1e-2, dtype=cp.float64):\n",
        "        self.loss = loss_funcs[loss]\n",
        "        self.lr = lr\n",
        "        self.dtype = dtype\n",
        "\n",
        "        if len(layers) > 0:\n",
        "            self.layers = [layers[0](lr=self.lr, dtype=self.dtype)]\n",
        "            output_size = self.layers[0].output_size\n",
        "\n",
        "            for layer in layers[1:]:\n",
        "                self.layers.append(layer(input_size=output_size, lr=self.lr, dtype=self.dtype))\n",
        "                output_size = self.layers[-1].output_size\n",
        "        else:\n",
        "            self.layers = []\n",
        "\n",
        "        self.history = []\n",
        "\n",
        "    def save(self, path, name):\n",
        "        if not os.path.exists(path):\n",
        "            os.mkdir(path)\n",
        "\n",
        "        if not os.path.exists(f'{path}{name}/'):\n",
        "            os.mkdir(f'{path}{name}/')\n",
        "\n",
        "        with npy(f'{path}{name}/history.npy', delete_if_exists=True) as h:\n",
        "            h.append(np.array(self.history))\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            l_type = str(type(layer).__name__)\n",
        "            try:\n",
        "                with npy(f'{path}{name}/{i}_{l_type}_weights.npy', delete_if_exists=True) as w:\n",
        "                    w.append(layer.weights)\n",
        "                with npy(f'{path}{name}/{i}_{l_type}_bias.npy', delete_if_exists=True) as b:\n",
        "                    b.append(layer.bias)\n",
        "            except AttributeError:\n",
        "                continue\n",
        "\n",
        "    def load_weights(self, path, name):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            l_type = str(type(layer).__name__)\n",
        "            try:\n",
        "                layer.weights = cp.load(f'{path}{name}/{i}_{l_type}_weights.npy')\n",
        "                layer.bias = cp.load(f'{path}{name}/{i}_{l_type}_bias.npy')\n",
        "            except AttributeError:\n",
        "                continue\n",
        "\n",
        "    def add(self, layer):\n",
        "        if len(self.layers) > 0:\n",
        "            output_size = self.layers[-1].output_size\n",
        "            self.layers.append(layer(input_size=output_size, lr=self.lr, dtype=self.dtype))\n",
        "        else:\n",
        "            self.layers.append(layer(lr=self.lr, dtype=self.dtype))\n",
        "    \n",
        "    def set(self, loss=None, lr=None):\n",
        "        if loss is not None:\n",
        "            self.loss = loss_funcs[loss]\n",
        "        if lr is not None:\n",
        "            self.lr = lr\n",
        "\n",
        "        for layer in self.layers:\n",
        "            layer.lr = self.lr\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backprop(self, grad):\n",
        "        for layer in reversed(self.layers):\n",
        "            grad = layer.backprop(grad)\n",
        "\n",
        "    def train_batch(self, x, y, epochs=1):\n",
        "        for epoch in range(1, epochs+1):\n",
        "            pred = self.predict(x)\n",
        "            loss, grad = self.loss(y, pred)\n",
        "\n",
        "            if not cp.all(cp.isfinite(loss)):\n",
        "                raise Exception('Training Error: Non-finite values present in loss') \n",
        "            if not cp.all(cp.isfinite(grad)):\n",
        "                raise Exception('Training Error: Non-finite values present in grad')\n",
        "\n",
        "            self.backprop(grad)\n",
        "            self.history.append(loss.get())\n",
        "    \n",
        "\n",
        "    def train(self, x, y, batch_size=None, epochs=1, loss=None, lr=None):\n",
        "        self.set(loss, lr)\n",
        "\n",
        "        if batch_size is not None:\n",
        "            batches = [range(i, i+batch_size) for i in range(0, y.shape[0]-batch_size, batch_size)]\n",
        "            batches += [range(y.shape[0]-(y.shape[0] % batch_size), y.shape[0])]*(y.shape[0] % batch_size > 0)\n",
        "        else:\n",
        "            batches = [range(y.shape[0])]\n",
        "\n",
        "        for epoch in range(1, epochs+1):\n",
        "            print(f'Epoch {epoch: >5}/{epochs: <3}:\\n')\n",
        "\n",
        "            for i, batch in enumerate(batches):\n",
        "                pred = self.predict(cp.array(x[batch]))\n",
        "                \n",
        "                loss, grad = self.loss(cp.array(y[batch]), pred)\n",
        "                self.backprop(grad)\n",
        "\n",
        "            self.history.append(loss)\n",
        "            print(f'loss[{loss:.3f}]\\n')\n"
      ],
      "metadata": {
        "id": "HicHRNVRFced"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Model Classes"
      ],
      "metadata": {
        "id": "mE_rVIl7G_Cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WaveModel(SchroModel):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sim_steps = cp.empty([0, *self.model.layers[0].input_size[:-1], 2])\n",
        "\n",
        "    def add_step(self, phi):\n",
        "        step = cp.stack([cp.sum(phi, axis=0), cp.sum(self.sim.V + self.sim.ev, axis=0)], axis=-1)[cp.newaxis]\n",
        "        self.sim_steps = cp.append(self.sim_steps, step, axis=0)\n",
        "    \n",
        "    def predict(self, phi, n_samp):\n",
        "        self.sim.e_field(phi, n_samp)\n",
        "        self.add_step(phi)\n",
        "\n",
        "        pred = self.model.predict(self.sim_steps)[:, :, :, 0] \n",
        "        self.clear_steps()\n",
        "        return self.sim.norm(pred)\n",
        "\n",
        "\n",
        "class EVModel(SchroModel):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sim_steps = cp.empty([0, *self.model.layers[0].input_size[:-1], 2])\n",
        "\n",
        "    def add_step(self, phi):\n",
        "        Vt = (cp.sum(self.sim.ev, axis=0, keepdims=True) - self.sim.ev) + self.sim.V\n",
        "        phin = cp.exp(1j * Vt * self.sim.dt/2.0) * phi\n",
        "        ev_step = cp.real(cp.log(cp.sum(phin, axis=0)/cp.sum(phi, axis=0)) * (-2j/self.sim.dt)) - self.sim.V\n",
        "        ev_step = cp.where(cp.isfinite(ev_step), ev_step, 0)\n",
        "\n",
        "        step = cp.stack([cp.sum(cp.square(cp.abs(phi)), axis=0, keepdims=True), ev_step*-1], axis=-1)\n",
        "        self.sim_steps = cp.append(self.sim_steps, step, axis=0)\n",
        "    \n",
        "    def train(self, epochs):\n",
        "        self.model.train_batch(self.sim_steps[:, :, :, :1], self.sim_steps[:, :, :, 1:], epochs=epochs)\n",
        "        self.clear_steps()\n",
        "        return np.mean(self.model.history[-epochs:])\n",
        "\n",
        "    def predict(self, phi, n_samp):\n",
        "        phi = cp.sum(phi, axis=0)\n",
        "\n",
        "        ev = -1 * self.model.predict(cp.square(cp.abs(phi))[cp.newaxis, :, :, cp.newaxis])[:, :, :, 0]\n",
        "        phi *= cp.exp(1j * (self.sim.V + ev) * self.sim.dt / 2.0)\n",
        "\n",
        "        phihat = fft.fftn(phi)\n",
        "        phihat = cp.exp(self.sim.dt * (-1j * fft.ifftshift(cp.square(self.sim.dists))/(2.0 * self.sim.me)))  * phihat\n",
        "        phi = fft.ifftn(phihat)\n",
        "\n",
        "        ev = -1 * self.model.predict(cp.square(cp.abs(phi))[cp.newaxis, :, :, cp.newaxis])[:, :, :, 0]\n",
        "        phi *= cp.exp(1j * (self.sim.V + ev) * self.sim.dt / 2.0)\n",
        "        \n",
        "        self.sim.ev = ev\n",
        "        return self.sim.norm(phi)\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "CwNx0UHOHCvw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Models"
      ],
      "metadata": {
        "id": "oBl4zjbxGIDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wave_mod = Sequential([\n",
        "    Conv2D(n_kernels=1, kernel_size=3, padding='same', padding_mode='wrap', input_size=(500,500,2), activation='norm')\n",
        "    ], loss='wave', lr=1e-7, dtype=cp.complex128)\n",
        "#wave_mod.layers[0].weights *= 1j\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    sim = SchroSim()\n",
        "    sim.add_proton(pos=randint(-2, 3, 2))\n",
        "    sim.add_electron(p=randint(-20, 21, 2), pos=randint(-2, 3, 2))\n",
        "\n",
        "    mod = WaveModel(model=wave_mod, sim=sim)\n",
        "    sim.simulate(dims=(5,5), dau=1e-2, time_step=1e-1, steps=5000, model=mod, train_model=20, ev_samp_rate=0)\n",
        "    wave_mod.save(path='./wave1_checks/', name=f'wave1_{i+1}')\n",
        "    print(f'Simulation wave training {i+1} done')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etTpvWteF4Wj",
        "outputId": "b785c593-3932-4251-b37e-51269f1f62ae"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Trained at Step 20: loss [0.03982271657477765]\n",
            "Model Trained at Step 40: loss [0.04017058619489774]\n",
            "Model Trained at Step 60: loss [0.04019960483870215]\n",
            "Model Trained at Step 80: loss [0.04021560787140915]\n",
            "Model Trained at Step 100: loss [0.040166055137399057]\n",
            "Model Trained at Step 120: loss [0.04015627994889745]\n",
            "Model Trained at Step 140: loss [0.04016447894660521]\n",
            "Model Trained at Step 160: loss [0.04016420432118802]\n",
            "Model Trained at Step 180: loss [0.04022447245224293]\n",
            "Model Trained at Step 200: loss [0.040643698893409776]\n",
            "Model Trained at Step 220: loss [0.04072228399896503]\n",
            "Model Trained at Step 240: loss [0.04083953586416002]\n",
            "Model Trained at Step 260: loss [0.04082953302449214]\n",
            "Model Trained at Step 280: loss [0.04077404443657022]\n",
            "Model Trained at Step 300: loss [0.0407623814585762]\n",
            "Model Trained at Step 320: loss [0.040718644804522613]\n",
            "Model Trained at Step 340: loss [0.04069226406499905]\n",
            "Model Trained at Step 360: loss [0.0406573988589312]\n",
            "Model Trained at Step 380: loss [0.04062530871286121]\n",
            "Model Trained at Step 400: loss [0.04058973372345831]\n",
            "Model Trained at Step 420: loss [0.04055872865871939]\n",
            "Model Trained at Step 440: loss [0.04051886546871432]\n",
            "Model Trained at Step 460: loss [0.04048962239258729]\n",
            "Model Trained at Step 480: loss [0.040450297037035496]\n",
            "Model Trained at Step 500: loss [0.04041962488941189]\n",
            "Model Trained at Step 520: loss [0.040383055772148325]\n",
            "Model Trained at Step 540: loss [0.04035278325652493]\n",
            "Model Trained at Step 560: loss [0.040318167716104344]\n",
            "Model Trained at Step 580: loss [0.04028988377640232]\n",
            "Model Trained at Step 600: loss [0.04025826396166445]\n",
            "Model Trained at Step 620: loss [0.04023172073403221]\n",
            "Model Trained at Step 640: loss [0.04020415511824578]\n",
            "Model Trained at Step 660: loss [0.040180647143362]\n",
            "Model Trained at Step 680: loss [0.04015806764573994]\n",
            "Model Trained at Step 700: loss [0.040137796666629345]\n",
            "Model Trained at Step 720: loss [0.0401210710291579]\n",
            "Model Trained at Step 740: loss [0.0401062165150934]\n",
            "Model Trained at Step 760: loss [0.04009464733403789]\n",
            "Model Trained at Step 780: loss [0.04008640172216514]\n",
            "Model Trained at Step 800: loss [0.04008024812878279]\n",
            "Model Trained at Step 820: loss [0.04007764528512111]\n",
            "Model Trained at Step 840: loss [0.04007767863201113]\n",
            "Model Trained at Step 860: loss [0.040080719600673095]\n",
            "Model Trained at Step 880: loss [0.040086173329824136]\n",
            "Model Trained at Step 900: loss [0.04009456136088527]\n",
            "Model Trained at Step 920: loss [0.04010589630325565]\n",
            "Model Trained at Step 940: loss [0.040118576025758565]\n",
            "Model Trained at Step 960: loss [0.04013395967493129]\n",
            "Model Trained at Step 980: loss [0.04015082019488461]\n",
            "Model Trained at Step 1000: loss [0.040168100559962935]\n",
            "Model Trained at Step 1020: loss [0.040187239079187656]\n",
            "Model Trained at Step 1040: loss [0.04020539757248954]\n",
            "Model Trained at Step 1060: loss [0.04022336142712794]\n",
            "Model Trained at Step 1080: loss [0.040239882016807116]\n",
            "Model Trained at Step 1100: loss [0.04025613299325938]\n",
            "Model Trained at Step 1120: loss [0.040269320513935905]\n",
            "Model Trained at Step 1140: loss [0.040279902380368576]\n",
            "Model Trained at Step 1160: loss [0.04028848817421603]\n",
            "Model Trained at Step 1180: loss [0.04029539112092239]\n",
            "Model Trained at Step 1200: loss [0.040295359405099554]\n",
            "Model Trained at Step 1220: loss [0.04029657051790463]\n",
            "Model Trained at Step 1240: loss [0.040295247038574386]\n",
            "Model Trained at Step 1260: loss [0.040290395536243806]\n",
            "Model Trained at Step 1280: loss [0.04028386639196374]\n",
            "Model Trained at Step 1300: loss [0.04027874148577719]\n",
            "Model Trained at Step 1320: loss [0.040267677977962026]\n",
            "Model Trained at Step 1340: loss [0.04025965555909391]\n",
            "Model Trained at Step 1360: loss [0.04024828921878697]\n",
            "Model Trained at Step 1380: loss [0.04023664997220673]\n",
            "Model Trained at Step 1400: loss [0.040225808396487005]\n",
            "Model Trained at Step 1420: loss [0.040213206444485436]\n",
            "Model Trained at Step 1440: loss [0.040202379504691674]\n",
            "Model Trained at Step 1460: loss [0.04019083657896497]\n",
            "Model Trained at Step 1480: loss [0.04017854986023624]\n",
            "Model Trained at Step 1500: loss [0.04016761767273399]\n",
            "Model Trained at Step 1520: loss [0.040156050056956286]\n",
            "Model Trained at Step 1540: loss [0.04014580754547159]\n",
            "Model Trained at Step 1560: loss [0.040135640572639096]\n",
            "Model Trained at Step 1580: loss [0.040126455920109785]\n",
            "Model Trained at Step 1600: loss [0.04011906156095163]\n",
            "Model Trained at Step 1620: loss [0.04011039166548876]\n",
            "Model Trained at Step 1640: loss [0.04010397722433555]\n",
            "Model Trained at Step 1660: loss [0.04009772725137867]\n",
            "Model Trained at Step 1680: loss [0.04009254887499002]\n",
            "Model Trained at Step 1700: loss [0.04008786899535619]\n",
            "Model Trained at Step 1720: loss [0.040084979793738804]\n",
            "Model Trained at Step 1740: loss [0.04008131503751864]\n",
            "Model Trained at Step 1760: loss [0.04007985557623271]\n",
            "Model Trained at Step 1780: loss [0.04007855576155504]\n",
            "Model Trained at Step 1800: loss [0.0400780313450178]\n",
            "Model Trained at Step 1820: loss [0.04007784998389745]\n",
            "Model Trained at Step 1840: loss [0.04007919567703304]\n",
            "Model Trained at Step 1860: loss [0.04008029707402068]\n",
            "Model Trained at Step 1880: loss [0.040082192987475916]\n",
            "Model Trained at Step 1900: loss [0.040084762788032226]\n",
            "Model Trained at Step 1920: loss [0.040088800990497275]\n",
            "Model Trained at Step 1940: loss [0.04009273781596608]\n",
            "Model Trained at Step 1960: loss [0.04009851721373235]\n",
            "Model Trained at Step 1980: loss [0.0401038060536673]\n",
            "Model Trained at Step 2000: loss [0.04010999273465832]\n",
            "Model Trained at Step 2020: loss [0.04011665977535369]\n",
            "Model Trained at Step 2040: loss [0.04012303233930289]\n",
            "Model Trained at Step 2060: loss [0.04012971846469512]\n",
            "Model Trained at Step 2080: loss [0.040137698556373155]\n",
            "Model Trained at Step 2100: loss [0.040144252904852504]\n",
            "Model Trained at Step 2120: loss [0.04015217658131649]\n",
            "Model Trained at Step 2140: loss [0.040159480496365085]\n",
            "Model Trained at Step 2160: loss [0.04016773929371018]\n",
            "Model Trained at Step 2180: loss [0.040175320420115096]\n",
            "Model Trained at Step 2200: loss [0.04018307281941202]\n",
            "Model Trained at Step 2220: loss [0.0401909789533391]\n",
            "Model Trained at Step 2240: loss [0.04019925364257261]\n",
            "Model Trained at Step 2260: loss [0.040207053368821105]\n",
            "Model Trained at Step 2280: loss [0.04021502278750152]\n",
            "Model Trained at Step 2300: loss [0.04022253261531166]\n",
            "Model Trained at Step 2320: loss [0.04023130035179086]\n",
            "Model Trained at Step 2340: loss [0.04023848045892261]\n",
            "Model Trained at Step 2360: loss [0.04024611805874277]\n",
            "Model Trained at Step 2380: loss [0.040254907653907296]\n",
            "Model Trained at Step 2400: loss [0.04026049031850946]\n",
            "Model Trained at Step 2420: loss [0.04026933981624837]\n",
            "Model Trained at Step 2440: loss [0.040276126556442686]\n",
            "Model Trained at Step 2460: loss [0.04028252970996528]\n",
            "Model Trained at Step 2480: loss [0.040294289218297065]\n",
            "Model Trained at Step 2500: loss [0.04029689276004726]\n",
            "Model Trained at Step 2520: loss [0.040307076896381695]\n",
            "Model Trained at Step 2540: loss [0.04031767710203699]\n",
            "Model Trained at Step 2560: loss [0.0403136952216147]\n",
            "Model Trained at Step 2580: loss [0.040332826515114495]\n",
            "Model Trained at Step 2600: loss [0.04033113270967155]\n",
            "Model Trained at Step 2620: loss [0.04033797957324322]\n",
            "Model Trained at Step 2640: loss [0.0403522965138001]\n",
            "Model Trained at Step 2660: loss [0.04034732048480409]\n",
            "Model Trained at Step 2680: loss [0.04036705280383593]\n",
            "Model Trained at Step 2700: loss [0.04036049495057462]\n",
            "Model Trained at Step 2720: loss [0.04038112940806904]\n",
            "Model Trained at Step 2740: loss [0.040374665617692894]\n",
            "Model Trained at Step 2760: loss [0.040391566895495355]\n",
            "Model Trained at Step 2780: loss [0.04039054277774694]\n",
            "Model Trained at Step 2800: loss [0.040401528650138574]\n",
            "Model Trained at Step 2820: loss [0.040406090136260724]\n",
            "Model Trained at Step 2840: loss [0.04041230418738751]\n",
            "Model Trained at Step 2860: loss [0.040420673043199976]\n",
            "Model Trained at Step 2880: loss [0.04042466177304997]\n",
            "Model Trained at Step 2900: loss [0.040433255412843115]\n",
            "Model Trained at Step 2920: loss [0.040437443934573006]\n",
            "Model Trained at Step 2940: loss [0.040444749570877174]\n",
            "Model Trained at Step 2960: loss [0.040450390325148425]\n",
            "Model Trained at Step 2980: loss [0.040457370805492485]\n",
            "Model Trained at Step 3000: loss [0.04046244038552957]\n",
            "Model Trained at Step 3020: loss [0.04046914544197912]\n",
            "Model Trained at Step 3040: loss [0.040473195538872984]\n",
            "Model Trained at Step 3060: loss [0.0404799497399833]\n",
            "Model Trained at Step 3080: loss [0.04048380224480972]\n",
            "Model Trained at Step 3100: loss [0.0404892486136388]\n",
            "Model Trained at Step 3120: loss [0.040493152867649505]\n",
            "Model Trained at Step 3140: loss [0.040497660360477375]\n",
            "Model Trained at Step 3160: loss [0.0405015574828562]\n",
            "Model Trained at Step 3180: loss [0.04050579170110744]\n",
            "Model Trained at Step 3200: loss [0.040510079603811464]\n",
            "Model Trained at Step 3220: loss [0.040515657997330395]\n",
            "Model Trained at Step 3240: loss [0.040519722709173554]\n",
            "Model Trained at Step 3260: loss [0.04052357472678652]\n",
            "Model Trained at Step 3280: loss [0.040528470679317544]\n",
            "Model Trained at Step 3300: loss [0.04053274717724816]\n",
            "Model Trained at Step 3320: loss [0.04053633221677515]\n",
            "Model Trained at Step 3340: loss [0.040541482855445114]\n",
            "Model Trained at Step 3360: loss [0.040543838259484884]\n",
            "Model Trained at Step 3380: loss [0.040550293835295986]\n",
            "Model Trained at Step 3400: loss [0.04055274651733919]\n",
            "Model Trained at Step 3420: loss [0.04055632185636924]\n",
            "Model Trained at Step 3440: loss [0.0405606954051808]\n",
            "Model Trained at Step 3460: loss [0.04056427505692044]\n",
            "Model Trained at Step 3480: loss [0.04056733695813318]\n",
            "Model Trained at Step 3500: loss [0.04057143927508165]\n",
            "Model Trained at Step 3520: loss [0.04057477509041617]\n",
            "Model Trained at Step 3540: loss [0.04057794570850567]\n",
            "Model Trained at Step 3560: loss [0.04058033714673687]\n",
            "Model Trained at Step 3580: loss [0.04058323587610176]\n",
            "Model Trained at Step 3600: loss [0.04058452786337719]\n",
            "Model Trained at Step 3620: loss [0.04058736305430503]\n",
            "Model Trained at Step 3640: loss [0.04058654819666112]\n",
            "Model Trained at Step 3660: loss [0.04058869991358244]\n",
            "Model Trained at Step 3680: loss [0.04058789621153578]\n",
            "Model Trained at Step 3700: loss [0.04058785403051357]\n",
            "Model Trained at Step 3720: loss [0.04058751020040093]\n",
            "Model Trained at Step 3740: loss [0.040586633637149885]\n",
            "Model Trained at Step 3760: loss [0.04058647464369476]\n",
            "Model Trained at Step 3780: loss [0.04058554230922718]\n",
            "Model Trained at Step 3800: loss [0.040583501094101165]\n",
            "Model Trained at Step 3820: loss [0.04058278541812861]\n",
            "Model Trained at Step 3840: loss [0.04057967506320971]\n",
            "Model Trained at Step 3860: loss [0.04057872557549842]\n",
            "Model Trained at Step 3880: loss [0.04057555730473142]\n",
            "Model Trained at Step 3900: loss [0.04057345920003217]\n",
            "Model Trained at Step 3920: loss [0.04057020190316363]\n",
            "Model Trained at Step 3940: loss [0.04056699005537317]\n",
            "Model Trained at Step 3960: loss [0.04056251561060202]\n",
            "Model Trained at Step 3980: loss [0.04055870614670662]\n",
            "Model Trained at Step 4000: loss [0.04055355377806843]\n",
            "Model Trained at Step 4020: loss [0.040549460362421824]\n",
            "Model Trained at Step 4040: loss [0.04054332784670202]\n",
            "Model Trained at Step 4060: loss [0.040538465498911117]\n",
            "Model Trained at Step 4080: loss [0.04053181854083804]\n",
            "Model Trained at Step 4100: loss [0.04052603038527421]\n",
            "Model Trained at Step 4120: loss [0.04051829807073069]\n",
            "Model Trained at Step 4140: loss [0.040511085966223506]\n",
            "Model Trained at Step 4160: loss [0.04050215866287842]\n",
            "Model Trained at Step 4180: loss [0.04049377409063195]\n",
            "Model Trained at Step 4200: loss [0.04048458956012954]\n",
            "Model Trained at Step 4220: loss [0.040476048688347074]\n",
            "Model Trained at Step 4240: loss [0.040467973671694836]\n",
            "Model Trained at Step 4260: loss [0.04046088935290073]\n",
            "Model Trained at Step 4280: loss [0.04045394700480649]\n",
            "Model Trained at Step 4300: loss [0.04044776169887463]\n",
            "Model Trained at Step 4320: loss [0.04044243752740939]\n",
            "Model Trained at Step 4340: loss [0.0404376257066598]\n",
            "Model Trained at Step 4360: loss [0.0404334004081756]\n",
            "Model Trained at Step 4380: loss [0.04042979061385046]\n",
            "Model Trained at Step 4400: loss [0.040427464994538836]\n",
            "Model Trained at Step 4420: loss [0.040426643658923525]\n",
            "Model Trained at Step 4440: loss [0.04042656463903624]\n",
            "Model Trained at Step 4460: loss [0.0404281136583132]\n",
            "Model Trained at Step 4480: loss [0.04043085069516552]\n",
            "Model Trained at Step 4500: loss [0.04043445812010204]\n",
            "Model Trained at Step 4520: loss [0.04043959335248629]\n",
            "Model Trained at Step 4540: loss [0.040447020319297276]\n",
            "Model Trained at Step 4560: loss [0.04045586020452241]\n",
            "Model Trained at Step 4580: loss [0.04046563866691735]\n",
            "Model Trained at Step 4600: loss [0.040477491192864846]\n",
            "Model Trained at Step 4620: loss [0.04049035859176543]\n",
            "Model Trained at Step 4640: loss [0.04050357159119865]\n",
            "Model Trained at Step 4660: loss [0.0405200989967913]\n",
            "Model Trained at Step 4680: loss [0.040536853881740956]\n",
            "Model Trained at Step 4700: loss [0.04055363235735471]\n",
            "Model Trained at Step 4720: loss [0.04057355391292784]\n",
            "Model Trained at Step 4740: loss [0.040593385054332146]\n",
            "Model Trained at Step 4760: loss [0.04061200372920821]\n",
            "Model Trained at Step 4780: loss [0.04063485294519826]\n",
            "Model Trained at Step 4800: loss [0.04065558855451258]\n",
            "Model Trained at Step 4820: loss [0.04067778007876008]\n",
            "Model Trained at Step 4840: loss [0.04070260135272026]\n",
            "Model Trained at Step 4860: loss [0.04072252917473195]\n",
            "Model Trained at Step 4880: loss [0.04074603042508747]\n",
            "Model Trained at Step 4900: loss [0.04076892241115541]\n",
            "Model Trained at Step 4920: loss [0.040786290103378905]\n",
            "Model Trained at Step 4940: loss [0.04080986570921064]\n",
            "Model Trained at Step 4960: loss [0.040827889140138754]\n",
            "Model Trained at Step 4980: loss [0.04084536850549568]\n",
            "Model Trained at Step 5000: loss [0.04086538742156394]\n",
            "Simulation wave training 1 done\n",
            "Model Trained at Step 20: loss [0.040314025590771255]\n",
            "Model Trained at Step 40: loss [0.040125951101629136]\n",
            "Model Trained at Step 60: loss [0.040118970699006044]\n",
            "Model Trained at Step 80: loss [0.04014477325790658]\n",
            "Model Trained at Step 100: loss [0.04026911618814878]\n",
            "Model Trained at Step 120: loss [0.040301656611042236]\n",
            "Model Trained at Step 140: loss [0.040272037298279245]\n",
            "Model Trained at Step 160: loss [0.040252697529811636]\n",
            "Model Trained at Step 180: loss [0.040218943733638914]\n",
            "Model Trained at Step 200: loss [0.040192978858049254]\n",
            "Model Trained at Step 220: loss [0.04016095573190599]\n",
            "Model Trained at Step 240: loss [0.04013388079242944]\n",
            "Model Trained at Step 260: loss [0.040108786492317985]\n",
            "Model Trained at Step 280: loss [0.0400865442220066]\n",
            "Model Trained at Step 300: loss [0.040069257075671855]\n",
            "Model Trained at Step 320: loss [0.040059104340860976]\n",
            "Model Trained at Step 340: loss [0.040056536165225975]\n",
            "Model Trained at Step 360: loss [0.0400611184998959]\n",
            "Model Trained at Step 380: loss [0.040075447280757404]\n",
            "Model Trained at Step 400: loss [0.04009707124834724]\n",
            "Model Trained at Step 420: loss [0.04011949203061359]\n",
            "Model Trained at Step 440: loss [0.0400932494618607]\n",
            "Model Trained at Step 460: loss [0.04000698473770527]\n",
            "Model Trained at Step 480: loss [0.04027598436816916]\n",
            "Model Trained at Step 500: loss [0.04082631724091035]\n",
            "Model Trained at Step 520: loss [0.040718864420202464]\n",
            "Model Trained at Step 540: loss [0.0405108320259968]\n",
            "Model Trained at Step 560: loss [0.040535567088406846]\n",
            "Model Trained at Step 580: loss [0.04044599120682646]\n",
            "Model Trained at Step 600: loss [0.04038481849487563]\n",
            "Model Trained at Step 620: loss [0.04032513118268732]\n",
            "Model Trained at Step 640: loss [0.04027576654946946]\n",
            "Model Trained at Step 660: loss [0.04022908180303607]\n",
            "Model Trained at Step 680: loss [0.04019578916155242]\n",
            "Model Trained at Step 700: loss [0.04017070388682749]\n",
            "Model Trained at Step 720: loss [0.040161458187981516]\n",
            "Model Trained at Step 740: loss [0.04016962115880983]\n",
            "Model Trained at Step 760: loss [0.0401846181855085]\n",
            "Model Trained at Step 780: loss [0.040209710792161436]\n",
            "Model Trained at Step 800: loss [0.04023853205618796]\n",
            "Model Trained at Step 820: loss [0.040269882939545]\n",
            "Model Trained at Step 840: loss [0.040262521452344394]\n",
            "Model Trained at Step 860: loss [0.040260484698407636]\n",
            "Model Trained at Step 880: loss [0.04014871126883239]\n",
            "Model Trained at Step 900: loss [0.04005842346491253]\n",
            "Model Trained at Step 920: loss [0.0399698676287784]\n",
            "Model Trained at Step 940: loss [0.03997591966486834]\n",
            "Model Trained at Step 960: loss [0.0400808002045257]\n",
            "Model Trained at Step 980: loss [0.04016868777899469]\n",
            "Model Trained at Step 1000: loss [0.040152079065849014]\n",
            "Model Trained at Step 1020: loss [0.04014848253819625]\n",
            "Model Trained at Step 1040: loss [0.040166007565697055]\n",
            "Model Trained at Step 1060: loss [0.04008082663228416]\n",
            "Model Trained at Step 1080: loss [0.04011678576730855]\n",
            "Model Trained at Step 1100: loss [0.040063593170756445]\n",
            "Model Trained at Step 1120: loss [0.040079563896122175]\n",
            "Model Trained at Step 1140: loss [0.0400985594826795]\n",
            "Model Trained at Step 1160: loss [0.04012581782962088]\n",
            "Model Trained at Step 1180: loss [0.04015552696668935]\n",
            "Model Trained at Step 1200: loss [0.040183753057460914]\n",
            "Model Trained at Step 1220: loss [0.04021668045699761]\n",
            "Model Trained at Step 1240: loss [0.04024946802111044]\n",
            "Model Trained at Step 1260: loss [0.04028144804088666]\n",
            "Model Trained at Step 1280: loss [0.04031600860387272]\n",
            "Model Trained at Step 1300: loss [0.04035082230925641]\n",
            "Model Trained at Step 1320: loss [0.04038359189000823]\n",
            "Model Trained at Step 1340: loss [0.04041129963344666]\n",
            "Model Trained at Step 1360: loss [0.04042606721922569]\n",
            "Model Trained at Step 1380: loss [0.04042193864002985]\n",
            "Model Trained at Step 1400: loss [0.04040112491505822]\n",
            "Model Trained at Step 1420: loss [0.040406024503684385]\n",
            "Model Trained at Step 1440: loss [0.04050125386159291]\n",
            "Model Trained at Step 1460: loss [0.04061265651077241]\n",
            "Model Trained at Step 1480: loss [0.04054708855874439]\n",
            "Model Trained at Step 1500: loss [0.04046827051206564]\n",
            "Model Trained at Step 1520: loss [0.04054862816984057]\n",
            "Model Trained at Step 1540: loss [0.0405176141938128]\n",
            "Model Trained at Step 1560: loss [0.0404756347116364]\n",
            "Model Trained at Step 1580: loss [0.040480976903616336]\n",
            "Model Trained at Step 1600: loss [0.04043525794382163]\n",
            "Model Trained at Step 1620: loss [0.04041703883187099]\n",
            "Model Trained at Step 1640: loss [0.04038165775992728]\n",
            "Model Trained at Step 1660: loss [0.04035308007583431]\n",
            "Model Trained at Step 1680: loss [0.04032482045372683]\n",
            "Model Trained at Step 1700: loss [0.040300975064321065]\n",
            "Model Trained at Step 1720: loss [0.04027205936250759]\n",
            "Model Trained at Step 1740: loss [0.04023254432024477]\n",
            "Model Trained at Step 1760: loss [0.0401608085701078]\n",
            "Model Trained at Step 1780: loss [0.04004602591056663]\n",
            "Model Trained at Step 1800: loss [0.039868558186218685]\n",
            "Model Trained at Step 1820: loss [0.0396505710466542]\n",
            "Model Trained at Step 1840: loss [0.039554556751553574]\n",
            "Model Trained at Step 1860: loss [0.03948354750754425]\n",
            "Model Trained at Step 1880: loss [0.03913059358318236]\n",
            "Model Trained at Step 1900: loss [0.038817532028036686]\n",
            "Model Trained at Step 1920: loss [0.03900381003327546]\n",
            "Model Trained at Step 1940: loss [0.03962909010839095]\n",
            "Model Trained at Step 1960: loss [0.03948280485851824]\n",
            "Model Trained at Step 1980: loss [0.03944318450531571]\n",
            "Model Trained at Step 2000: loss [0.039733395931249256]\n",
            "Model Trained at Step 2020: loss [0.03983086337198559]\n",
            "Model Trained at Step 2040: loss [0.0398164745548068]\n",
            "Model Trained at Step 2060: loss [0.039825628888210875]\n",
            "Model Trained at Step 2080: loss [0.03972402487819639]\n",
            "Model Trained at Step 2100: loss [0.0396055522613951]\n",
            "Model Trained at Step 2120: loss [0.039291009969667626]\n",
            "Model Trained at Step 2140: loss [0.03974147308707221]\n",
            "Model Trained at Step 2160: loss [0.03951214707219571]\n",
            "Model Trained at Step 2180: loss [0.03960333019538089]\n",
            "Model Trained at Step 2200: loss [0.03972605306342175]\n",
            "Model Trained at Step 2220: loss [0.03972806557181518]\n",
            "Model Trained at Step 2240: loss [0.03980977805727837]\n",
            "Model Trained at Step 2260: loss [0.039876716921319985]\n",
            "Model Trained at Step 2280: loss [0.03993842437440521]\n",
            "Model Trained at Step 2300: loss [0.03998260259426347]\n",
            "Model Trained at Step 2320: loss [0.040003107604533725]\n",
            "Model Trained at Step 2340: loss [0.03999081802126292]\n",
            "Model Trained at Step 2360: loss [0.03997445339368422]\n",
            "Model Trained at Step 2380: loss [0.04002474576915251]\n",
            "Model Trained at Step 2400: loss [0.040155798049299016]\n",
            "Model Trained at Step 2420: loss [0.04025442871676179]\n",
            "Model Trained at Step 2440: loss [0.040173086124799265]\n",
            "Model Trained at Step 2460: loss [0.040095451508031774]\n",
            "Model Trained at Step 2480: loss [0.040215011622538245]\n",
            "Model Trained at Step 2500: loss [0.04023232130703575]\n",
            "Model Trained at Step 2520: loss [0.040126300992281226]\n",
            "Model Trained at Step 2540: loss [0.040183337996402634]\n",
            "Model Trained at Step 2560: loss [0.040159420418932515]\n",
            "Model Trained at Step 2580: loss [0.04011329742001989]\n",
            "Model Trained at Step 2600: loss [0.040112913071391615]\n",
            "Model Trained at Step 2620: loss [0.04004574065681439]\n",
            "Model Trained at Step 2640: loss [0.04001494206037003]\n",
            "Model Trained at Step 2660: loss [0.03992952903693174]\n",
            "Model Trained at Step 2680: loss [0.039882523967120886]\n",
            "Model Trained at Step 2700: loss [0.039767760335310806]\n",
            "Model Trained at Step 2720: loss [0.039708184604106564]\n",
            "Model Trained at Step 2740: loss [0.03963410425919457]\n",
            "Model Trained at Step 2760: loss [0.03946616849129249]\n",
            "Model Trained at Step 2780: loss [0.03920488636765403]\n",
            "Model Trained at Step 2800: loss [0.039043151878024054]\n",
            "Model Trained at Step 2820: loss [0.038819611584331955]\n",
            "Model Trained at Step 2840: loss [0.03821372160929553]\n",
            "Model Trained at Step 2860: loss [0.039568816844758506]\n",
            "Model Trained at Step 2880: loss [0.039469236460538584]\n",
            "Model Trained at Step 2900: loss [0.039317711668331]\n",
            "Model Trained at Step 2920: loss [0.03878291278910245]\n",
            "Model Trained at Step 2940: loss [0.03743927543426997]\n",
            "Model Trained at Step 2960: loss [0.03995537632532292]\n",
            "Model Trained at Step 2980: loss [0.039915180004627716]\n",
            "Model Trained at Step 3000: loss [0.03971441494165112]\n",
            "Model Trained at Step 3020: loss [0.03905327590260431]\n",
            "Model Trained at Step 3040: loss [0.03834739133044431]\n",
            "Model Trained at Step 3060: loss [0.0391631283336088]\n",
            "Model Trained at Step 3080: loss [0.03836333652179894]\n",
            "Model Trained at Step 3100: loss [0.039171215081973645]\n",
            "Model Trained at Step 3120: loss [0.03857675937846249]\n",
            "Model Trained at Step 3140: loss [0.03900441684137394]\n",
            "Model Trained at Step 3160: loss [0.03874867597881224]\n",
            "Model Trained at Step 3180: loss [0.03890130468814139]\n",
            "Model Trained at Step 3200: loss [0.038986030005684826]\n",
            "Model Trained at Step 3220: loss [0.039056853117252245]\n",
            "Model Trained at Step 3240: loss [0.03916685302585616]\n",
            "Model Trained at Step 3260: loss [0.03927305281335345]\n",
            "Model Trained at Step 3280: loss [0.03929559725736527]\n",
            "Model Trained at Step 3300: loss [0.03933990272759289]\n",
            "Model Trained at Step 3320: loss [0.039421192543335294]\n",
            "Model Trained at Step 3340: loss [0.03950829719976863]\n",
            "Model Trained at Step 3360: loss [0.03960372493041432]\n",
            "Model Trained at Step 3380: loss [0.039651512359246535]\n",
            "Model Trained at Step 3400: loss [0.039667983981521396]\n",
            "Model Trained at Step 3420: loss [0.03969041314081319]\n",
            "Model Trained at Step 3440: loss [0.039748215791422535]\n",
            "Model Trained at Step 3460: loss [0.03976996875409024]\n",
            "Model Trained at Step 3480: loss [0.03974276901075764]\n",
            "Model Trained at Step 3500: loss [0.03976349132884076]\n",
            "Model Trained at Step 3520: loss [0.03977895481871764]\n",
            "Model Trained at Step 3540: loss [0.039748968996497366]\n",
            "Model Trained at Step 3560: loss [0.03974912599237122]\n",
            "Model Trained at Step 3580: loss [0.03972444139201651]\n",
            "Model Trained at Step 3600: loss [0.039723048634862]\n",
            "Model Trained at Step 3620: loss [0.039693862607831545]\n",
            "Model Trained at Step 3640: loss [0.039682974989132425]\n",
            "Model Trained at Step 3660: loss [0.03966387920592806]\n",
            "Model Trained at Step 3680: loss [0.039691832818639076]\n",
            "Model Trained at Step 3700: loss [0.03967821034760054]\n",
            "Model Trained at Step 3720: loss [0.03965820475838931]\n",
            "Model Trained at Step 3740: loss [0.039575223067261925]\n",
            "Model Trained at Step 3760: loss [0.03938935947856082]\n",
            "Model Trained at Step 3780: loss [0.03915722925293903]\n",
            "Model Trained at Step 3800: loss [0.039030150894935445]\n",
            "Model Trained at Step 3820: loss [0.039155824018496196]\n",
            "Model Trained at Step 3840: loss [0.039302170230752186]\n",
            "Model Trained at Step 3860: loss [0.03930108343939531]\n",
            "Model Trained at Step 3880: loss [0.038896171773057986]\n",
            "Model Trained at Step 3900: loss [0.03894098786095219]\n",
            "Model Trained at Step 3920: loss [0.039087480041036626]\n",
            "Model Trained at Step 3940: loss [0.03902201066455334]\n",
            "Model Trained at Step 3960: loss [0.03867152706275224]\n",
            "Model Trained at Step 3980: loss [0.03935314157098381]\n",
            "Model Trained at Step 4000: loss [0.03872415834233235]\n",
            "Model Trained at Step 4020: loss [0.039365027703428444]\n",
            "Model Trained at Step 4040: loss [0.03866396565602254]\n",
            "Model Trained at Step 4060: loss [0.039192802819481634]\n",
            "Model Trained at Step 4080: loss [0.03875406758594441]\n",
            "Model Trained at Step 4100: loss [0.0389327547008196]\n",
            "Model Trained at Step 4120: loss [0.03924396944277904]\n",
            "Model Trained at Step 4140: loss [0.038994414989798734]\n",
            "Model Trained at Step 4160: loss [0.03912456835740486]\n",
            "Model Trained at Step 4180: loss [0.03912623279039301]\n",
            "Model Trained at Step 4200: loss [0.03942706619045126]\n",
            "Model Trained at Step 4220: loss [0.03920414862782357]\n",
            "Model Trained at Step 4240: loss [0.03930655129501777]\n",
            "Model Trained at Step 4260: loss [0.0392702899295449]\n",
            "Model Trained at Step 4280: loss [0.0394437712216454]\n",
            "Model Trained at Step 4300: loss [0.039516010370898635]\n",
            "Model Trained at Step 4320: loss [0.039532399731887394]\n",
            "Model Trained at Step 4340: loss [0.03952841844732895]\n",
            "Model Trained at Step 4360: loss [0.03949812787076049]\n",
            "Model Trained at Step 4380: loss [0.03957769230301628]\n",
            "Model Trained at Step 4400: loss [0.03961839534239801]\n",
            "Model Trained at Step 4420: loss [0.039615235998949956]\n",
            "Model Trained at Step 4440: loss [0.03962901486822655]\n",
            "Model Trained at Step 4460: loss [0.03966608947934934]\n",
            "Model Trained at Step 4480: loss [0.03968419659756095]\n",
            "Model Trained at Step 4500: loss [0.039693162998468236]\n",
            "Model Trained at Step 4520: loss [0.03971366356915272]\n",
            "Model Trained at Step 4540: loss [0.03971729394373882]\n",
            "Model Trained at Step 4560: loss [0.03973705037737562]\n",
            "Model Trained at Step 4580: loss [0.03974400818508496]\n",
            "Model Trained at Step 4600: loss [0.03973328717697397]\n",
            "Model Trained at Step 4620: loss [0.03981948330835478]\n",
            "Model Trained at Step 4640: loss [0.03985584443318419]\n",
            "Model Trained at Step 4660: loss [0.03987463935775508]\n",
            "Model Trained at Step 4680: loss [0.03990645392029038]\n",
            "Model Trained at Step 4700: loss [0.03980874944734889]\n",
            "Model Trained at Step 4720: loss [0.0396766074239675]\n",
            "Model Trained at Step 4740: loss [0.03958324431035752]\n",
            "Model Trained at Step 4760: loss [0.039549047073641576]\n",
            "Model Trained at Step 4780: loss [0.039694899372997416]\n",
            "Model Trained at Step 4800: loss [0.03977596458724765]\n",
            "Model Trained at Step 4820: loss [0.03964374655672823]\n",
            "Model Trained at Step 4840: loss [0.039576549567503505]\n",
            "Model Trained at Step 4860: loss [0.03950621146923297]\n",
            "Model Trained at Step 4880: loss [0.03970348338152814]\n",
            "Model Trained at Step 4900: loss [0.03959165855512787]\n",
            "Model Trained at Step 4920: loss [0.03944326684631978]\n",
            "Model Trained at Step 4940: loss [0.03961650863653011]\n",
            "Model Trained at Step 4960: loss [0.0395577326096269]\n",
            "Model Trained at Step 4980: loss [0.03946883823816146]\n",
            "Model Trained at Step 5000: loss [0.039480595605521654]\n",
            "Simulation wave training 2 done\n",
            "Model Trained at Step 20: loss [0.038807807140311804]\n",
            "Model Trained at Step 40: loss [0.03955794821384746]\n",
            "Model Trained at Step 60: loss [0.03955308620140472]\n",
            "Model Trained at Step 80: loss [0.039334907033861014]\n",
            "Model Trained at Step 100: loss [0.038364863155573005]\n",
            "Model Trained at Step 120: loss [0.03798634838707498]\n",
            "Model Trained at Step 140: loss [0.03820889658299287]\n",
            "Model Trained at Step 160: loss [0.03836691459656454]\n",
            "Model Trained at Step 180: loss [0.038613097934964355]\n",
            "Model Trained at Step 200: loss [0.03881526784582749]\n",
            "Model Trained at Step 220: loss [0.039060772402688695]\n",
            "Model Trained at Step 240: loss [0.039276842439987705]\n",
            "Model Trained at Step 260: loss [0.03950599788005904]\n",
            "Model Trained at Step 280: loss [0.03970695421499484]\n",
            "Model Trained at Step 300: loss [0.039897811773192286]\n",
            "Model Trained at Step 320: loss [0.040059004183351524]\n",
            "Model Trained at Step 340: loss [0.0401922930805045]\n",
            "Model Trained at Step 360: loss [0.04029596557692957]\n",
            "Model Trained at Step 380: loss [0.040360883027510897]\n",
            "Model Trained at Step 400: loss [0.04039895493748056]\n",
            "Model Trained at Step 420: loss [0.04040266649130271]\n",
            "Model Trained at Step 440: loss [0.040395088773673864]\n",
            "Model Trained at Step 460: loss [0.040376892389735165]\n",
            "Model Trained at Step 480: loss [0.04027881915596588]\n",
            "Model Trained at Step 500: loss [0.03991417081649542]\n",
            "Model Trained at Step 520: loss [0.039850411727443864]\n",
            "Model Trained at Step 540: loss [0.040195264696586894]\n",
            "Model Trained at Step 560: loss [0.04011322894915361]\n",
            "Model Trained at Step 580: loss [0.040169137035850434]\n",
            "Model Trained at Step 600: loss [0.04020781469671844]\n",
            "Model Trained at Step 620: loss [0.04025357452122154]\n",
            "Model Trained at Step 640: loss [0.04026369958273155]\n",
            "Model Trained at Step 660: loss [0.04025157165964216]\n",
            "Model Trained at Step 680: loss [0.04020986344420517]\n",
            "Model Trained at Step 700: loss [0.04013353370183435]\n",
            "Model Trained at Step 720: loss [0.04001582773111187]\n",
            "Model Trained at Step 740: loss [0.03985533369556454]\n",
            "Model Trained at Step 760: loss [0.03965234259778534]\n",
            "Model Trained at Step 780: loss [0.03942872545832548]\n",
            "Model Trained at Step 800: loss [0.039141084538085444]\n",
            "Model Trained at Step 820: loss [0.03888699495415133]\n",
            "Model Trained at Step 840: loss [0.03858825719811809]\n",
            "Model Trained at Step 860: loss [0.03831546609430719]\n",
            "Model Trained at Step 880: loss [0.03826758851622955]\n",
            "Model Trained at Step 900: loss [0.03863968856357145]\n",
            "Model Trained at Step 920: loss [0.0359867058632016]\n",
            "Model Trained at Step 940: loss [0.03927199184727557]\n",
            "Model Trained at Step 960: loss [0.03927247457326518]\n",
            "Model Trained at Step 980: loss [0.039463910561245076]\n",
            "Model Trained at Step 1000: loss [0.03963770209926369]\n",
            "Model Trained at Step 1020: loss [0.039725655001603466]\n",
            "Model Trained at Step 1040: loss [0.039540168796237334]\n",
            "Model Trained at Step 1060: loss [0.03915714531935364]\n",
            "Model Trained at Step 1080: loss [0.03956677928080424]\n",
            "Model Trained at Step 1100: loss [0.03953867747291432]\n",
            "Model Trained at Step 1120: loss [0.03931204185319083]\n",
            "Model Trained at Step 1140: loss [0.03897158736687529]\n",
            "Model Trained at Step 1160: loss [0.039227763857788436]\n",
            "Model Trained at Step 1180: loss [0.03926999459473211]\n",
            "Model Trained at Step 1200: loss [0.03944744869737882]\n",
            "Model Trained at Step 1220: loss [0.039608688059646915]\n",
            "Model Trained at Step 1240: loss [0.039762195932481835]\n",
            "Model Trained at Step 1260: loss [0.03990062995872285]\n",
            "Model Trained at Step 1280: loss [0.04002073521592773]\n",
            "Model Trained at Step 1300: loss [0.040121472205996704]\n",
            "Model Trained at Step 1320: loss [0.04020319877139556]\n",
            "Model Trained at Step 1340: loss [0.040265406383845036]\n",
            "Model Trained at Step 1360: loss [0.040311498510488944]\n",
            "Model Trained at Step 1380: loss [0.04034247412172877]\n",
            "Model Trained at Step 1400: loss [0.04035592145166097]\n",
            "Model Trained at Step 1420: loss [0.04034648481370523]\n",
            "Model Trained at Step 1440: loss [0.04031298326844893]\n",
            "Model Trained at Step 1460: loss [0.040284588956543146]\n",
            "Model Trained at Step 1480: loss [0.040313804233790086]\n",
            "Model Trained at Step 1500: loss [0.04033887776511215]\n",
            "Model Trained at Step 1520: loss [0.04027280126873576]\n",
            "Model Trained at Step 1540: loss [0.04025818223717443]\n",
            "Model Trained at Step 1560: loss [0.04027978575977963]\n",
            "Model Trained at Step 1580: loss [0.04024035018336612]\n",
            "Model Trained at Step 1600: loss [0.04023819993823978]\n",
            "Model Trained at Step 1620: loss [0.04021317689030849]\n",
            "Model Trained at Step 1640: loss [0.04019079786270872]\n",
            "Model Trained at Step 1660: loss [0.040150517778575]\n",
            "Model Trained at Step 1680: loss [0.04011026181089704]\n",
            "Model Trained at Step 1700: loss [0.04005277281328955]\n",
            "Model Trained at Step 1720: loss [0.039979934752181215]\n",
            "Model Trained at Step 1740: loss [0.03989291882652189]\n",
            "Model Trained at Step 1760: loss [0.03979389173321396]\n",
            "Model Trained at Step 1780: loss [0.03968811029904027]\n",
            "Model Trained at Step 1800: loss [0.039580009679969885]\n",
            "Model Trained at Step 1820: loss [0.039470194023940294]\n",
            "Model Trained at Step 1840: loss [0.03945896134266652]\n",
            "Model Trained at Step 1860: loss [0.039500494404559404]\n",
            "Model Trained at Step 1880: loss [0.03962059653219309]\n",
            "Model Trained at Step 1900: loss [0.03941047191457736]\n",
            "Model Trained at Step 1920: loss [0.03919195069581791]\n",
            "Model Trained at Step 1940: loss [0.03846448824332085]\n",
            "Model Trained at Step 1960: loss [0.039539667070834096]\n",
            "Model Trained at Step 1980: loss [0.03969170021969354]\n",
            "Model Trained at Step 2000: loss [0.03972846471493622]\n",
            "Model Trained at Step 2020: loss [0.0397065874801693]\n",
            "Model Trained at Step 2040: loss [0.03972027910396343]\n",
            "Model Trained at Step 2060: loss [0.03962380490869082]\n",
            "Model Trained at Step 2080: loss [0.03954308498669823]\n",
            "Model Trained at Step 2100: loss [0.0396045726324359]\n",
            "Model Trained at Step 2120: loss [0.039574703689473564]\n",
            "Model Trained at Step 2140: loss [0.039555382179720565]\n",
            "Model Trained at Step 2160: loss [0.039381313570792865]\n",
            "Model Trained at Step 2180: loss [0.03911353906258152]\n",
            "Model Trained at Step 2200: loss [0.0399257649153598]\n",
            "Model Trained at Step 2220: loss [0.03956243749552668]\n",
            "Model Trained at Step 2240: loss [0.03953612956637165]\n",
            "Model Trained at Step 2260: loss [0.03964965716991359]\n",
            "Model Trained at Step 2280: loss [0.03974164029361582]\n",
            "Model Trained at Step 2300: loss [0.03984760448327774]\n",
            "Model Trained at Step 2320: loss [0.03993953430376]\n",
            "Model Trained at Step 2340: loss [0.0400335715323739]\n",
            "Model Trained at Step 2360: loss [0.04011562881955445]\n",
            "Model Trained at Step 2380: loss [0.04015404497809051]\n",
            "Model Trained at Step 2400: loss [0.040147091665773946]\n",
            "Model Trained at Step 2420: loss [0.04014793063378911]\n",
            "Model Trained at Step 2440: loss [0.04023214188135589]\n",
            "Model Trained at Step 2460: loss [0.040329782250631024]\n",
            "Model Trained at Step 2480: loss [0.0402764614278048]\n",
            "Model Trained at Step 2500: loss [0.0402178267516117]\n",
            "Model Trained at Step 2520: loss [0.04031597864208215]\n",
            "Model Trained at Step 2540: loss [0.040282649752878694]\n",
            "Model Trained at Step 2560: loss [0.04021412066590725]\n",
            "Model Trained at Step 2580: loss [0.04026343791610795]\n",
            "Model Trained at Step 2600: loss [0.04019313185868903]\n",
            "Model Trained at Step 2620: loss [0.040173974664556925]\n",
            "Model Trained at Step 2640: loss [0.04013199926285778]\n",
            "Model Trained at Step 2660: loss [0.04007536619777318]\n",
            "Model Trained at Step 2680: loss [0.040039859755333904]\n",
            "Model Trained at Step 2700: loss [0.039989131364328245]\n",
            "Model Trained at Step 2720: loss [0.03994914748326285]\n",
            "Model Trained at Step 2740: loss [0.039933593196398484]\n",
            "Model Trained at Step 2760: loss [0.03993072433736508]\n",
            "Model Trained at Step 2780: loss [0.0399457439372321]\n",
            "Model Trained at Step 2800: loss [0.03997914326455711]\n",
            "Model Trained at Step 2820: loss [0.03996912008715443]\n",
            "Model Trained at Step 2840: loss [0.03982855598325676]\n",
            "Model Trained at Step 2860: loss [0.03952905806250005]\n",
            "Model Trained at Step 2880: loss [0.039670281097074295]\n",
            "Model Trained at Step 2900: loss [0.03932869741091105]\n",
            "Model Trained at Step 2920: loss [0.0397459915201559]\n",
            "Model Trained at Step 2940: loss [0.039872640536261314]\n",
            "Model Trained at Step 2960: loss [0.039961525052975296]\n",
            "Model Trained at Step 2980: loss [0.03988334445114278]\n",
            "Model Trained at Step 3000: loss [0.03992424320682318]\n",
            "Model Trained at Step 3020: loss [0.03983467818100729]\n",
            "Model Trained at Step 3040: loss [0.039682065338765796]\n",
            "Model Trained at Step 3060: loss [0.03956471230915613]\n",
            "Model Trained at Step 3080: loss [0.039727802503113425]\n",
            "Model Trained at Step 3100: loss [0.03956397878039712]\n",
            "Model Trained at Step 3120: loss [0.039803684360068245]\n",
            "Model Trained at Step 3140: loss [0.03978776981268206]\n",
            "Model Trained at Step 3160: loss [0.0398128502693299]\n",
            "Model Trained at Step 3180: loss [0.03981366470806974]\n",
            "Model Trained at Step 3200: loss [0.04000504067984704]\n",
            "Model Trained at Step 3220: loss [0.03995163597649636]\n",
            "Model Trained at Step 3240: loss [0.04000019612138763]\n",
            "Model Trained at Step 3260: loss [0.04005921998805449]\n",
            "Model Trained at Step 3280: loss [0.04008418078737922]\n",
            "Model Trained at Step 3300: loss [0.04011820191231261]\n",
            "Model Trained at Step 3320: loss [0.040165520719011305]\n",
            "Model Trained at Step 3340: loss [0.040197710505572595]\n",
            "Model Trained at Step 3360: loss [0.040220536456659815]\n",
            "Model Trained at Step 3380: loss [0.04024433164015678]\n",
            "Model Trained at Step 3400: loss [0.04027455630371033]\n",
            "Model Trained at Step 3420: loss [0.04030171637246416]\n",
            "Model Trained at Step 3440: loss [0.04030776288644676]\n",
            "Model Trained at Step 3460: loss [0.040307761103813476]\n",
            "Model Trained at Step 3480: loss [0.04033204256342167]\n",
            "Model Trained at Step 3500: loss [0.04034409652644353]\n",
            "Model Trained at Step 3520: loss [0.04033254196076437]\n",
            "Model Trained at Step 3540: loss [0.040337303522670544]\n",
            "Model Trained at Step 3560: loss [0.04034079628857932]\n",
            "Model Trained at Step 3580: loss [0.04031514362519052]\n",
            "Model Trained at Step 3600: loss [0.0403119998124662]\n",
            "Model Trained at Step 3620: loss [0.04029593998450989]\n",
            "Model Trained at Step 3640: loss [0.0402631844072553]\n",
            "Model Trained at Step 3660: loss [0.040246484608104115]\n",
            "Model Trained at Step 3680: loss [0.04020456555628786]\n",
            "Model Trained at Step 3700: loss [0.04019076026409264]\n",
            "Model Trained at Step 3720: loss [0.04016396292358308]\n",
            "Model Trained at Step 3740: loss [0.04016739892056444]\n",
            "Model Trained at Step 3760: loss [0.04019109651263016]\n",
            "Model Trained at Step 3780: loss [0.04021345818547054]\n",
            "Model Trained at Step 3800: loss [0.040201662067962794]\n",
            "Model Trained at Step 3820: loss [0.04011017625679297]\n",
            "Model Trained at Step 3840: loss [0.039982532460342]\n",
            "Model Trained at Step 3860: loss [0.03994698351670378]\n",
            "Model Trained at Step 3880: loss [0.04006048635524299]\n",
            "Model Trained at Step 3900: loss [0.040126252827179694]\n",
            "Model Trained at Step 3920: loss [0.03993672647273684]\n",
            "Model Trained at Step 3940: loss [0.03980925283556256]\n",
            "Model Trained at Step 3960: loss [0.040046926016320275]\n",
            "Model Trained at Step 3980: loss [0.03996118464686685]\n",
            "Model Trained at Step 4000: loss [0.039693214391841564]\n",
            "Model Trained at Step 4020: loss [0.03981675498980168]\n",
            "Model Trained at Step 4040: loss [0.04005654967438503]\n",
            "Model Trained at Step 4060: loss [0.039787897433235714]\n",
            "Model Trained at Step 4080: loss [0.03994511178053499]\n",
            "Model Trained at Step 4100: loss [0.04009958678865314]\n",
            "Model Trained at Step 4120: loss [0.03997512801414878]\n",
            "Model Trained at Step 4140: loss [0.0403196397548484]\n",
            "Model Trained at Step 4160: loss [0.04026558207521639]\n",
            "Model Trained at Step 4180: loss [0.04024888025435959]\n",
            "Model Trained at Step 4200: loss [0.04028301614048661]\n",
            "Model Trained at Step 4220: loss [0.04024446401024041]\n",
            "Model Trained at Step 4240: loss [0.04026521890392016]\n",
            "Model Trained at Step 4260: loss [0.040306530487842725]\n",
            "Model Trained at Step 4280: loss [0.040295623464431315]\n",
            "Model Trained at Step 4300: loss [0.040337853438993085]\n",
            "Model Trained at Step 4320: loss [0.04032496047354105]\n",
            "Model Trained at Step 4340: loss [0.04038265612786548]\n",
            "Model Trained at Step 4360: loss [0.04038604171591233]\n",
            "Model Trained at Step 4380: loss [0.04040860745773396]\n",
            "Model Trained at Step 4400: loss [0.040395311025713095]\n",
            "Model Trained at Step 4420: loss [0.040417091632963975]\n",
            "Model Trained at Step 4440: loss [0.04044387771547118]\n",
            "Model Trained at Step 4460: loss [0.040436983592212636]\n",
            "Model Trained at Step 4480: loss [0.040417279896658345]\n",
            "Model Trained at Step 4500: loss [0.040430655991270346]\n",
            "Model Trained at Step 4520: loss [0.04042612917726172]\n",
            "Model Trained at Step 4540: loss [0.040386238609498346]\n",
            "Model Trained at Step 4560: loss [0.0403859574134051]\n",
            "Model Trained at Step 4580: loss [0.04037926405960265]\n",
            "Model Trained at Step 4600: loss [0.04034929171200439]\n",
            "Model Trained at Step 4620: loss [0.040348931434417534]\n",
            "Model Trained at Step 4640: loss [0.040342623945995235]\n",
            "Model Trained at Step 4660: loss [0.04033043032390939]\n",
            "Model Trained at Step 4680: loss [0.04033620127303346]\n",
            "Model Trained at Step 4700: loss [0.04036712890754456]\n",
            "Model Trained at Step 4720: loss [0.04037380702036473]\n",
            "Model Trained at Step 4740: loss [0.04036222124000224]\n",
            "Model Trained at Step 4760: loss [0.04033858509690018]\n",
            "Model Trained at Step 4780: loss [0.04027263528580332]\n",
            "Model Trained at Step 4800: loss [0.0402563248006442]\n",
            "Model Trained at Step 4820: loss [0.040292073393928776]\n",
            "Model Trained at Step 4840: loss [0.04030568535143488]\n",
            "Model Trained at Step 4860: loss [0.04027584875741115]\n",
            "Model Trained at Step 4880: loss [0.040236788960926854]\n",
            "Model Trained at Step 4900: loss [0.040232376001148575]\n",
            "Model Trained at Step 4920: loss [0.04027270449175896]\n",
            "Model Trained at Step 4940: loss [0.04026768252641778]\n",
            "Model Trained at Step 4960: loss [0.040206859289806114]\n",
            "Model Trained at Step 4980: loss [0.04022610734891096]\n",
            "Model Trained at Step 5000: loss [0.04025291037357288]\n",
            "Simulation wave training 3 done\n",
            "Model Trained at Step 20: loss [0.0391924553176326]\n",
            "Model Trained at Step 40: loss [0.039210309743633286]\n",
            "Model Trained at Step 60: loss [0.039347088335093]\n",
            "Model Trained at Step 80: loss [0.03953532067509561]\n",
            "Model Trained at Step 100: loss [0.03976437010588385]\n",
            "Model Trained at Step 120: loss [0.039974895764988735]\n",
            "Model Trained at Step 140: loss [0.04016491438282197]\n",
            "Model Trained at Step 160: loss [0.040310280824600184]\n",
            "Model Trained at Step 180: loss [0.04035867765906106]\n",
            "Model Trained at Step 200: loss [0.040430746796849916]\n",
            "Model Trained at Step 220: loss [0.04090370889198758]\n",
            "Model Trained at Step 240: loss [0.041262143695833045]\n",
            "Model Trained at Step 260: loss [0.040636491892707606]\n",
            "Model Trained at Step 280: loss [0.04006592997351901]\n",
            "Model Trained at Step 300: loss [0.03973334255648945]\n",
            "Model Trained at Step 320: loss [0.03933178731866053]\n",
            "Model Trained at Step 340: loss [0.038973381631866755]\n",
            "Model Trained at Step 360: loss [0.0387977543849611]\n",
            "Model Trained at Step 380: loss [0.03851724765492095]\n",
            "Model Trained at Step 400: loss [0.03829782152098881]\n",
            "Model Trained at Step 420: loss [0.03886182786678285]\n",
            "Model Trained at Step 440: loss [0.037890958595217086]\n",
            "Model Trained at Step 460: loss [0.03295691985208993]\n",
            "Model Trained at Step 480: loss [0.03932530176857138]\n",
            "Model Trained at Step 500: loss [0.03934909074782067]\n",
            "Model Trained at Step 520: loss [0.03936275436964969]\n",
            "Model Trained at Step 540: loss [0.03896665752018826]\n",
            "Model Trained at Step 560: loss [0.039417146393350776]\n",
            "Model Trained at Step 580: loss [0.0400159254099284]\n",
            "Model Trained at Step 600: loss [0.04047048376152331]\n",
            "Model Trained at Step 620: loss [0.04096705340924132]\n",
            "Model Trained at Step 640: loss [0.0415213909080032]\n",
            "Model Trained at Step 660: loss [0.041924614796713416]\n",
            "Model Trained at Step 680: loss [0.042312128271914606]\n",
            "Model Trained at Step 700: loss [0.04191660493704441]\n",
            "Model Trained at Step 720: loss [0.04086267211619954]\n",
            "Model Trained at Step 740: loss [0.0402488378279664]\n",
            "Model Trained at Step 760: loss [0.0394898994426135]\n",
            "Model Trained at Step 780: loss [0.038585729090010824]\n",
            "Model Trained at Step 800: loss [0.03771534300846018]\n",
            "Model Trained at Step 820: loss [0.03741900216282355]\n",
            "Model Trained at Step 840: loss [0.03688730139378965]\n",
            "Model Trained at Step 860: loss [0.03769029170467638]\n",
            "Model Trained at Step 880: loss [0.037249861222953394]\n",
            "Model Trained at Step 900: loss [0.03904860360900693]\n",
            "Model Trained at Step 920: loss [0.03855338498918143]\n",
            "Model Trained at Step 940: loss [0.03871125857426559]\n",
            "Model Trained at Step 960: loss [0.039167692546152696]\n",
            "Model Trained at Step 980: loss [0.039697127298891646]\n",
            "Model Trained at Step 1000: loss [0.04017850741819867]\n",
            "Model Trained at Step 1020: loss [0.04061040231489188]\n",
            "Model Trained at Step 1040: loss [0.040969702396286424]\n",
            "Model Trained at Step 1060: loss [0.041100369534551824]\n",
            "Model Trained at Step 1080: loss [0.04101127195228689]\n",
            "Model Trained at Step 1100: loss [0.04077234607512277]\n",
            "Model Trained at Step 1120: loss [0.04046284448810326]\n",
            "Model Trained at Step 1140: loss [0.04021476079207571]\n",
            "Model Trained at Step 1160: loss [0.03997308656578283]\n",
            "Model Trained at Step 1180: loss [0.03964846425592385]\n",
            "Model Trained at Step 1200: loss [0.039235411496345914]\n",
            "Model Trained at Step 1220: loss [0.03885475473657978]\n",
            "Model Trained at Step 1240: loss [0.0386788417297438]\n",
            "Model Trained at Step 1260: loss [0.038808755824706234]\n",
            "Model Trained at Step 1280: loss [0.038942581371472995]\n",
            "Model Trained at Step 1300: loss [0.03918933733772848]\n",
            "Model Trained at Step 1320: loss [0.0391917087842077]\n",
            "Model Trained at Step 1340: loss [0.03984367759452049]\n",
            "Model Trained at Step 1360: loss [0.03991834312279528]\n",
            "Model Trained at Step 1380: loss [0.03984489955427515]\n",
            "Model Trained at Step 1400: loss [0.04006639410977538]\n",
            "Model Trained at Step 1420: loss [0.040099859287557854]\n",
            "Model Trained at Step 1440: loss [0.04020016975269458]\n",
            "Model Trained at Step 1460: loss [0.04033134014933072]\n",
            "Model Trained at Step 1480: loss [0.04027148547119781]\n",
            "Model Trained at Step 1500: loss [0.04026952441702826]\n",
            "Model Trained at Step 1520: loss [0.040288642382849474]\n",
            "Model Trained at Step 1540: loss [0.040290311495306846]\n",
            "Model Trained at Step 1560: loss [0.04017905853852779]\n",
            "Model Trained at Step 1580: loss [0.040000425045688695]\n",
            "Model Trained at Step 1600: loss [0.03984281401610583]\n",
            "Model Trained at Step 1620: loss [0.039794935023655956]\n",
            "Model Trained at Step 1640: loss [0.03975650983761642]\n",
            "Model Trained at Step 1660: loss [0.03980167264933746]\n",
            "Model Trained at Step 1680: loss [0.039792234929656216]\n",
            "Model Trained at Step 1700: loss [0.03966406712281508]\n",
            "Model Trained at Step 1720: loss [0.03954923416576614]\n",
            "Model Trained at Step 1740: loss [0.03934945128114383]\n",
            "Model Trained at Step 1760: loss [0.03892835958892403]\n",
            "Model Trained at Step 1780: loss [0.039281324988155036]\n",
            "Model Trained at Step 1800: loss [0.0389225548804362]\n",
            "Model Trained at Step 1820: loss [0.03917942451598247]\n",
            "Model Trained at Step 1840: loss [0.0393127394470066]\n",
            "Model Trained at Step 1860: loss [0.03928966042240707]\n",
            "Model Trained at Step 1880: loss [0.039316769030965884]\n",
            "Model Trained at Step 1900: loss [0.03943765310331064]\n",
            "Model Trained at Step 1920: loss [0.039554764800398566]\n",
            "Model Trained at Step 1940: loss [0.039531440567112594]\n",
            "Model Trained at Step 1960: loss [0.039643440554886535]\n",
            "Model Trained at Step 1980: loss [0.03972218128200532]\n",
            "Model Trained at Step 2000: loss [0.03977865346284677]\n",
            "Model Trained at Step 2020: loss [0.03979326637447444]\n",
            "Model Trained at Step 2040: loss [0.039776119551007946]\n",
            "Model Trained at Step 2060: loss [0.039722924582520844]\n",
            "Model Trained at Step 2080: loss [0.03966673670529689]\n",
            "Model Trained at Step 2100: loss [0.039601291228653746]\n",
            "Model Trained at Step 2120: loss [0.039499837441006096]\n",
            "Model Trained at Step 2140: loss [0.03940199258731251]\n",
            "Model Trained at Step 2160: loss [0.03942193335363947]\n",
            "Model Trained at Step 2180: loss [0.03944297465958051]\n",
            "Model Trained at Step 2200: loss [0.039334524779734675]\n",
            "Model Trained at Step 2220: loss [0.0392172943258415]\n",
            "Model Trained at Step 2240: loss [0.03910083247716038]\n",
            "Model Trained at Step 2260: loss [0.03895806529047841]\n",
            "Model Trained at Step 2280: loss [0.039093255880858616]\n",
            "Model Trained at Step 2300: loss [0.03915701926326919]\n",
            "Model Trained at Step 2320: loss [0.03921758070259441]\n",
            "Model Trained at Step 2340: loss [0.03911554793885853]\n",
            "Model Trained at Step 2360: loss [0.039090482287906576]\n",
            "Model Trained at Step 2380: loss [0.03901633558445945]\n",
            "Model Trained at Step 2400: loss [0.03896518219543839]\n",
            "Model Trained at Step 2420: loss [0.03885439365629428]\n",
            "Model Trained at Step 2440: loss [0.039017647488017115]\n",
            "Model Trained at Step 2460: loss [0.03899877577474818]\n",
            "Model Trained at Step 2480: loss [0.039122013424353855]\n",
            "Model Trained at Step 2500: loss [0.03909659372883271]\n",
            "Model Trained at Step 2520: loss [0.039119083058549456]\n",
            "Model Trained at Step 2540: loss [0.039164913293796795]\n",
            "Model Trained at Step 2560: loss [0.03931119427631237]\n",
            "Model Trained at Step 2580: loss [0.0395326657283886]\n",
            "Model Trained at Step 2600: loss [0.039701237913070016]\n",
            "Model Trained at Step 2620: loss [0.039936939341590275]\n",
            "Model Trained at Step 2640: loss [0.04007158057972304]\n",
            "Model Trained at Step 2660: loss [0.04021949537218624]\n",
            "Model Trained at Step 2680: loss [0.04031551190549139]\n",
            "Model Trained at Step 2700: loss [0.04028365146266576]\n",
            "Model Trained at Step 2720: loss [0.04015147549547325]\n",
            "Model Trained at Step 2740: loss [0.04001987353821616]\n",
            "Model Trained at Step 2760: loss [0.04005030950362558]\n",
            "Model Trained at Step 2780: loss [0.040111097703219065]\n",
            "Model Trained at Step 2800: loss [0.04000892652877222]\n",
            "Model Trained at Step 2820: loss [0.039931086255222806]\n",
            "Model Trained at Step 2840: loss [0.03999749231011953]\n",
            "Model Trained at Step 2860: loss [0.03993745966329767]\n",
            "Model Trained at Step 2880: loss [0.039866583558215006]\n",
            "Model Trained at Step 2900: loss [0.03979228714640519]\n",
            "Model Trained at Step 2920: loss [0.03982378325128737]\n",
            "Model Trained at Step 2940: loss [0.03993914620832296]\n",
            "Model Trained at Step 2960: loss [0.040040033522642585]\n",
            "Model Trained at Step 2980: loss [0.04002685526505113]\n",
            "Model Trained at Step 3000: loss [0.03987483780552128]\n",
            "Model Trained at Step 3020: loss [0.03976557500504993]\n",
            "Model Trained at Step 3040: loss [0.03983060659310322]\n",
            "Model Trained at Step 3060: loss [0.039647685490225494]\n",
            "Model Trained at Step 3080: loss [0.03960085940971882]\n",
            "Model Trained at Step 3100: loss [0.03966422163031737]\n",
            "Model Trained at Step 3120: loss [0.03966654030899673]\n",
            "Model Trained at Step 3140: loss [0.03969664749165215]\n",
            "Model Trained at Step 3160: loss [0.03969314574967432]\n",
            "Model Trained at Step 3180: loss [0.03976687494944791]\n",
            "Model Trained at Step 3200: loss [0.03971037342652061]\n",
            "Model Trained at Step 3220: loss [0.03982540805947869]\n",
            "Model Trained at Step 3240: loss [0.039880483244957486]\n",
            "Model Trained at Step 3260: loss [0.0399586632361195]\n",
            "Model Trained at Step 3280: loss [0.039957457757474994]\n",
            "Model Trained at Step 3300: loss [0.040057697649137206]\n",
            "Model Trained at Step 3320: loss [0.04015174685757375]\n",
            "Model Trained at Step 3340: loss [0.040165510887818744]\n",
            "Model Trained at Step 3360: loss [0.04006950663737154]\n",
            "Model Trained at Step 3380: loss [0.03993171636723232]\n",
            "Model Trained at Step 3400: loss [0.0398028886985732]\n",
            "Model Trained at Step 3420: loss [0.03981505920077917]\n",
            "Model Trained at Step 3440: loss [0.039781460499399346]\n",
            "Model Trained at Step 3460: loss [0.0397251799761707]\n",
            "Model Trained at Step 3480: loss [0.039708364673896324]\n",
            "Model Trained at Step 3500: loss [0.03974441160148506]\n",
            "Model Trained at Step 3520: loss [0.03958349013457643]\n",
            "Model Trained at Step 3540: loss [0.03945671893025264]\n",
            "Model Trained at Step 3560: loss [0.03935696627636951]\n",
            "Model Trained at Step 3580: loss [0.039388194906931906]\n",
            "Model Trained at Step 3600: loss [0.0394390117007222]\n",
            "Model Trained at Step 3620: loss [0.03960182039048007]\n",
            "Model Trained at Step 3640: loss [0.03962155107933029]\n",
            "Model Trained at Step 3660: loss [0.03954712316498405]\n",
            "Model Trained at Step 3680: loss [0.039378255086227595]\n",
            "Model Trained at Step 3700: loss [0.03946898200729019]\n",
            "Model Trained at Step 3720: loss [0.039564015690290374]\n",
            "Model Trained at Step 3740: loss [0.039574820886807714]\n",
            "Model Trained at Step 3760: loss [0.03960538405415826]\n",
            "Model Trained at Step 3780: loss [0.03958114382290556]\n",
            "Model Trained at Step 3800: loss [0.039588207050045884]\n",
            "Model Trained at Step 3820: loss [0.03960982286355761]\n",
            "Model Trained at Step 3840: loss [0.03957257987094703]\n",
            "Model Trained at Step 3860: loss [0.039694287351268016]\n",
            "Model Trained at Step 3880: loss [0.03978102526586872]\n",
            "Model Trained at Step 3900: loss [0.039944498723229166]\n",
            "Model Trained at Step 3920: loss [0.040034842920818436]\n",
            "Model Trained at Step 3940: loss [0.040037491201209495]\n",
            "Model Trained at Step 3960: loss [0.040108227928650785]\n",
            "Model Trained at Step 3980: loss [0.04017191136891045]\n",
            "Model Trained at Step 4000: loss [0.040127429581126806]\n",
            "Model Trained at Step 4020: loss [0.0400495761148418]\n",
            "Model Trained at Step 4040: loss [0.039964013729761695]\n",
            "Model Trained at Step 4060: loss [0.039861745525522914]\n",
            "Model Trained at Step 4080: loss [0.039832341236402086]\n",
            "Model Trained at Step 4100: loss [0.03984569352299972]\n",
            "Model Trained at Step 4120: loss [0.03982253264939241]\n",
            "Model Trained at Step 4140: loss [0.039665948728047074]\n",
            "Model Trained at Step 4160: loss [0.03955658460839428]\n",
            "Model Trained at Step 4180: loss [0.03952016696506757]\n",
            "Model Trained at Step 4200: loss [0.039541851235278064]\n",
            "Model Trained at Step 4220: loss [0.03956626458148303]\n",
            "Model Trained at Step 4240: loss [0.03963337679975874]\n",
            "Model Trained at Step 4260: loss [0.03977430734880018]\n",
            "Model Trained at Step 4280: loss [0.03986179111237289]\n",
            "Model Trained at Step 4300: loss [0.0398030273089912]\n",
            "Model Trained at Step 4320: loss [0.03977376070731955]\n",
            "Model Trained at Step 4340: loss [0.03963061335102504]\n",
            "Model Trained at Step 4360: loss [0.039787733354167465]\n",
            "Model Trained at Step 4380: loss [0.03987628695961127]\n",
            "Model Trained at Step 4400: loss [0.039842369339955525]\n",
            "Model Trained at Step 4420: loss [0.039820862168031754]\n",
            "Model Trained at Step 4440: loss [0.039879571549577206]\n",
            "Model Trained at Step 4460: loss [0.03991246706319659]\n",
            "Model Trained at Step 4480: loss [0.03986769646052067]\n",
            "Model Trained at Step 4500: loss [0.03990039212497386]\n",
            "Model Trained at Step 4520: loss [0.03991980893364157]\n",
            "Model Trained at Step 4540: loss [0.03995945789331798]\n",
            "Model Trained at Step 4560: loss [0.0400609824592928]\n",
            "Model Trained at Step 4580: loss [0.0401436854899011]\n",
            "Model Trained at Step 4600: loss [0.04021139015671356]\n",
            "Model Trained at Step 4620: loss [0.04021496224500762]\n",
            "Model Trained at Step 4640: loss [0.040208767673748536]\n",
            "Model Trained at Step 4660: loss [0.040069051467353275]\n",
            "Model Trained at Step 4680: loss [0.03993209220344891]\n",
            "Model Trained at Step 4700: loss [0.039714190731276286]\n",
            "Model Trained at Step 4720: loss [0.039686868384636226]\n",
            "Model Trained at Step 4740: loss [0.03973123505415568]\n",
            "Model Trained at Step 4760: loss [0.0397759446393958]\n",
            "Model Trained at Step 4780: loss [0.03967354133698223]\n",
            "Model Trained at Step 4800: loss [0.039493100334355206]\n",
            "Model Trained at Step 4820: loss [0.03944076591085779]\n",
            "Model Trained at Step 4840: loss [0.03945405863185159]\n",
            "Model Trained at Step 4860: loss [0.039539370237988425]\n",
            "Model Trained at Step 4880: loss [0.039623000864518844]\n",
            "Model Trained at Step 4900: loss [0.03980100948458644]\n",
            "Model Trained at Step 4920: loss [0.03994188822066874]\n",
            "Model Trained at Step 4940: loss [0.03990822628757721]\n",
            "Model Trained at Step 4960: loss [0.039868780391172615]\n",
            "Model Trained at Step 4980: loss [0.03985721121988044]\n",
            "Model Trained at Step 5000: loss [0.03983046777109704]\n",
            "Simulation wave training 4 done\n",
            "Model Trained at Step 20: loss [0.036748758123958974]\n",
            "Model Trained at Step 40: loss [0.03850256226469358]\n",
            "Model Trained at Step 60: loss [0.038782062662989565]\n",
            "Model Trained at Step 80: loss [0.03851994206369327]\n",
            "Model Trained at Step 100: loss [0.03764084548025927]\n",
            "Model Trained at Step 120: loss [0.034587908273069845]\n",
            "Model Trained at Step 140: loss [0.034782058516200164]\n",
            "Model Trained at Step 160: loss [0.03541441297549576]\n",
            "Model Trained at Step 180: loss [0.03607344869705352]\n",
            "Model Trained at Step 200: loss [0.03673945721438772]\n",
            "Model Trained at Step 220: loss [0.037397182012142906]\n",
            "Model Trained at Step 240: loss [0.038036604449371195]\n",
            "Model Trained at Step 260: loss [0.03862535559310909]\n",
            "Model Trained at Step 280: loss [0.0391451527454]\n",
            "Model Trained at Step 300: loss [0.03958333129233226]\n",
            "Model Trained at Step 320: loss [0.039921323560550136]\n",
            "Model Trained at Step 340: loss [0.04014728597464111]\n",
            "Model Trained at Step 360: loss [0.040257995252524666]\n",
            "Model Trained at Step 380: loss [0.04025326036629538]\n",
            "Model Trained at Step 400: loss [0.04014323827614933]\n",
            "Model Trained at Step 420: loss [0.039951979383659415]\n",
            "Model Trained at Step 440: loss [0.03971517662243266]\n",
            "Model Trained at Step 460: loss [0.03948525569746577]\n",
            "Model Trained at Step 480: loss [0.0392661272508434]\n",
            "Model Trained at Step 500: loss [0.03879580156534988]\n",
            "Model Trained at Step 520: loss [0.03842196937207947]\n",
            "Model Trained at Step 540: loss [0.03874491233295086]\n",
            "Model Trained at Step 560: loss [0.03898900975700677]\n",
            "Model Trained at Step 580: loss [0.03915071859703264]\n",
            "Model Trained at Step 600: loss [0.039420594190472495]\n",
            "Model Trained at Step 620: loss [0.039620547828562905]\n",
            "Model Trained at Step 640: loss [0.03975124308633778]\n",
            "Model Trained at Step 660: loss [0.039803232323875885]\n",
            "Model Trained at Step 680: loss [0.03975519109779703]\n",
            "Model Trained at Step 700: loss [0.03959273616103175]\n",
            "Model Trained at Step 720: loss [0.03930317053391442]\n",
            "Model Trained at Step 740: loss [0.03888791998985308]\n",
            "Model Trained at Step 760: loss [0.03834280580274538]\n",
            "Model Trained at Step 780: loss [0.037679510742940785]\n",
            "Model Trained at Step 800: loss [0.036974845853553616]\n",
            "Model Trained at Step 820: loss [0.03611496768317539]\n",
            "Model Trained at Step 840: loss [0.03543148680605126]\n",
            "Model Trained at Step 860: loss [0.034826031637004914]\n",
            "Model Trained at Step 880: loss [0.034562849341088185]\n",
            "Model Trained at Step 900: loss [0.03526694648608984]\n",
            "Model Trained at Step 920: loss [0.03463142426986844]\n",
            "Model Trained at Step 940: loss [0.03768183043614676]\n",
            "Model Trained at Step 960: loss [0.03837331838413403]\n",
            "Model Trained at Step 980: loss [0.03869798131051119]\n",
            "Model Trained at Step 1000: loss [0.038868232985071166]\n",
            "Model Trained at Step 1020: loss [0.03893449042887026]\n",
            "Model Trained at Step 1040: loss [0.03844846149243669]\n",
            "Model Trained at Step 1060: loss [0.03735206629539453]\n",
            "Model Trained at Step 1080: loss [0.03844002481625382]\n",
            "Model Trained at Step 1100: loss [0.03840646590011432]\n",
            "Model Trained at Step 1120: loss [0.03770970432331019]\n",
            "Model Trained at Step 1140: loss [0.036602313738779715]\n",
            "Model Trained at Step 1160: loss [0.03739061211230769]\n",
            "Model Trained at Step 1180: loss [0.03753752781053918]\n",
            "Model Trained at Step 1200: loss [0.03801924720355902]\n",
            "Model Trained at Step 1220: loss [0.03848136154685157]\n",
            "Model Trained at Step 1240: loss [0.03884354680394307]\n",
            "Model Trained at Step 1260: loss [0.03918913634699348]\n",
            "Model Trained at Step 1280: loss [0.03945527025152814]\n",
            "Model Trained at Step 1300: loss [0.039682086517586965]\n",
            "Model Trained at Step 1320: loss [0.039836672040851326]\n",
            "Model Trained at Step 1340: loss [0.03994723322210885]\n",
            "Model Trained at Step 1360: loss [0.04000223617306402]\n",
            "Model Trained at Step 1380: loss [0.040025575750748596]\n",
            "Model Trained at Step 1400: loss [0.04001188073490787]\n",
            "Model Trained at Step 1420: loss [0.03997793415043801]\n",
            "Model Trained at Step 1440: loss [0.039889737623794885]\n",
            "Model Trained at Step 1460: loss [0.03975223996714165]\n",
            "Model Trained at Step 1480: loss [0.039621813701200256]\n",
            "Model Trained at Step 1500: loss [0.039643019336854046]\n",
            "Model Trained at Step 1520: loss [0.03960274029950905]\n",
            "Model Trained at Step 1540: loss [0.03948043747587334]\n",
            "Model Trained at Step 1560: loss [0.03949492598830241]\n",
            "Model Trained at Step 1580: loss [0.039506092133326606]\n",
            "Model Trained at Step 1600: loss [0.03947283451033821]\n",
            "Model Trained at Step 1620: loss [0.03950660480445226]\n",
            "Model Trained at Step 1640: loss [0.039468868009119376]\n",
            "Model Trained at Step 1660: loss [0.03945896442410894]\n",
            "Model Trained at Step 1680: loss [0.039387985523233374]\n",
            "Model Trained at Step 1700: loss [0.03931073052676444]\n",
            "Model Trained at Step 1720: loss [0.03917409561293479]\n",
            "Model Trained at Step 1740: loss [0.03899443330059034]\n",
            "Model Trained at Step 1760: loss [0.03879303316714876]\n",
            "Model Trained at Step 1780: loss [0.03855094605081443]\n",
            "Model Trained at Step 1800: loss [0.0383178202048976]\n",
            "Model Trained at Step 1820: loss [0.03795730040476138]\n",
            "Model Trained at Step 1840: loss [0.03770468759151877]\n",
            "Model Trained at Step 1860: loss [0.038371928156043705]\n",
            "Model Trained at Step 1880: loss [0.03795853760308464]\n",
            "Model Trained at Step 1900: loss [0.03893518709901669]\n",
            "Model Trained at Step 1920: loss [0.03870999241595267]\n",
            "Model Trained at Step 1940: loss [0.03857245380982538]\n",
            "Model Trained at Step 1960: loss [0.038733612410247226]\n",
            "Model Trained at Step 1980: loss [0.03904880794555244]\n",
            "Model Trained at Step 2000: loss [0.03921289575448772]\n",
            "Model Trained at Step 2020: loss [0.039208580697835616]\n",
            "Model Trained at Step 2040: loss [0.03913853799525411]\n",
            "Model Trained at Step 2060: loss [0.03883385994946674]\n",
            "Model Trained at Step 2080: loss [0.03861582137868645]\n",
            "Model Trained at Step 2100: loss [0.03879605890181741]\n",
            "Model Trained at Step 2120: loss [0.03881533510909267]\n",
            "Model Trained at Step 2140: loss [0.038734971267804945]\n",
            "Model Trained at Step 2160: loss [0.03847718798713454]\n",
            "Model Trained at Step 2180: loss [0.03778824970055156]\n",
            "Model Trained at Step 2200: loss [0.038839512258287814]\n",
            "Model Trained at Step 2220: loss [0.03840779256711187]\n",
            "Model Trained at Step 2240: loss [0.03864921003161011]\n",
            "Model Trained at Step 2260: loss [0.03889829378257782]\n",
            "Model Trained at Step 2280: loss [0.03898252152752932]\n",
            "Model Trained at Step 2300: loss [0.039167135534773176]\n",
            "Model Trained at Step 2320: loss [0.03931984116534083]\n",
            "Model Trained at Step 2340: loss [0.03945634539957751]\n",
            "Model Trained at Step 2360: loss [0.03957751495100836]\n",
            "Model Trained at Step 2380: loss [0.03966505790375226]\n",
            "Model Trained at Step 2400: loss [0.03968669768147333]\n",
            "Model Trained at Step 2420: loss [0.039662090319107474]\n",
            "Model Trained at Step 2440: loss [0.039679157974237336]\n",
            "Model Trained at Step 2460: loss [0.03977774462954543]\n",
            "Model Trained at Step 2480: loss [0.03979205286403883]\n",
            "Model Trained at Step 2500: loss [0.039667913788218434]\n",
            "Model Trained at Step 2520: loss [0.0396850646240012]\n",
            "Model Trained at Step 2540: loss [0.03971867923517487]\n",
            "Model Trained at Step 2560: loss [0.03959823720511183]\n",
            "Model Trained at Step 2580: loss [0.039595942331262304]\n",
            "Model Trained at Step 2600: loss [0.039556966529312454]\n",
            "Model Trained at Step 2620: loss [0.03947460285156849]\n",
            "Model Trained at Step 2640: loss [0.03943805389280667]\n",
            "Model Trained at Step 2660: loss [0.03933370815850974]\n",
            "Model Trained at Step 2680: loss [0.039270501223689205]\n",
            "Model Trained at Step 2700: loss [0.03915291142401781]\n",
            "Model Trained at Step 2720: loss [0.039126133150338686]\n",
            "Model Trained at Step 2740: loss [0.03901209808277846]\n",
            "Model Trained at Step 2760: loss [0.03893752496446185]\n",
            "Model Trained at Step 2780: loss [0.03886887381118836]\n",
            "Model Trained at Step 2800: loss [0.03882284623577041]\n",
            "Model Trained at Step 2820: loss [0.038868937865533804]\n",
            "Model Trained at Step 2840: loss [0.03913727235023799]\n",
            "Model Trained at Step 2860: loss [0.03903804997888449]\n",
            "Model Trained at Step 2880: loss [0.03865840414307628]\n",
            "Model Trained at Step 2900: loss [0.03790595171718206]\n",
            "Model Trained at Step 2920: loss [0.03957262317474173]\n",
            "Model Trained at Step 2940: loss [0.039454093057072336]\n",
            "Model Trained at Step 2960: loss [0.03931962695110008]\n",
            "Model Trained at Step 2980: loss [0.03912990915794521]\n",
            "Model Trained at Step 3000: loss [0.039088836949244074]\n",
            "Model Trained at Step 3020: loss [0.03901122517809528]\n",
            "Model Trained at Step 3040: loss [0.037062492210973005]\n",
            "Model Trained at Step 3060: loss [0.03982813954416501]\n",
            "Model Trained at Step 3080: loss [0.039430036913413964]\n",
            "Model Trained at Step 3100: loss [0.03866428979722434]\n",
            "Model Trained at Step 3120: loss [0.03825217651113052]\n",
            "Model Trained at Step 3140: loss [0.038641972372804013]\n",
            "Model Trained at Step 3160: loss [0.038722715240252434]\n",
            "Model Trained at Step 3180: loss [0.03866245316172233]\n",
            "Model Trained at Step 3200: loss [0.0391768654267442]\n",
            "Model Trained at Step 3220: loss [0.03894469048691873]\n",
            "Model Trained at Step 3240: loss [0.03915166190452586]\n",
            "Model Trained at Step 3260: loss [0.03927258592110204]\n",
            "Model Trained at Step 3280: loss [0.039220108198909844]\n",
            "Model Trained at Step 3300: loss [0.039357534175107366]\n",
            "Model Trained at Step 3320: loss [0.039433894179159414]\n",
            "Model Trained at Step 3340: loss [0.03950871049214676]\n",
            "Model Trained at Step 3360: loss [0.039575101793879554]\n",
            "Model Trained at Step 3380: loss [0.039581684437765614]\n",
            "Model Trained at Step 3400: loss [0.03964677631263608]\n",
            "Model Trained at Step 3420: loss [0.039705476869858994]\n",
            "Model Trained at Step 3440: loss [0.03973643493917582]\n",
            "Model Trained at Step 3460: loss [0.03973143496924594]\n",
            "Model Trained at Step 3480: loss [0.039762462538248756]\n",
            "Model Trained at Step 3500: loss [0.03977206026116639]\n",
            "Model Trained at Step 3520: loss [0.03975086119701089]\n",
            "Model Trained at Step 3540: loss [0.03973815487266926]\n",
            "Model Trained at Step 3560: loss [0.03974804806847504]\n",
            "Model Trained at Step 3580: loss [0.03970394643817166]\n",
            "Model Trained at Step 3600: loss [0.039679733766166944]\n",
            "Model Trained at Step 3620: loss [0.03963467763836062]\n",
            "Model Trained at Step 3640: loss [0.03956975372840741]\n",
            "Model Trained at Step 3660: loss [0.039522503737618765]\n",
            "Model Trained at Step 3680: loss [0.03945736592964033]\n",
            "Model Trained at Step 3700: loss [0.03939081369132254]\n",
            "Model Trained at Step 3720: loss [0.03931935506849553]\n",
            "Model Trained at Step 3740: loss [0.0392300744909432]\n",
            "Model Trained at Step 3760: loss [0.039242165766568755]\n",
            "Model Trained at Step 3780: loss [0.03925218162058501]\n",
            "Model Trained at Step 3800: loss [0.03924526545861658]\n",
            "Model Trained at Step 3820: loss [0.039153178545322624]\n",
            "Model Trained at Step 3840: loss [0.03899422520887915]\n",
            "Model Trained at Step 3860: loss [0.038852915854049125]\n",
            "Model Trained at Step 3880: loss [0.038881321234370714]\n",
            "Model Trained at Step 3900: loss [0.03895123473894985]\n",
            "Model Trained at Step 3920: loss [0.03887395027693262]\n",
            "Model Trained at Step 3940: loss [0.03867747977958848]\n",
            "Model Trained at Step 3960: loss [0.03854027404486656]\n",
            "Model Trained at Step 3980: loss [0.039325463649974024]\n",
            "Model Trained at Step 4000: loss [0.038722813563709366]\n",
            "Model Trained at Step 4020: loss [0.03937879844733967]\n",
            "Model Trained at Step 4040: loss [0.038541168746819544]\n",
            "Model Trained at Step 4060: loss [0.03940058282159066]\n",
            "Model Trained at Step 4080: loss [0.038604248595349645]\n",
            "Model Trained at Step 4100: loss [0.039123861419695406]\n",
            "Model Trained at Step 4120: loss [0.03869030334179607]\n",
            "Model Trained at Step 4140: loss [0.03951099451449584]\n",
            "Model Trained at Step 4160: loss [0.03875454659078106]\n",
            "Model Trained at Step 4180: loss [0.03944208962946095]\n",
            "Model Trained at Step 4200: loss [0.038915333756530626]\n",
            "Model Trained at Step 4220: loss [0.03889158910592073]\n",
            "Model Trained at Step 4240: loss [0.03935009010951991]\n",
            "Model Trained at Step 4260: loss [0.039111620275833384]\n",
            "Model Trained at Step 4280: loss [0.03936555236892989]\n",
            "Model Trained at Step 4300: loss [0.03931889090985533]\n",
            "Model Trained at Step 4320: loss [0.039327028465985556]\n",
            "Model Trained at Step 4340: loss [0.03936485410116679]\n",
            "Model Trained at Step 4360: loss [0.03948556116777273]\n",
            "Model Trained at Step 4380: loss [0.03950053215948761]\n",
            "Model Trained at Step 4400: loss [0.03950313689025974]\n",
            "Model Trained at Step 4420: loss [0.03951364526625719]\n",
            "Model Trained at Step 4440: loss [0.03954113502522889]\n",
            "Model Trained at Step 4460: loss [0.03958439083743463]\n",
            "Model Trained at Step 4480: loss [0.03957114531761213]\n",
            "Model Trained at Step 4500: loss [0.03955390711263174]\n",
            "Model Trained at Step 4520: loss [0.03958461475839274]\n",
            "Model Trained at Step 4540: loss [0.03955685015643295]\n",
            "Model Trained at Step 4560: loss [0.03953089167099498]\n",
            "Model Trained at Step 4580: loss [0.039546890471017455]\n",
            "Model Trained at Step 4600: loss [0.0394662573379931]\n",
            "Model Trained at Step 4620: loss [0.039432067433923315]\n",
            "Model Trained at Step 4640: loss [0.03941720854420807]\n",
            "Model Trained at Step 4660: loss [0.03932475916210563]\n",
            "Model Trained at Step 4680: loss [0.03927416623774989]\n",
            "Model Trained at Step 4700: loss [0.03924868305194307]\n",
            "Model Trained at Step 4720: loss [0.03920412170398989]\n",
            "Model Trained at Step 4740: loss [0.039244269341223804]\n",
            "Model Trained at Step 4760: loss [0.03929486826559962]\n",
            "Model Trained at Step 4780: loss [0.039161220687126054]\n",
            "Model Trained at Step 4800: loss [0.03902660330075568]\n",
            "Model Trained at Step 4820: loss [0.03898346022633082]\n",
            "Model Trained at Step 4840: loss [0.0390496680366868]\n",
            "Model Trained at Step 4860: loss [0.039026009903142804]\n",
            "Model Trained at Step 4880: loss [0.03895283871728617]\n",
            "Model Trained at Step 4900: loss [0.038809497684684446]\n",
            "Model Trained at Step 4920: loss [0.03887505229230622]\n",
            "Model Trained at Step 4940: loss [0.038992524167194356]\n",
            "Model Trained at Step 4960: loss [0.038726777602301414]\n",
            "Model Trained at Step 4980: loss [0.03907673082036027]\n",
            "Model Trained at Step 5000: loss [0.03877252244499688]\n",
            "Simulation wave training 5 done\n",
            "Model Trained at Step 20: loss [0.03508091704194983]\n",
            "Model Trained at Step 40: loss [0.03782772604242846]\n",
            "Model Trained at Step 60: loss [0.038414121915658635]\n",
            "Model Trained at Step 80: loss [0.03850576312869574]\n",
            "Model Trained at Step 100: loss [0.03826697566028646]\n",
            "Model Trained at Step 120: loss [0.03641094260256694]\n",
            "Model Trained at Step 140: loss [0.0361778485575428]\n",
            "Model Trained at Step 160: loss [0.03584514391357168]\n",
            "Model Trained at Step 180: loss [0.035925919962499234]\n",
            "Model Trained at Step 200: loss [0.036062497183673825]\n",
            "Model Trained at Step 220: loss [0.03626526247977942]\n",
            "Model Trained at Step 240: loss [0.03641716450486333]\n",
            "Model Trained at Step 260: loss [0.036608880386237296]\n",
            "Model Trained at Step 280: loss [0.03680264391253717]\n",
            "Model Trained at Step 300: loss [0.03700885854929178]\n",
            "Model Trained at Step 320: loss [0.0372060079951563]\n",
            "Model Trained at Step 340: loss [0.03742165121031533]\n",
            "Model Trained at Step 360: loss [0.03765581639747943]\n",
            "Model Trained at Step 380: loss [0.03789165437663673]\n",
            "Model Trained at Step 400: loss [0.038124662841186055]\n",
            "Model Trained at Step 420: loss [0.03836274371884925]\n",
            "Model Trained at Step 440: loss [0.03861197620032701]\n",
            "Model Trained at Step 460: loss [0.03885468480926001]\n",
            "Model Trained at Step 480: loss [0.03909475825691633]\n",
            "Model Trained at Step 500: loss [0.0393314435237264]\n",
            "Model Trained at Step 520: loss [0.039566801533272015]\n",
            "Model Trained at Step 540: loss [0.03978961149198621]\n",
            "Model Trained at Step 560: loss [0.04000219896779879]\n",
            "Model Trained at Step 580: loss [0.040202308095265676]\n",
            "Model Trained at Step 600: loss [0.04038499618208369]\n",
            "Model Trained at Step 620: loss [0.040550596950319456]\n",
            "Model Trained at Step 640: loss [0.04069553068672867]\n",
            "Model Trained at Step 660: loss [0.040849837260996384]\n",
            "Model Trained at Step 680: loss [0.0410783608997214]\n",
            "Model Trained at Step 700: loss [0.041506315229963835]\n",
            "Model Trained at Step 720: loss [0.04223716419006329]\n",
            "Model Trained at Step 740: loss [0.043500447549496536]\n",
            "Model Trained at Step 760: loss [0.045556569179566596]\n",
            "Model Trained at Step 780: loss [0.04860518679532241]\n",
            "Model Trained at Step 800: loss [0.05208424112945522]\n",
            "Model Trained at Step 820: loss [0.05472999620525101]\n",
            "Model Trained at Step 840: loss [0.05546956205262081]\n",
            "Model Trained at Step 860: loss [0.05472069627865865]\n",
            "Model Trained at Step 880: loss [0.0537287816842526]\n",
            "Model Trained at Step 900: loss [0.0529962571757248]\n",
            "Model Trained at Step 920: loss [0.052299487004963674]\n",
            "Model Trained at Step 940: loss [0.051523462562052716]\n",
            "Model Trained at Step 960: loss [0.050696666237806706]\n",
            "Model Trained at Step 980: loss [0.049803889214223795]\n",
            "Model Trained at Step 1000: loss [0.04881149670009437]\n",
            "Model Trained at Step 1020: loss [0.0477739565035149]\n",
            "Model Trained at Step 1040: loss [0.04671364849370776]\n",
            "Model Trained at Step 1060: loss [0.04561462572267557]\n",
            "Model Trained at Step 1080: loss [0.04446317500408421]\n",
            "Model Trained at Step 1100: loss [0.04330030772809438]\n",
            "Model Trained at Step 1120: loss [0.0421549871247124]\n",
            "Model Trained at Step 1140: loss [0.041003814574933524]\n",
            "Model Trained at Step 1160: loss [0.03987228927697702]\n",
            "Model Trained at Step 1180: loss [0.038768189732291986]\n",
            "Model Trained at Step 1200: loss [0.03770517689906587]\n",
            "Model Trained at Step 1220: loss [0.036675521725949047]\n",
            "Model Trained at Step 1240: loss [0.035733387411993764]\n",
            "Model Trained at Step 1260: loss [0.034891695563753114]\n",
            "Model Trained at Step 1280: loss [0.03391087936131204]\n",
            "Model Trained at Step 1300: loss [0.033397318664945594]\n",
            "Model Trained at Step 1320: loss [0.03265375251848091]\n",
            "Model Trained at Step 1340: loss [0.032029136205491936]\n",
            "Model Trained at Step 1360: loss [0.031416620483035056]\n",
            "Model Trained at Step 1380: loss [0.03101927304984433]\n",
            "Model Trained at Step 1400: loss [0.032422409809050146]\n",
            "Model Trained at Step 1420: loss [0.03363408658876413]\n",
            "Model Trained at Step 1440: loss [0.03429942136801485]\n",
            "Model Trained at Step 1460: loss [0.03157453049450551]\n",
            "Model Trained at Step 1480: loss [0.03558173112685344]\n",
            "Model Trained at Step 1500: loss [0.03790561401032755]\n",
            "Model Trained at Step 1520: loss [0.039286995825639735]\n",
            "Model Trained at Step 1540: loss [0.03985462272016914]\n",
            "Model Trained at Step 1560: loss [0.040147347153100706]\n",
            "Model Trained at Step 1580: loss [0.04031683319861419]\n",
            "Model Trained at Step 1600: loss [0.04041579561706081]\n",
            "Model Trained at Step 1620: loss [0.04046923458191374]\n",
            "Model Trained at Step 1640: loss [0.04049723291573219]\n",
            "Model Trained at Step 1660: loss [0.04052074394848877]\n",
            "Model Trained at Step 1680: loss [0.040559775883246156]\n",
            "Model Trained at Step 1700: loss [0.04062864015123589]\n",
            "Model Trained at Step 1720: loss [0.04073975264038806]\n",
            "Model Trained at Step 1740: loss [0.040923500762186406]\n",
            "Model Trained at Step 1760: loss [0.04130330508474135]\n",
            "Model Trained at Step 1780: loss [0.04293816150184445]\n",
            "Model Trained at Step 1800: loss [0.04484413864059048]\n",
            "Model Trained at Step 1820: loss [0.045131980164546226]\n",
            "Model Trained at Step 1840: loss [0.04536257943189366]\n",
            "Model Trained at Step 1860: loss [0.04561255665686558]\n",
            "Model Trained at Step 1880: loss [0.045875989774821865]\n",
            "Model Trained at Step 1900: loss [0.046161472714759656]\n",
            "Model Trained at Step 1920: loss [0.046433088134391474]\n",
            "Model Trained at Step 1940: loss [0.046698214373932796]\n",
            "Model Trained at Step 1960: loss [0.046985993279694854]\n",
            "Model Trained at Step 1980: loss [0.04727858713503478]\n",
            "Model Trained at Step 2000: loss [0.04755396820739212]\n",
            "Model Trained at Step 2020: loss [0.047808242374907016]\n",
            "Model Trained at Step 2040: loss [0.048089058922557666]\n",
            "Model Trained at Step 2060: loss [0.04837322353083344]\n",
            "Model Trained at Step 2080: loss [0.04864217667043041]\n",
            "Model Trained at Step 2100: loss [0.04888558148896506]\n",
            "Model Trained at Step 2120: loss [0.049134571908255965]\n",
            "Model Trained at Step 2140: loss [0.049373637662239504]\n",
            "Model Trained at Step 2160: loss [0.04960730653453156]\n",
            "Model Trained at Step 2180: loss [0.04987984119546205]\n",
            "Model Trained at Step 2200: loss [0.05027143675999181]\n",
            "Model Trained at Step 2220: loss [0.05080326675623148]\n",
            "Model Trained at Step 2240: loss [0.051415906247810096]\n",
            "Model Trained at Step 2260: loss [0.05214726106262559]\n",
            "Model Trained at Step 2280: loss [0.052919815232552084]\n",
            "Model Trained at Step 2300: loss [0.053813565608063475]\n",
            "Model Trained at Step 2320: loss [0.05461312358466054]\n",
            "Model Trained at Step 2340: loss [0.05561755153495793]\n",
            "Model Trained at Step 2360: loss [0.05637427325365032]\n",
            "Model Trained at Step 2380: loss [0.057346396037496174]\n",
            "Model Trained at Step 2400: loss [0.057622964120178574]\n",
            "Model Trained at Step 2420: loss [0.057611959239251644]\n",
            "Model Trained at Step 2440: loss [0.05680993820790368]\n",
            "Model Trained at Step 2460: loss [0.05589692691255518]\n",
            "Model Trained at Step 2480: loss [0.05491881752089398]\n",
            "Model Trained at Step 2500: loss [0.054152896024695064]\n",
            "Model Trained at Step 2520: loss [0.05360052049363573]\n",
            "Model Trained at Step 2540: loss [0.05300306837917211]\n",
            "Model Trained at Step 2560: loss [0.052395658859858106]\n",
            "Model Trained at Step 2580: loss [0.0516684788665616]\n",
            "Model Trained at Step 2600: loss [0.05100172397835072]\n",
            "Model Trained at Step 2620: loss [0.05044454187110943]\n",
            "Model Trained at Step 2640: loss [0.0496415532720059]\n",
            "Model Trained at Step 2660: loss [0.04922428443778136]\n",
            "Model Trained at Step 2680: loss [0.04827988677834463]\n",
            "Model Trained at Step 2700: loss [0.04769418089762974]\n",
            "Model Trained at Step 2720: loss [0.047369524412428754]\n",
            "Model Trained at Step 2740: loss [0.04634160438872327]\n",
            "Model Trained at Step 2760: loss [0.045912033792325176]\n",
            "Model Trained at Step 2780: loss [0.04560963664263784]\n",
            "Model Trained at Step 2800: loss [0.04489475005434866]\n",
            "Model Trained at Step 2820: loss [0.044467143463933996]\n",
            "Model Trained at Step 2840: loss [0.04383954693454864]\n",
            "Model Trained at Step 2860: loss [0.04303559753996401]\n",
            "Model Trained at Step 2880: loss [0.04218861997999786]\n",
            "Model Trained at Step 2900: loss [0.04167139617576583]\n",
            "Model Trained at Step 2920: loss [0.04272626642408185]\n",
            "Model Trained at Step 2940: loss [0.04024159492012721]\n",
            "Model Trained at Step 2960: loss [0.040500064485349575]\n",
            "Model Trained at Step 2980: loss [0.04319306799239099]\n",
            "Model Trained at Step 3000: loss [0.04449355833063351]\n",
            "Model Trained at Step 3020: loss [0.0442274541799673]\n",
            "Model Trained at Step 3040: loss [0.04380815479301893]\n",
            "Model Trained at Step 3060: loss [0.04343360562063595]\n",
            "Model Trained at Step 3080: loss [0.04318225616662552]\n",
            "Model Trained at Step 3100: loss [0.04308288154773762]\n",
            "Model Trained at Step 3120: loss [0.04314709165753526]\n",
            "Model Trained at Step 3140: loss [0.043394938200783925]\n",
            "Model Trained at Step 3160: loss [0.04387922033613364]\n",
            "Model Trained at Step 3180: loss [0.04477786674164163]\n",
            "Model Trained at Step 3200: loss [0.04709812717814847]\n",
            "Model Trained at Step 3220: loss [0.05439250838955499]\n",
            "Model Trained at Step 3240: loss [0.04705044672658435]\n",
            "Model Trained at Step 3260: loss [0.05232261156138359]\n",
            "Model Trained at Step 3280: loss [0.04779978450804194]\n",
            "Model Trained at Step 3300: loss [0.05033102930503024]\n",
            "Model Trained at Step 3320: loss [0.05023672456316297]\n",
            "Model Trained at Step 3340: loss [0.05056388544081984]\n",
            "Model Trained at Step 3360: loss [0.05120059160155606]\n",
            "Model Trained at Step 3380: loss [0.05147277475911292]\n",
            "Model Trained at Step 3400: loss [0.05176156539420553]\n",
            "Model Trained at Step 3420: loss [0.052053337587863215]\n",
            "Model Trained at Step 3440: loss [0.05232429573870401]\n",
            "Model Trained at Step 3460: loss [0.05263101669530612]\n",
            "Model Trained at Step 3480: loss [0.052819695669527586]\n",
            "Model Trained at Step 3500: loss [0.05295581615740517]\n",
            "Model Trained at Step 3520: loss [0.0531321172600682]\n",
            "Model Trained at Step 3540: loss [0.05329838549375439]\n",
            "Model Trained at Step 3560: loss [0.053480666294603]\n",
            "Model Trained at Step 3580: loss [0.053676488339942985]\n",
            "Model Trained at Step 3600: loss [0.05388979046116539]\n",
            "Model Trained at Step 3620: loss [0.054087343151047775]\n",
            "Model Trained at Step 3640: loss [0.054287019250222134]\n",
            "Model Trained at Step 3660: loss [0.054421441325449925]\n",
            "Model Trained at Step 3680: loss [0.05463784252535443]\n",
            "Model Trained at Step 3700: loss [0.05497822306946828]\n",
            "Model Trained at Step 3720: loss [0.05549152033808298]\n",
            "Model Trained at Step 3740: loss [0.05608248844128093]\n",
            "Model Trained at Step 3760: loss [0.05657128325645494]\n",
            "Model Trained at Step 3780: loss [0.057127205849556474]\n",
            "Model Trained at Step 3800: loss [0.05754542337903194]\n",
            "Model Trained at Step 3820: loss [0.05768778658226211]\n",
            "Model Trained at Step 3840: loss [0.05783150846045972]\n",
            "Model Trained at Step 3860: loss [0.05766241514850301]\n",
            "Model Trained at Step 3880: loss [0.057797501357935134]\n",
            "Model Trained at Step 3900: loss [0.057811958422171995]\n",
            "Model Trained at Step 3920: loss [0.058325045628250215]\n",
            "Model Trained at Step 3940: loss [0.05885386251414234]\n",
            "Model Trained at Step 3960: loss [0.059467264300174995]\n",
            "Model Trained at Step 3980: loss [0.06010291606436161]\n",
            "Model Trained at Step 4000: loss [0.060260152100112895]\n",
            "Model Trained at Step 4020: loss [0.06013747376044702]\n",
            "Model Trained at Step 4040: loss [0.059472222676759515]\n",
            "Model Trained at Step 4060: loss [0.05844294657694832]\n",
            "Model Trained at Step 4080: loss [0.05772710891653495]\n",
            "Model Trained at Step 4100: loss [0.05687199689741497]\n",
            "Model Trained at Step 4120: loss [0.056504065898683044]\n",
            "Model Trained at Step 4140: loss [0.0562981544862035]\n",
            "Model Trained at Step 4160: loss [0.055653216733031884]\n",
            "Model Trained at Step 4180: loss [0.055319954657717975]\n",
            "Model Trained at Step 4200: loss [0.05492860262319843]\n",
            "Model Trained at Step 4220: loss [0.05419584474818821]\n",
            "Model Trained at Step 4240: loss [0.05391232896626016]\n",
            "Model Trained at Step 4260: loss [0.05378921984042785]\n",
            "Model Trained at Step 4280: loss [0.05303540447556052]\n",
            "Model Trained at Step 4300: loss [0.052769051457480134]\n",
            "Model Trained at Step 4320: loss [0.05253466994287288]\n",
            "Model Trained at Step 4340: loss [0.0523675241316666]\n",
            "Model Trained at Step 4360: loss [0.05220956746556497]\n",
            "Model Trained at Step 4380: loss [0.05024584043621259]\n",
            "Model Trained at Step 4400: loss [0.047989682770870935]\n",
            "Model Trained at Step 4420: loss [0.04819327682096511]\n",
            "Model Trained at Step 4440: loss [0.052029467910017294]\n",
            "Model Trained at Step 4460: loss [0.04294550243318617]\n",
            "Model Trained at Step 4480: loss [0.04337684666390666]\n",
            "Model Trained at Step 4500: loss [0.04442732927616878]\n",
            "Model Trained at Step 4520: loss [0.052030491284641135]\n",
            "Model Trained at Step 4540: loss [0.05331904313742508]\n",
            "Model Trained at Step 4560: loss [0.05514825973593176]\n",
            "Model Trained at Step 4580: loss [0.05478242048409301]\n",
            "Model Trained at Step 4600: loss [0.05467423019525104]\n",
            "Model Trained at Step 4620: loss [0.05497909157857318]\n",
            "Model Trained at Step 4640: loss [0.0556771010193141]\n",
            "Model Trained at Step 4660: loss [0.05662669323632426]\n",
            "Model Trained at Step 4680: loss [0.0577414240641788]\n",
            "Model Trained at Step 4700: loss [0.0589705183641482]\n",
            "Model Trained at Step 4720: loss [0.060372224767762536]\n",
            "Model Trained at Step 4740: loss [0.061854063562475484]\n",
            "Model Trained at Step 4760: loss [0.0634842891306748]\n",
            "Model Trained at Step 4780: loss [0.06522806052211254]\n",
            "Model Trained at Step 4800: loss [0.06694186639694555]\n",
            "Model Trained at Step 4820: loss [0.06814154222230567]\n",
            "Model Trained at Step 4840: loss [0.06665779826867282]\n",
            "Model Trained at Step 4860: loss [0.06447100344184549]\n",
            "Model Trained at Step 4880: loss [0.0633963510771358]\n",
            "Model Trained at Step 4900: loss [0.05721505644837234]\n",
            "Model Trained at Step 4920: loss [0.061964277977584706]\n",
            "Model Trained at Step 4940: loss [0.06383928035879019]\n",
            "Model Trained at Step 4960: loss [0.0623541643380122]\n",
            "Model Trained at Step 4980: loss [0.0628121849616933]\n",
            "Model Trained at Step 5000: loss [0.062294391230598344]\n",
            "Simulation wave training 6 done\n",
            "Model Trained at Step 20: loss [0.05021781600361737]\n",
            "Model Trained at Step 40: loss [0.04536037601702573]\n",
            "Model Trained at Step 60: loss [0.044895738293688245]\n",
            "Model Trained at Step 80: loss [0.04646622254991366]\n",
            "Model Trained at Step 100: loss [0.05429146391473638]\n",
            "Model Trained at Step 120: loss [0.04653686542877931]\n",
            "Model Trained at Step 140: loss [0.04690293545374963]\n",
            "Model Trained at Step 160: loss [0.05095627663831977]\n",
            "Model Trained at Step 180: loss [0.060948850392978836]\n",
            "Model Trained at Step 200: loss [0.056937893402960016]\n",
            "Model Trained at Step 220: loss [0.061492074405767094]\n",
            "Model Trained at Step 240: loss [0.07341700003804627]\n",
            "Model Trained at Step 260: loss [0.07444842752746607]\n",
            "Model Trained at Step 280: loss [0.07452501969474386]\n",
            "Model Trained at Step 300: loss [0.0741452359510916]\n",
            "Model Trained at Step 320: loss [0.0734009361691248]\n",
            "Model Trained at Step 340: loss [0.07262719024815424]\n",
            "Model Trained at Step 360: loss [0.07172428606782895]\n",
            "Model Trained at Step 380: loss [0.07054271751232166]\n",
            "Model Trained at Step 400: loss [0.06949051383168973]\n",
            "Model Trained at Step 420: loss [0.0685899038855297]\n",
            "Model Trained at Step 440: loss [0.06750659758529425]\n",
            "Model Trained at Step 460: loss [0.0662704524498407]\n",
            "Model Trained at Step 480: loss [0.06519133021848475]\n",
            "Model Trained at Step 500: loss [0.06412919773919838]\n",
            "Model Trained at Step 520: loss [0.06284690851295789]\n",
            "Model Trained at Step 540: loss [0.061528629651116676]\n",
            "Model Trained at Step 560: loss [0.0603549832790119]\n",
            "Model Trained at Step 580: loss [0.059153672548059824]\n",
            "Model Trained at Step 600: loss [0.0578036626770981]\n",
            "Model Trained at Step 620: loss [0.05647417905394035]\n",
            "Model Trained at Step 640: loss [0.0552332149272833]\n",
            "Model Trained at Step 660: loss [0.05395862899056213]\n",
            "Model Trained at Step 680: loss [0.05260801844026612]\n",
            "Model Trained at Step 700: loss [0.05131554911215101]\n",
            "Model Trained at Step 720: loss [0.050073855289596556]\n",
            "Model Trained at Step 740: loss [0.04882643829782335]\n",
            "Model Trained at Step 760: loss [0.047563078620000344]\n",
            "Model Trained at Step 780: loss [0.04636777435641689]\n",
            "Model Trained at Step 800: loss [0.04521610823635321]\n",
            "Model Trained at Step 820: loss [0.04409996046464955]\n",
            "Model Trained at Step 840: loss [0.04302169906043217]\n",
            "Model Trained at Step 860: loss [0.04202217096631298]\n",
            "Model Trained at Step 880: loss [0.041085637754456554]\n",
            "Model Trained at Step 900: loss [0.040223088962292104]\n",
            "Model Trained at Step 920: loss [0.039439012992956414]\n",
            "Model Trained at Step 940: loss [0.038728963469124765]\n",
            "Model Trained at Step 960: loss [0.03809892628456557]\n",
            "Model Trained at Step 980: loss [0.037544303960827585]\n",
            "Model Trained at Step 1000: loss [0.03708740122066271]\n",
            "Model Trained at Step 1020: loss [0.03671153889526067]\n",
            "Model Trained at Step 1040: loss [0.036458338083975625]\n",
            "Model Trained at Step 1060: loss [0.036330238646570784]\n",
            "Model Trained at Step 1080: loss [0.03636286758896711]\n",
            "Model Trained at Step 1100: loss [0.03653586606412966]\n",
            "Model Trained at Step 1120: loss [0.03687149506442864]\n",
            "Model Trained at Step 1140: loss [0.03736607503969456]\n",
            "Model Trained at Step 1160: loss [0.03802224547194339]\n",
            "Model Trained at Step 1180: loss [0.03881584669968802]\n",
            "Model Trained at Step 1200: loss [0.03974905720715282]\n",
            "Model Trained at Step 1220: loss [0.040809981570913444]\n",
            "Model Trained at Step 1240: loss [0.04197763853594445]\n",
            "Model Trained at Step 1260: loss [0.0432361455610691]\n",
            "Model Trained at Step 1280: loss [0.04456831500617789]\n",
            "Model Trained at Step 1300: loss [0.04595318721011384]\n",
            "Model Trained at Step 1320: loss [0.04732592536915337]\n",
            "Model Trained at Step 1340: loss [0.04872268612363988]\n",
            "Model Trained at Step 1360: loss [0.05011636260639572]\n",
            "Model Trained at Step 1380: loss [0.05147557031792714]\n",
            "Model Trained at Step 1400: loss [0.05274254324071363]\n",
            "Model Trained at Step 1420: loss [0.05396844662247219]\n",
            "Model Trained at Step 1440: loss [0.05518045039369189]\n",
            "Model Trained at Step 1460: loss [0.056316171044884754]\n",
            "Model Trained at Step 1480: loss [0.05730526723752505]\n",
            "Model Trained at Step 1500: loss [0.058224757998519824]\n",
            "Model Trained at Step 1520: loss [0.05912884703869083]\n",
            "Model Trained at Step 1540: loss [0.05995425427351506]\n",
            "Model Trained at Step 1560: loss [0.06060273888439369]\n",
            "Model Trained at Step 1580: loss [0.061153273199470545]\n",
            "Model Trained at Step 1600: loss [0.061709786353949324]\n",
            "Model Trained at Step 1620: loss [0.0621922276873285]\n",
            "Model Trained at Step 1640: loss [0.062491647583974054]\n",
            "Model Trained at Step 1660: loss [0.06268041087075585]\n",
            "Model Trained at Step 1680: loss [0.06288874941230801]\n",
            "Model Trained at Step 1700: loss [0.06304261880483737]\n",
            "Model Trained at Step 1720: loss [0.06301307438618955]\n",
            "Model Trained at Step 1740: loss [0.06287333830324164]\n",
            "Model Trained at Step 1760: loss [0.06276596866461492]\n",
            "Model Trained at Step 1780: loss [0.06261297057773488]\n",
            "Model Trained at Step 1800: loss [0.062288040988046814]\n",
            "Model Trained at Step 1820: loss [0.0618757468694995]\n",
            "Model Trained at Step 1840: loss [0.06150896412420644]\n",
            "Model Trained at Step 1860: loss [0.06109883369705599]\n",
            "Model Trained at Step 1880: loss [0.06053036358872562]\n",
            "Model Trained at Step 1900: loss [0.05990456562894882]\n",
            "Model Trained at Step 1920: loss [0.059332736582406176]\n",
            "Model Trained at Step 1940: loss [0.058714386718202595]\n",
            "Model Trained at Step 1960: loss [0.05796526774290565]\n",
            "Model Trained at Step 1980: loss [0.057197998569964895]\n",
            "Model Trained at Step 2000: loss [0.05647975983331284]\n",
            "Model Trained at Step 2020: loss [0.055728052946884266]\n",
            "Model Trained at Step 2040: loss [0.05488101908056127]\n",
            "Model Trained at Step 2060: loss [0.05404499439179703]\n",
            "Model Trained at Step 2080: loss [0.05325518872436943]\n",
            "Model Trained at Step 2100: loss [0.05244311016694693]\n",
            "Model Trained at Step 2120: loss [0.05157922459681079]\n",
            "Model Trained at Step 2140: loss [0.050750519130779735]\n",
            "Model Trained at Step 2160: loss [0.049963566668678905]\n",
            "Model Trained at Step 2180: loss [0.0491771972697998]\n",
            "Model Trained at Step 2200: loss [0.048374570439457996]\n",
            "Model Trained at Step 2220: loss [0.04762010887254499]\n",
            "Model Trained at Step 2240: loss [0.046906241145599395]\n",
            "Model Trained at Step 2260: loss [0.046214061309233744]\n",
            "Model Trained at Step 2280: loss [0.04553124186863241]\n",
            "Model Trained at Step 2300: loss [0.04489900972458043]\n",
            "Model Trained at Step 2320: loss [0.04430776575065567]\n",
            "Model Trained at Step 2340: loss [0.04374760739613357]\n",
            "Model Trained at Step 2360: loss [0.04320784146912428]\n",
            "Model Trained at Step 2380: loss [0.042719312696477815]\n",
            "Model Trained at Step 2400: loss [0.042271358173627226]\n",
            "Model Trained at Step 2420: loss [0.04184780951907774]\n",
            "Model Trained at Step 2440: loss [0.04143037864310516]\n",
            "Model Trained at Step 2460: loss [0.0411172183651975]\n",
            "Model Trained at Step 2480: loss [0.04083750131967855]\n",
            "Model Trained at Step 2500: loss [0.040420120725601624]\n",
            "Model Trained at Step 2520: loss [0.040281797596386025]\n",
            "Model Trained at Step 2540: loss [0.04016643162650725]\n",
            "Model Trained at Step 2560: loss [0.03968404284567319]\n",
            "Model Trained at Step 2580: loss [0.039913149677337056]\n",
            "Model Trained at Step 2600: loss [0.039677796362252514]\n",
            "Model Trained at Step 2620: loss [0.03949660563715908]\n",
            "Model Trained at Step 2640: loss [0.03987979463620411]\n",
            "Model Trained at Step 2660: loss [0.039463960466898415]\n",
            "Model Trained at Step 2680: loss [0.040008945386399083]\n",
            "Model Trained at Step 2700: loss [0.03984845814455791]\n",
            "Model Trained at Step 2720: loss [0.04026409228414781]\n",
            "Model Trained at Step 2740: loss [0.04040586371445852]\n",
            "Model Trained at Step 2760: loss [0.04077979230053109]\n",
            "Model Trained at Step 2780: loss [0.0410813452147883]\n",
            "Model Trained at Step 2800: loss [0.04153472457917942]\n",
            "Model Trained at Step 2820: loss [0.04193114263079669]\n",
            "Model Trained at Step 2840: loss [0.04248404838536006]\n",
            "Model Trained at Step 2860: loss [0.04297909595868784]\n",
            "Model Trained at Step 2880: loss [0.043619681073307284]\n",
            "Model Trained at Step 2900: loss [0.044245226914149775]\n",
            "Model Trained at Step 2920: loss [0.044932608433813075]\n",
            "Model Trained at Step 2940: loss [0.04566420365245825]\n",
            "Model Trained at Step 2960: loss [0.0464425174720368]\n",
            "Model Trained at Step 2980: loss [0.047269201216574325]\n",
            "Model Trained at Step 3000: loss [0.048090521728501465]\n",
            "Model Trained at Step 3020: loss [0.04895918749397156]\n",
            "Model Trained at Step 3040: loss [0.04987277981603865]\n",
            "Model Trained at Step 3060: loss [0.050806474126272386]\n",
            "Model Trained at Step 3080: loss [0.051709959173388]\n",
            "Model Trained at Step 3100: loss [0.05263492072697834]\n",
            "Model Trained at Step 3120: loss [0.05360463819766268]\n",
            "Model Trained at Step 3140: loss [0.05457058419653724]\n",
            "Model Trained at Step 3160: loss [0.055469199619411304]\n",
            "Model Trained at Step 3180: loss [0.056358542600518514]\n",
            "Model Trained at Step 3200: loss [0.05729227605271443]\n",
            "Model Trained at Step 3220: loss [0.058204621001146]\n",
            "Model Trained at Step 3240: loss [0.05901369668801291]\n",
            "Model Trained at Step 3260: loss [0.059782861821699215]\n",
            "Model Trained at Step 3280: loss [0.0605999019931364]\n",
            "Model Trained at Step 3300: loss [0.06139657804741251]\n",
            "Model Trained at Step 3320: loss [0.06206356063712559]\n",
            "Model Trained at Step 3340: loss [0.06264789806033957]\n",
            "Model Trained at Step 3360: loss [0.06327157803052116]\n",
            "Model Trained at Step 3380: loss [0.0638744272750579]\n",
            "Model Trained at Step 3400: loss [0.06432382815760454]\n",
            "Model Trained at Step 3420: loss [0.06464961279836136]\n",
            "Model Trained at Step 3440: loss [0.06499533441414805]\n",
            "Model Trained at Step 3460: loss [0.0653194008597912]\n",
            "Model Trained at Step 3480: loss [0.06547815914893215]\n",
            "Model Trained at Step 3500: loss [0.06548643175444975]\n",
            "Model Trained at Step 3520: loss [0.06550029315740814]\n",
            "Model Trained at Step 3540: loss [0.0654785048388374]\n",
            "Model Trained at Step 3560: loss [0.0652770488411477]\n",
            "Model Trained at Step 3580: loss [0.06491057045670676]\n",
            "Model Trained at Step 3600: loss [0.06454458247797903]\n",
            "Model Trained at Step 3620: loss [0.06413670432717163]\n",
            "Model Trained at Step 3640: loss [0.06353634550213043]\n",
            "Model Trained at Step 3660: loss [0.06279852925598446]\n",
            "Model Trained at Step 3680: loss [0.06206795379288647]\n",
            "Model Trained at Step 3700: loss [0.06127911676276052]\n",
            "Model Trained at Step 3720: loss [0.06033303256256421]\n",
            "Model Trained at Step 3740: loss [0.05931545850011514]\n",
            "Model Trained at Step 3760: loss [0.05832297519894351]\n",
            "Model Trained at Step 3780: loss [0.057283069411113496]\n",
            "Model Trained at Step 3800: loss [0.056140377230186354]\n",
            "Model Trained at Step 3820: loss [0.054985444324986366]\n",
            "Model Trained at Step 3840: loss [0.05386369259144761]\n",
            "Model Trained at Step 3860: loss [0.05272339093597085]\n",
            "Model Trained at Step 3880: loss [0.051546677387241405]\n",
            "Model Trained at Step 3900: loss [0.05040276993828814]\n",
            "Model Trained at Step 3920: loss [0.04930497794249228]\n",
            "Model Trained at Step 3940: loss [0.04822375432998806]\n",
            "Model Trained at Step 3960: loss [0.04715917615709813]\n",
            "Model Trained at Step 3980: loss [0.04615199215529557]\n",
            "Model Trained at Step 4000: loss [0.04520423158633613]\n",
            "Model Trained at Step 4020: loss [0.044305339099045285]\n",
            "Model Trained at Step 4040: loss [0.04345976293846794]\n",
            "Model Trained at Step 4060: loss [0.042688318747638745]\n",
            "Model Trained at Step 4080: loss [0.04199069807651755]\n",
            "Model Trained at Step 4100: loss [0.04136873855516088]\n",
            "Model Trained at Step 4120: loss [0.040823982504198206]\n",
            "Model Trained at Step 4140: loss [0.040363601957334856]\n",
            "Model Trained at Step 4160: loss [0.039988184240816675]\n",
            "Model Trained at Step 4180: loss [0.03970001280714815]\n",
            "Model Trained at Step 4200: loss [0.03950018832514114]\n",
            "Model Trained at Step 4220: loss [0.039384776329289946]\n",
            "Model Trained at Step 4240: loss [0.03935626315239441]\n",
            "Model Trained at Step 4260: loss [0.039413131389081826]\n",
            "Model Trained at Step 4280: loss [0.039553713405033046]\n",
            "Model Trained at Step 4300: loss [0.03976999821261579]\n",
            "Model Trained at Step 4320: loss [0.040062180356495294]\n",
            "Model Trained at Step 4340: loss [0.04042732214697389]\n",
            "Model Trained at Step 4360: loss [0.040857593592246555]\n",
            "Model Trained at Step 4380: loss [0.041349680858252504]\n",
            "Model Trained at Step 4400: loss [0.041901490486778895]\n",
            "Model Trained at Step 4420: loss [0.04250899392787834]\n",
            "Model Trained at Step 4440: loss [0.043157505712064934]\n",
            "Model Trained at Step 4460: loss [0.043855465946734215]\n",
            "Model Trained at Step 4480: loss [0.04459844900031429]\n",
            "Model Trained at Step 4500: loss [0.045379976078579375]\n",
            "Model Trained at Step 4520: loss [0.04617841302175142]\n",
            "Model Trained at Step 4540: loss [0.047015922658384494]\n",
            "Model Trained at Step 4560: loss [0.047894625582802816]\n",
            "Model Trained at Step 4580: loss [0.04879924143065419]\n",
            "Model Trained at Step 4600: loss [0.04969961482139552]\n",
            "Model Trained at Step 4620: loss [0.05062904457500961]\n",
            "Model Trained at Step 4640: loss [0.05160031846803755]\n",
            "Model Trained at Step 4660: loss [0.05258619105132043]\n",
            "Model Trained at Step 4680: loss [0.053541580994936444]\n",
            "Model Trained at Step 4700: loss [0.054510015585154406]\n",
            "Model Trained at Step 4720: loss [0.05553460520600471]\n",
            "Model Trained at Step 4740: loss [0.056567018943063804]\n",
            "Model Trained at Step 4760: loss [0.05753439923259507]\n",
            "Model Trained at Step 4780: loss [0.058486201631918325]\n",
            "Model Trained at Step 4800: loss [0.05950272824546464]\n",
            "Model Trained at Step 4820: loss [0.06053539392231476]\n",
            "Model Trained at Step 4840: loss [0.0614956284242418]\n",
            "Model Trained at Step 4860: loss [0.062387463742320404]\n",
            "Model Trained at Step 4880: loss [0.06331430153142097]\n",
            "Model Trained at Step 4900: loss [0.0642565447108792]\n",
            "Model Trained at Step 4920: loss [0.06505650463004777]\n",
            "Model Trained at Step 4940: loss [0.06569748765902314]\n",
            "Model Trained at Step 4960: loss [0.0663696658725776]\n",
            "Model Trained at Step 4980: loss [0.06705748975369898]\n",
            "Model Trained at Step 5000: loss [0.06764381712046295]\n",
            "Simulation wave training 7 done\n",
            "Model Trained at Step 20: loss [0.049990333325212674]\n",
            "Model Trained at Step 40: loss [0.044410510839165605]\n",
            "Model Trained at Step 60: loss [0.04471499171882141]\n",
            "Model Trained at Step 80: loss [0.04730272726706192]\n",
            "Model Trained at Step 100: loss [0.052609347493695835]\n",
            "Model Trained at Step 120: loss [0.04756737665563694]\n",
            "Model Trained at Step 140: loss [0.047038645169097074]\n",
            "Model Trained at Step 160: loss [0.04884973092569017]\n",
            "Model Trained at Step 180: loss [0.05913782760203483]\n",
            "Model Trained at Step 200: loss [0.06543135958691948]\n",
            "Model Trained at Step 220: loss [0.07315205838930638]\n",
            "Model Trained at Step 240: loss [0.07653887986490364]\n",
            "Model Trained at Step 260: loss [0.07671267471489013]\n",
            "Model Trained at Step 280: loss [0.07655788358413726]\n",
            "Model Trained at Step 300: loss [0.07599909205480326]\n",
            "Model Trained at Step 320: loss [0.07498857886496405]\n",
            "Model Trained at Step 340: loss [0.07392447342606004]\n",
            "Model Trained at Step 360: loss [0.0728114673376049]\n",
            "Model Trained at Step 380: loss [0.07149585800181656]\n",
            "Model Trained at Step 400: loss [0.07031704702868566]\n",
            "Model Trained at Step 420: loss [0.06927100464388436]\n",
            "Model Trained at Step 440: loss [0.06802323293541582]\n",
            "Model Trained at Step 460: loss [0.06662707951143704]\n",
            "Model Trained at Step 480: loss [0.06539286451636392]\n",
            "Model Trained at Step 500: loss [0.06416220434564406]\n",
            "Model Trained at Step 520: loss [0.06270559416835185]\n",
            "Model Trained at Step 540: loss [0.06124853724097151]\n",
            "Model Trained at Step 560: loss [0.0599276704458054]\n",
            "Model Trained at Step 580: loss [0.058555820184349086]\n",
            "Model Trained at Step 600: loss [0.05704096161119436]\n",
            "Model Trained at Step 620: loss [0.055582471740952454]\n",
            "Model Trained at Step 640: loss [0.054204235119203456]\n",
            "Model Trained at Step 660: loss [0.052787201007499916]\n",
            "Model Trained at Step 680: loss [0.05131442050902495]\n",
            "Model Trained at Step 700: loss [0.04991827679251869]\n",
            "Model Trained at Step 720: loss [0.04856802286529753]\n",
            "Model Trained at Step 740: loss [0.04722521102478912]\n",
            "Model Trained at Step 760: loss [0.04588993284177094]\n",
            "Model Trained at Step 780: loss [0.04463566093015429]\n",
            "Model Trained at Step 800: loss [0.04343391631798636]\n",
            "Model Trained at Step 820: loss [0.04228962024930874]\n",
            "Model Trained at Step 840: loss [0.04120760901968483]\n",
            "Model Trained at Step 860: loss [0.04021536063867701]\n",
            "Model Trained at Step 880: loss [0.039305202733293375]\n",
            "Model Trained at Step 900: loss [0.03848227069514667]\n",
            "Model Trained at Step 920: loss [0.03775921675158192]\n",
            "Model Trained at Step 940: loss [0.037109725320963294]\n",
            "Model Trained at Step 960: loss [0.03655701342576449]\n",
            "Model Trained at Step 980: loss [0.03609173861069064]\n",
            "Model Trained at Step 1000: loss [0.03575348195852873]\n",
            "Model Trained at Step 1020: loss [0.03551926300214916]\n",
            "Model Trained at Step 1040: loss [0.03543893429963796]\n",
            "Model Trained at Step 1060: loss [0.03551543305523259]\n",
            "Model Trained at Step 1080: loss [0.03578773772509162]\n",
            "Model Trained at Step 1100: loss [0.03622457315979409]\n",
            "Model Trained at Step 1120: loss [0.03684293621801948]\n",
            "Model Trained at Step 1140: loss [0.037640524293744204]\n",
            "Model Trained at Step 1160: loss [0.03861657585594045]\n",
            "Model Trained at Step 1180: loss [0.03974417299441079]\n",
            "Model Trained at Step 1200: loss [0.04102130028089358]\n",
            "Model Trained at Step 1220: loss [0.04243058572619626]\n",
            "Model Trained at Step 1240: loss [0.04393623989464537]\n",
            "Model Trained at Step 1260: loss [0.04552910366618675]\n",
            "Model Trained at Step 1280: loss [0.04718389497748286]\n",
            "Model Trained at Step 1300: loss [0.04887423340632248]\n",
            "Model Trained at Step 1320: loss [0.050517230495273104]\n",
            "Model Trained at Step 1340: loss [0.052163622173960235]\n",
            "Model Trained at Step 1360: loss [0.0538067118059814]\n",
            "Model Trained at Step 1380: loss [0.055391794826108044]\n",
            "Model Trained at Step 1400: loss [0.05684695465969221]\n",
            "Model Trained at Step 1420: loss [0.05823795694000975]\n",
            "Model Trained at Step 1440: loss [0.059622837122998706]\n",
            "Model Trained at Step 1460: loss [0.06091956680310025]\n",
            "Model Trained at Step 1480: loss [0.062026918498094344]\n",
            "Model Trained at Step 1500: loss [0.06302487996635286]\n",
            "Model Trained at Step 1520: loss [0.06401124731042468]\n",
            "Model Trained at Step 1540: loss [0.06492491234653348]\n",
            "Model Trained at Step 1560: loss [0.06563452884158459]\n",
            "Model Trained at Step 1580: loss [0.06618634171092111]\n",
            "Model Trained at Step 1600: loss [0.06673219020108401]\n",
            "Model Trained at Step 1620: loss [0.06722337593146407]\n",
            "Model Trained at Step 1640: loss [0.06752442784970139]\n",
            "Model Trained at Step 1660: loss [0.0676447139173384]\n",
            "Model Trained at Step 1680: loss [0.06775576561324126]\n",
            "Model Trained at Step 1700: loss [0.06783978848342155]\n",
            "Model Trained at Step 1720: loss [0.0677304904571136]\n",
            "Model Trained at Step 1740: loss [0.06743941223664847]\n",
            "Model Trained at Step 1760: loss [0.06715850171881432]\n",
            "Model Trained at Step 1780: loss [0.06685688682711763]\n",
            "Model Trained at Step 1800: loss [0.06636680868366789]\n",
            "Model Trained at Step 1820: loss [0.06572419581949698]\n",
            "Model Trained at Step 1840: loss [0.06512284890482845]\n",
            "Model Trained at Step 1860: loss [0.06449470786226513]\n",
            "Model Trained at Step 1880: loss [0.06367887507519174]\n",
            "Model Trained at Step 1900: loss [0.06276720981950339]\n",
            "Model Trained at Step 1920: loss [0.0619238072485486]\n",
            "Model Trained at Step 1940: loss [0.06103455905669197]\n",
            "Model Trained at Step 1960: loss [0.0599859935633477]\n",
            "Model Trained at Step 1980: loss [0.0589166813646642]\n",
            "Model Trained at Step 2000: loss [0.05791037075345099]\n",
            "Model Trained at Step 2020: loss [0.05686390043473087]\n",
            "Model Trained at Step 2040: loss [0.05571427847664588]\n",
            "Model Trained at Step 2060: loss [0.05459065545488824]\n",
            "Model Trained at Step 2080: loss [0.053525282148717136]\n",
            "Model Trained at Step 2100: loss [0.052440664173456705]\n",
            "Model Trained at Step 2120: loss [0.051314467111485754]\n",
            "Model Trained at Step 2140: loss [0.050246658713896775]\n",
            "Model Trained at Step 2160: loss [0.04923433236276501]\n",
            "Model Trained at Step 2180: loss [0.04823779472677937]\n",
            "Model Trained at Step 2200: loss [0.04725029425663187]\n",
            "Model Trained at Step 2220: loss [0.04633643207524188]\n",
            "Model Trained at Step 2240: loss [0.04548053511396455]\n",
            "Model Trained at Step 2260: loss [0.044670325667345]\n",
            "Model Trained at Step 2280: loss [0.04389941939108635]\n",
            "Model Trained at Step 2300: loss [0.04320134804193933]\n",
            "Model Trained at Step 2320: loss [0.0425641170843758]\n",
            "Model Trained at Step 2340: loss [0.04198528595664133]\n",
            "Model Trained at Step 2360: loss [0.04145600657199685]\n",
            "Model Trained at Step 2380: loss [0.04098986233946585]\n",
            "Model Trained at Step 2400: loss [0.04058644877168962]\n",
            "Model Trained at Step 2420: loss [0.04025747911357842]\n",
            "Model Trained at Step 2440: loss [0.039943634034468004]\n",
            "Model Trained at Step 2460: loss [0.03967851216621211]\n",
            "Model Trained at Step 2480: loss [0.0395957518841582]\n",
            "Model Trained at Step 2500: loss [0.03939059098812383]\n",
            "Model Trained at Step 2520: loss [0.03925187125378925]\n",
            "Model Trained at Step 2540: loss [0.03947157270602873]\n",
            "Model Trained at Step 2560: loss [0.0392332946051304]\n",
            "Model Trained at Step 2580: loss [0.039468202765030894]\n",
            "Model Trained at Step 2600: loss [0.03969758517792077]\n",
            "Model Trained at Step 2620: loss [0.03959501032337962]\n",
            "Model Trained at Step 2640: loss [0.0402021762135102]\n",
            "Model Trained at Step 2660: loss [0.04009792729467451]\n",
            "Model Trained at Step 2680: loss [0.040717497649933]\n",
            "Model Trained at Step 2700: loss [0.0408354729952667]\n",
            "Model Trained at Step 2720: loss [0.041376198571096076]\n",
            "Model Trained at Step 2740: loss [0.04166447004370959]\n",
            "Model Trained at Step 2760: loss [0.042202242553392014]\n",
            "Model Trained at Step 2780: loss [0.04257276197497262]\n",
            "Model Trained at Step 2800: loss [0.04316203458359169]\n",
            "Model Trained at Step 2820: loss [0.04361264128819656]\n",
            "Model Trained at Step 2840: loss [0.04421244327167943]\n",
            "Model Trained at Step 2860: loss [0.044754800471204795]\n",
            "Model Trained at Step 2880: loss [0.04538088121140302]\n",
            "Model Trained at Step 2900: loss [0.0460155843046738]\n",
            "Model Trained at Step 2920: loss [0.04663746917524143]\n",
            "Model Trained at Step 2940: loss [0.04731387174845269]\n",
            "Model Trained at Step 2960: loss [0.04800668600563214]\n",
            "Model Trained at Step 2980: loss [0.048722132841355445]\n",
            "Model Trained at Step 3000: loss [0.049401368681620386]\n",
            "Model Trained at Step 3020: loss [0.0501133974886125]\n",
            "Model Trained at Step 3040: loss [0.05085562494799958]\n",
            "Model Trained at Step 3060: loss [0.05159397786487898]\n",
            "Model Trained at Step 3080: loss [0.05227604665898146]\n",
            "Model Trained at Step 3100: loss [0.052974622595693764]\n",
            "Model Trained at Step 3120: loss [0.053708560613791215]\n",
            "Model Trained at Step 3140: loss [0.054422148079093124]\n",
            "Model Trained at Step 3160: loss [0.055053260404078076]\n",
            "Model Trained at Step 3180: loss [0.055683899437972514]\n",
            "Model Trained at Step 3200: loss [0.05635446612349794]\n",
            "Model Trained at Step 3220: loss [0.0569930982292457]\n",
            "Model Trained at Step 3240: loss [0.0575264288917511]\n",
            "Model Trained at Step 3260: loss [0.05804623139125274]\n",
            "Model Trained at Step 3280: loss [0.058614815560716016]\n",
            "Model Trained at Step 3300: loss [0.05914775139092075]\n",
            "Model Trained at Step 3320: loss [0.05955065293868931]\n",
            "Model Trained at Step 3340: loss [0.05991320775455304]\n",
            "Model Trained at Step 3360: loss [0.06032351334998294]\n",
            "Model Trained at Step 3380: loss [0.06069296081710618]\n",
            "Model Trained at Step 3400: loss [0.06090597445205924]\n",
            "Model Trained at Step 3420: loss [0.06105448619769188]\n",
            "Model Trained at Step 3440: loss [0.06124597487497424]\n",
            "Model Trained at Step 3460: loss [0.06138815786069307]\n",
            "Model Trained at Step 3480: loss [0.06136032793233659]\n",
            "Model Trained at Step 3500: loss [0.06125303921289815]\n",
            "Model Trained at Step 3520: loss [0.06117832873907163]\n",
            "Model Trained at Step 3540: loss [0.06103553489636319]\n",
            "Model Trained at Step 3560: loss [0.06071487762310934]\n",
            "Model Trained at Step 3580: loss [0.060310713125876506]\n",
            "Model Trained at Step 3600: loss [0.05992998315052508]\n",
            "Model Trained at Step 3620: loss [0.05947929279298164]\n",
            "Model Trained at Step 3640: loss [0.0588551389748741]\n",
            "Model Trained at Step 3660: loss [0.058169146845857465]\n",
            "Model Trained at Step 3680: loss [0.05750035252449006]\n",
            "Model Trained at Step 3700: loss [0.05676194931774856]\n",
            "Model Trained at Step 3720: loss [0.05590009973920971]\n",
            "Model Trained at Step 3740: loss [0.05501644446438381]\n",
            "Model Trained at Step 3760: loss [0.0541526166741366]\n",
            "Model Trained at Step 3780: loss [0.05324329404114281]\n",
            "Model Trained at Step 3800: loss [0.05226224905218095]\n",
            "Model Trained at Step 3820: loss [0.051294202049027925]\n",
            "Model Trained at Step 3840: loss [0.05035351004100689]\n",
            "Model Trained at Step 3860: loss [0.04940416541406536]\n",
            "Model Trained at Step 3880: loss [0.04843637412568638]\n",
            "Model Trained at Step 3900: loss [0.04750616323364136]\n",
            "Model Trained at Step 3920: loss [0.04661679040895556]\n",
            "Model Trained at Step 3940: loss [0.04575042881929893]\n",
            "Model Trained at Step 3960: loss [0.04490579566601597]\n",
            "Model Trained at Step 3980: loss [0.04411628536623531]\n",
            "Model Trained at Step 4000: loss [0.04338039279430688]\n",
            "Model Trained at Step 4020: loss [0.0426944436692438]\n",
            "Model Trained at Step 4040: loss [0.04206025184154011]\n",
            "Model Trained at Step 4060: loss [0.041493912227827494]\n",
            "Model Trained at Step 4080: loss [0.040995016396805375]\n",
            "Model Trained at Step 4100: loss [0.04056706287713678]\n",
            "Model Trained at Step 4120: loss [0.04021056723035479]\n",
            "Model Trained at Step 4140: loss [0.03992876885662051]\n",
            "Model Trained at Step 4160: loss [0.039723549062123234]\n",
            "Model Trained at Step 4180: loss [0.03959615841621609]\n",
            "Model Trained at Step 4200: loss [0.03954805326837141]\n",
            "Model Trained at Step 4220: loss [0.039572216188661934]\n",
            "Model Trained at Step 4240: loss [0.039672599031195604]\n",
            "Model Trained at Step 4260: loss [0.0398459357681448]\n",
            "Model Trained at Step 4280: loss [0.04008969175226619]\n",
            "Model Trained at Step 4300: loss [0.040397027299090865]\n",
            "Model Trained at Step 4320: loss [0.04076808562136531]\n",
            "Model Trained at Step 4340: loss [0.04119969190788079]\n",
            "Model Trained at Step 4360: loss [0.0416804905512033]\n",
            "Model Trained at Step 4380: loss [0.04221418055742821]\n",
            "Model Trained at Step 4400: loss [0.04279581815659417]\n",
            "Model Trained at Step 4420: loss [0.04342350078823009]\n",
            "Model Trained at Step 4440: loss [0.044075974722560606]\n",
            "Model Trained at Step 4460: loss [0.04477220749805079]\n",
            "Model Trained at Step 4480: loss [0.045505074959155775]\n",
            "Model Trained at Step 4500: loss [0.04626963827883257]\n",
            "Model Trained at Step 4520: loss [0.0470382609354318]\n",
            "Model Trained at Step 4540: loss [0.047843603095866195]\n",
            "Model Trained at Step 4560: loss [0.04868682604548345]\n",
            "Model Trained at Step 4580: loss [0.04954971907412668]\n",
            "Model Trained at Step 4600: loss [0.050396037679368996]\n",
            "Model Trained at Step 4620: loss [0.051270468617401734]\n",
            "Model Trained at Step 4640: loss [0.05218537173811355]\n",
            "Model Trained at Step 4660: loss [0.05311028174691067]\n",
            "Model Trained at Step 4680: loss [0.05399531413323808]\n",
            "Model Trained at Step 4700: loss [0.05489315352865929]\n",
            "Model Trained at Step 4720: loss [0.05584805882281925]\n",
            "Model Trained at Step 4740: loss [0.05680376499472084]\n",
            "Model Trained at Step 4760: loss [0.057678465178972585]\n",
            "Model Trained at Step 4780: loss [0.05855309886531335]\n",
            "Model Trained at Step 4800: loss [0.059499025199786504]\n",
            "Model Trained at Step 4820: loss [0.060447517623704664]\n",
            "Model Trained at Step 4840: loss [0.0612960308130487]\n",
            "Model Trained at Step 4860: loss [0.062061733603011704]\n",
            "Model Trained at Step 4880: loss [0.06287842462461227]\n",
            "Model Trained at Step 4900: loss [0.06367777091620591]\n",
            "Model Trained at Step 4920: loss [0.06433934623194912]\n",
            "Model Trained at Step 4940: loss [0.06486656469375054]\n",
            "Model Trained at Step 4960: loss [0.06543215120470422]\n",
            "Model Trained at Step 4980: loss [0.0660203064445648]\n",
            "Model Trained at Step 5000: loss [0.06639238646238041]\n",
            "Simulation wave training 8 done\n",
            "Model Trained at Step 20: loss [0.055754774614756974]\n",
            "Model Trained at Step 40: loss [0.058823920961362595]\n",
            "Model Trained at Step 60: loss [0.05601354904012691]\n",
            "Model Trained at Step 80: loss [0.055893986494481684]\n",
            "Model Trained at Step 100: loss [0.05546020163761491]\n",
            "Model Trained at Step 120: loss [0.05494937771765559]\n",
            "Model Trained at Step 140: loss [0.054535262558255804]\n",
            "Model Trained at Step 160: loss [0.05421783641210999]\n",
            "Model Trained at Step 180: loss [0.054078962708862754]\n",
            "Model Trained at Step 200: loss [0.05431374821773786]\n",
            "Model Trained at Step 220: loss [0.05508208622488416]\n",
            "Model Trained at Step 240: loss [0.056087563929857864]\n",
            "Model Trained at Step 260: loss [0.057436911666443116]\n",
            "Model Trained at Step 280: loss [0.059726385706838636]\n",
            "Model Trained at Step 300: loss [0.061715579651314224]\n",
            "Model Trained at Step 320: loss [0.06251766979353243]\n",
            "Model Trained at Step 340: loss [0.06256489921240296]\n",
            "Model Trained at Step 360: loss [0.06371851875114928]\n",
            "Model Trained at Step 380: loss [0.061297859285925196]\n",
            "Model Trained at Step 400: loss [0.06190454725451281]\n",
            "Model Trained at Step 420: loss [0.06167562738700968]\n",
            "Model Trained at Step 440: loss [0.05654917622507936]\n",
            "Model Trained at Step 460: loss [0.050628927296652235]\n",
            "Model Trained at Step 480: loss [0.04847157915800786]\n",
            "Model Trained at Step 500: loss [0.04757801040543093]\n",
            "Model Trained at Step 520: loss [0.048584600950554814]\n",
            "Model Trained at Step 540: loss [0.05483220007206631]\n",
            "Model Trained at Step 560: loss [0.05519363142220548]\n",
            "Model Trained at Step 580: loss [0.05470022645150856]\n",
            "Model Trained at Step 600: loss [0.05414134763464039]\n",
            "Model Trained at Step 620: loss [0.0538598628195945]\n",
            "Model Trained at Step 640: loss [0.05382346016031887]\n",
            "Model Trained at Step 660: loss [0.052398451716254804]\n",
            "Model Trained at Step 680: loss [0.056492562809075796]\n",
            "Model Trained at Step 700: loss [0.04966232101850845]\n",
            "Model Trained at Step 720: loss [0.05151643021200837]\n",
            "Model Trained at Step 740: loss [0.05258826161251159]\n",
            "Model Trained at Step 760: loss [0.05424040924103265]\n",
            "Model Trained at Step 780: loss [0.057516622385384075]\n",
            "Model Trained at Step 800: loss [0.061766053519959896]\n",
            "Model Trained at Step 820: loss [0.061072585507105925]\n",
            "Model Trained at Step 840: loss [0.06181356066117829]\n",
            "Model Trained at Step 860: loss [0.05852568383934015]\n",
            "Model Trained at Step 880: loss [0.053188867382905215]\n",
            "Model Trained at Step 900: loss [0.056448655089202625]\n",
            "Model Trained at Step 920: loss [0.054697098711903425]\n",
            "Model Trained at Step 940: loss [0.053659380564854066]\n",
            "Model Trained at Step 960: loss [0.0527972745951204]\n",
            "Model Trained at Step 980: loss [0.05354231235791144]\n",
            "Model Trained at Step 1000: loss [0.05315299321503767]\n",
            "Model Trained at Step 1020: loss [0.05202311917730871]\n",
            "Model Trained at Step 1040: loss [0.05284755721338484]\n",
            "Model Trained at Step 1060: loss [0.052847965448902044]\n",
            "Model Trained at Step 1080: loss [0.053109229963313444]\n",
            "Model Trained at Step 1100: loss [0.05325171675842353]\n",
            "Model Trained at Step 1120: loss [0.05461457123716799]\n",
            "Model Trained at Step 1140: loss [0.053138142034689015]\n",
            "Model Trained at Step 1160: loss [0.05516350979934641]\n",
            "Model Trained at Step 1180: loss [0.05679395607889667]\n",
            "Model Trained at Step 1200: loss [0.05689563544453705]\n",
            "Model Trained at Step 1220: loss [0.05772359157218813]\n",
            "Model Trained at Step 1240: loss [0.05778466152714935]\n",
            "Model Trained at Step 1260: loss [0.056068307383042804]\n",
            "Model Trained at Step 1280: loss [0.05281209042296683]\n",
            "Model Trained at Step 1300: loss [0.058843021893114966]\n",
            "Model Trained at Step 1320: loss [0.05018891608116073]\n",
            "Model Trained at Step 1340: loss [0.056337851827713316]\n",
            "Model Trained at Step 1360: loss [0.05046875610008254]\n",
            "Model Trained at Step 1380: loss [0.05752876741329105]\n",
            "Model Trained at Step 1400: loss [0.05323913529906356]\n",
            "Model Trained at Step 1420: loss [0.05460206656722507]\n",
            "Model Trained at Step 1440: loss [0.05653940450103845]\n",
            "Model Trained at Step 1460: loss [0.05196369405068614]\n",
            "Model Trained at Step 1480: loss [0.05740268334983475]\n",
            "Model Trained at Step 1500: loss [0.053027858648575434]\n",
            "Model Trained at Step 1520: loss [0.05725885917048677]\n",
            "Model Trained at Step 1540: loss [0.056113478527322715]\n",
            "Model Trained at Step 1560: loss [0.05601631807199028]\n",
            "Model Trained at Step 1580: loss [0.055118617894098734]\n",
            "Model Trained at Step 1600: loss [0.055864954962732735]\n",
            "Model Trained at Step 1620: loss [0.05685870986842615]\n",
            "Model Trained at Step 1640: loss [0.0566379957287074]\n",
            "Model Trained at Step 1660: loss [0.055448978878752456]\n",
            "Model Trained at Step 1680: loss [0.05595671722658985]\n",
            "Model Trained at Step 1700: loss [0.05439926379793988]\n",
            "Model Trained at Step 1720: loss [0.05719712296502535]\n",
            "Model Trained at Step 1740: loss [0.05153335403231962]\n",
            "Model Trained at Step 1760: loss [0.05507185406321482]\n",
            "Model Trained at Step 1780: loss [0.05781537771292611]\n",
            "Model Trained at Step 1800: loss [0.055956935329062284]\n",
            "Model Trained at Step 1820: loss [0.05235641630258527]\n",
            "Model Trained at Step 1840: loss [0.05573524496333413]\n",
            "Model Trained at Step 1860: loss [0.05665599666190209]\n",
            "Model Trained at Step 1880: loss [0.0554841285516902]\n",
            "Model Trained at Step 1900: loss [0.05640263229872665]\n",
            "Model Trained at Step 1920: loss [0.051340925107488354]\n",
            "Model Trained at Step 1940: loss [0.056488848433267694]\n",
            "Model Trained at Step 1960: loss [0.0559347184052452]\n",
            "Model Trained at Step 1980: loss [0.05591175950489411]\n",
            "Model Trained at Step 2000: loss [0.054220604175930845]\n",
            "Model Trained at Step 2020: loss [0.056075357506897225]\n",
            "Model Trained at Step 2040: loss [0.05664880480849316]\n",
            "Model Trained at Step 2060: loss [0.05596133304868337]\n",
            "Model Trained at Step 2080: loss [0.05689476579805182]\n",
            "Model Trained at Step 2100: loss [0.05659519862357385]\n",
            "Model Trained at Step 2120: loss [0.056001386186631884]\n",
            "Model Trained at Step 2140: loss [0.055414593639435475]\n",
            "Model Trained at Step 2160: loss [0.05640507324361206]\n",
            "Model Trained at Step 2180: loss [0.055866924659001914]\n",
            "Model Trained at Step 2200: loss [0.05574475470230844]\n",
            "Model Trained at Step 2220: loss [0.05585458573770683]\n",
            "Model Trained at Step 2240: loss [0.055116112682217035]\n",
            "Model Trained at Step 2260: loss [0.0553188380093194]\n",
            "Model Trained at Step 2280: loss [0.05717671926114086]\n",
            "Model Trained at Step 2300: loss [0.05283915940810628]\n",
            "Model Trained at Step 2320: loss [0.05648275556469662]\n",
            "Model Trained at Step 2340: loss [0.055323509018803396]\n",
            "Model Trained at Step 2360: loss [0.05662942044931239]\n",
            "Model Trained at Step 2380: loss [0.05423882081787862]\n",
            "Model Trained at Step 2400: loss [0.057589845343978284]\n",
            "Model Trained at Step 2420: loss [0.05796585018239886]\n",
            "Model Trained at Step 2440: loss [0.05805871205940316]\n",
            "Model Trained at Step 2460: loss [0.05756755850888745]\n",
            "Model Trained at Step 2480: loss [0.05763762256598455]\n",
            "Model Trained at Step 2500: loss [0.05800632815780802]\n",
            "Model Trained at Step 2520: loss [0.055748431749872464]\n",
            "Model Trained at Step 2540: loss [0.05742814679321077]\n",
            "Model Trained at Step 2560: loss [0.0567392015639027]\n",
            "Model Trained at Step 2580: loss [0.05563382597828228]\n",
            "Model Trained at Step 2600: loss [0.05402302618568228]\n",
            "Model Trained at Step 2620: loss [0.05516906601892244]\n",
            "Model Trained at Step 2640: loss [0.05645895738658794]\n",
            "Model Trained at Step 2660: loss [0.05181725339911214]\n",
            "Model Trained at Step 2680: loss [0.05454459965996713]\n",
            "Model Trained at Step 2700: loss [0.055485332540737065]\n",
            "Model Trained at Step 2720: loss [0.05371870994207036]\n",
            "Model Trained at Step 2740: loss [0.055276679838215034]\n",
            "Model Trained at Step 2760: loss [0.05500256398308068]\n",
            "Model Trained at Step 2780: loss [0.05376877951034899]\n",
            "Model Trained at Step 2800: loss [0.054278476436374457]\n",
            "Model Trained at Step 2820: loss [0.05470953329887116]\n",
            "Model Trained at Step 2840: loss [0.054849995917291436]\n",
            "Model Trained at Step 2860: loss [0.054042455372625774]\n",
            "Model Trained at Step 2880: loss [0.054988426301762636]\n",
            "Model Trained at Step 2900: loss [0.054796680688118896]\n",
            "Model Trained at Step 2920: loss [0.054845304307959794]\n",
            "Model Trained at Step 2940: loss [0.05432848893097316]\n",
            "Model Trained at Step 2960: loss [0.053493645492892816]\n",
            "Model Trained at Step 2980: loss [0.05342664129155571]\n",
            "Model Trained at Step 3000: loss [0.054811088035258124]\n",
            "Model Trained at Step 3020: loss [0.05547563483927829]\n",
            "Model Trained at Step 3040: loss [0.05260040484175612]\n",
            "Model Trained at Step 3060: loss [0.05667663252639933]\n",
            "Model Trained at Step 3080: loss [0.05576253031095276]\n",
            "Model Trained at Step 3100: loss [0.0545551508916049]\n",
            "Model Trained at Step 3120: loss [0.05595723715649491]\n",
            "Model Trained at Step 3140: loss [0.055323675548636195]\n",
            "Model Trained at Step 3160: loss [0.054254860080036736]\n",
            "Model Trained at Step 3180: loss [0.05643382256242151]\n",
            "Model Trained at Step 3200: loss [0.052440951043909126]\n",
            "Model Trained at Step 3220: loss [0.05362640022919814]\n",
            "Model Trained at Step 3240: loss [0.05678131446060754]\n",
            "Model Trained at Step 3260: loss [0.05605567878745969]\n",
            "Model Trained at Step 3280: loss [0.055473403123848554]\n",
            "Model Trained at Step 3300: loss [0.05260874413270276]\n",
            "Model Trained at Step 3320: loss [0.05546323565558222]\n",
            "Model Trained at Step 3340: loss [0.05519251437109975]\n",
            "Model Trained at Step 3360: loss [0.055855315520342876]\n",
            "Model Trained at Step 3380: loss [0.05112160925866342]\n",
            "Model Trained at Step 3400: loss [0.05647853602503555]\n",
            "Model Trained at Step 3420: loss [0.05591066407223656]\n",
            "Model Trained at Step 3440: loss [0.05542694204406908]\n",
            "Model Trained at Step 3460: loss [0.05471610890763963]\n",
            "Model Trained at Step 3480: loss [0.05499551980042518]\n",
            "Model Trained at Step 3500: loss [0.05405011923417198]\n",
            "Model Trained at Step 3520: loss [0.05473402866013769]\n",
            "Model Trained at Step 3540: loss [0.05396322766643972]\n",
            "Model Trained at Step 3560: loss [0.05487294826781809]\n",
            "Model Trained at Step 3580: loss [0.05554746263435212]\n",
            "Model Trained at Step 3600: loss [0.05429969173696142]\n",
            "Model Trained at Step 3620: loss [0.0556307440992422]\n",
            "Model Trained at Step 3640: loss [0.05490198736396011]\n",
            "Model Trained at Step 3660: loss [0.05464483742736657]\n",
            "Model Trained at Step 3680: loss [0.05532649192465874]\n",
            "Model Trained at Step 3700: loss [0.0549114375475381]\n",
            "Model Trained at Step 3720: loss [0.05478014576513133]\n",
            "Model Trained at Step 3740: loss [0.052797927761063415]\n",
            "Model Trained at Step 3760: loss [0.05534907445575628]\n",
            "Model Trained at Step 3780: loss [0.05554431115415339]\n",
            "Model Trained at Step 3800: loss [0.05240034116800883]\n",
            "Model Trained at Step 3820: loss [0.05452941199738787]\n",
            "Model Trained at Step 3840: loss [0.055231022348572315]\n",
            "Model Trained at Step 3860: loss [0.05507356544794697]\n",
            "Model Trained at Step 3880: loss [0.05471686781382099]\n",
            "Model Trained at Step 3900: loss [0.055100243850323714]\n",
            "Model Trained at Step 3920: loss [0.054331383525394426]\n",
            "Model Trained at Step 3940: loss [0.053436909719026114]\n",
            "Model Trained at Step 3960: loss [0.054753569515750763]\n",
            "Model Trained at Step 3980: loss [0.05389322710366817]\n",
            "Model Trained at Step 4000: loss [0.054259838157990445]\n",
            "Model Trained at Step 4020: loss [0.05293116620763825]\n",
            "Model Trained at Step 4040: loss [0.05484541817934925]\n",
            "Model Trained at Step 4060: loss [0.055103380923518575]\n",
            "Model Trained at Step 4080: loss [0.05300964712973875]\n",
            "Model Trained at Step 4100: loss [0.05465647566174984]\n",
            "Model Trained at Step 4120: loss [0.05475842853293185]\n",
            "Model Trained at Step 4140: loss [0.05522012829883956]\n",
            "Model Trained at Step 4160: loss [0.054525184694000195]\n",
            "Model Trained at Step 4180: loss [0.053799506102919614]\n",
            "Model Trained at Step 4200: loss [0.055221266648573485]\n",
            "Model Trained at Step 4220: loss [0.052497237618444534]\n",
            "Model Trained at Step 4240: loss [0.055043705779932964]\n",
            "Model Trained at Step 4260: loss [0.05499239166983729]\n",
            "Model Trained at Step 4280: loss [0.05581585801687009]\n",
            "Model Trained at Step 4300: loss [0.05524949444227524]\n",
            "Model Trained at Step 4320: loss [0.055757379618960344]\n",
            "Model Trained at Step 4340: loss [0.0507134194397591]\n",
            "Model Trained at Step 4360: loss [0.055186862786335736]\n",
            "Model Trained at Step 4380: loss [0.05617069532480619]\n",
            "Model Trained at Step 4400: loss [0.05534085298326606]\n",
            "Model Trained at Step 4420: loss [0.05646918323635917]\n",
            "Model Trained at Step 4440: loss [0.052390911754089]\n",
            "Model Trained at Step 4460: loss [0.05583346788235784]\n",
            "Model Trained at Step 4480: loss [0.056242886077687085]\n",
            "Model Trained at Step 4500: loss [0.05541562566818039]\n",
            "Model Trained at Step 4520: loss [0.05498527069619389]\n",
            "Model Trained at Step 4540: loss [0.053334824291221924]\n",
            "Model Trained at Step 4560: loss [0.05540984152738243]\n",
            "Model Trained at Step 4580: loss [0.05572632746912306]\n",
            "Model Trained at Step 4600: loss [0.05376975002095154]\n",
            "Model Trained at Step 4620: loss [0.05423933202973817]\n",
            "Model Trained at Step 4640: loss [0.05515907386508993]\n",
            "Model Trained at Step 4660: loss [0.05407427332704816]\n",
            "Model Trained at Step 4680: loss [0.05488974027304751]\n",
            "Model Trained at Step 4700: loss [0.05346403590320607]\n",
            "Model Trained at Step 4720: loss [0.05513249082024317]\n",
            "Model Trained at Step 4740: loss [0.05573900946425381]\n",
            "Model Trained at Step 4760: loss [0.05566183502212203]\n",
            "Model Trained at Step 4780: loss [0.05737943855644183]\n",
            "Model Trained at Step 4800: loss [0.047180790702488866]\n",
            "Model Trained at Step 4820: loss [0.05386967512669659]\n",
            "Model Trained at Step 4840: loss [0.055552955934345685]\n",
            "Model Trained at Step 4860: loss [0.055166493785480754]\n",
            "Model Trained at Step 4880: loss [0.054631511453350844]\n",
            "Model Trained at Step 4900: loss [0.05520662952204517]\n",
            "Model Trained at Step 4920: loss [0.056099697987481136]\n",
            "Model Trained at Step 4940: loss [0.05174815272980796]\n",
            "Model Trained at Step 4960: loss [0.05554487153186701]\n",
            "Model Trained at Step 4980: loss [0.055622353741759854]\n",
            "Model Trained at Step 5000: loss [0.053946090874244804]\n",
            "Simulation wave training 9 done\n",
            "Model Trained at Step 20: loss [0.05003301660362667]\n",
            "Model Trained at Step 40: loss [0.04495060941524242]\n",
            "Model Trained at Step 60: loss [0.04501307004966766]\n",
            "Model Trained at Step 80: loss [0.04578300479537782]\n",
            "Model Trained at Step 100: loss [0.04710573524302043]\n",
            "Model Trained at Step 120: loss [0.04974227620603161]\n",
            "Model Trained at Step 140: loss [0.060115833366930625]\n",
            "Model Trained at Step 160: loss [0.06669044581815341]\n",
            "Model Trained at Step 180: loss [0.06475376885660207]\n",
            "Model Trained at Step 200: loss [0.062720559235518]\n",
            "Model Trained at Step 220: loss [0.06047610104375345]\n",
            "Model Trained at Step 240: loss [0.05805263823363468]\n",
            "Model Trained at Step 260: loss [0.05560599639584955]\n",
            "Model Trained at Step 280: loss [0.05315028395554746]\n",
            "Model Trained at Step 300: loss [0.05068134635279429]\n",
            "Model Trained at Step 320: loss [0.04825826847911361]\n",
            "Model Trained at Step 340: loss [0.045950544566438226]\n",
            "Model Trained at Step 360: loss [0.043798042311579924]\n",
            "Model Trained at Step 380: loss [0.04184741488498443]\n",
            "Model Trained at Step 400: loss [0.04015359083342359]\n",
            "Model Trained at Step 420: loss [0.03875808648511355]\n",
            "Model Trained at Step 440: loss [0.037675832451937355]\n",
            "Model Trained at Step 460: loss [0.03695298937953648]\n",
            "Model Trained at Step 480: loss [0.03680716078953096]\n",
            "Model Trained at Step 500: loss [0.03697655468901986]\n",
            "Model Trained at Step 520: loss [0.03688950317849833]\n",
            "Model Trained at Step 540: loss [0.03724863421509583]\n",
            "Model Trained at Step 560: loss [0.038103161738721004]\n",
            "Model Trained at Step 580: loss [0.038947644031667825]\n",
            "Model Trained at Step 600: loss [0.04004022544143405]\n",
            "Model Trained at Step 620: loss [0.04134910745065801]\n",
            "Model Trained at Step 640: loss [0.04291122558334912]\n",
            "Model Trained at Step 660: loss [0.04474980361300287]\n",
            "Model Trained at Step 680: loss [0.04686680445906382]\n",
            "Model Trained at Step 700: loss [0.04925397198669321]\n",
            "Model Trained at Step 720: loss [0.05186879188843409]\n",
            "Model Trained at Step 740: loss [0.05472596345643364]\n",
            "Model Trained at Step 760: loss [0.05788866090737136]\n",
            "Model Trained at Step 780: loss [0.0604517658841348]\n",
            "Model Trained at Step 800: loss [0.06346767863545244]\n",
            "Model Trained at Step 820: loss [0.06665199384538911]\n",
            "Model Trained at Step 840: loss [0.06709346488821454]\n",
            "Model Trained at Step 860: loss [0.06353489244799711]\n",
            "Model Trained at Step 880: loss [0.062484349562505405]\n",
            "Model Trained at Step 900: loss [0.05445772254997433]\n",
            "Model Trained at Step 920: loss [0.049692507311951774]\n",
            "Model Trained at Step 940: loss [0.04702262513945075]\n",
            "Model Trained at Step 960: loss [0.045660721156967385]\n",
            "Model Trained at Step 980: loss [0.04504100623824708]\n",
            "Model Trained at Step 1000: loss [0.04504881524061035]\n",
            "Model Trained at Step 1020: loss [0.046389933145317266]\n",
            "Model Trained at Step 1040: loss [0.05332478554187586]\n",
            "Model Trained at Step 1060: loss [0.04861783838128016]\n",
            "Model Trained at Step 1080: loss [0.04896550941685752]\n",
            "Model Trained at Step 1100: loss [0.05290263017584742]\n",
            "Model Trained at Step 1120: loss [0.05575762631551527]\n",
            "Model Trained at Step 1140: loss [0.060385638488005634]\n",
            "Model Trained at Step 1160: loss [0.05801582756850365]\n",
            "Model Trained at Step 1180: loss [0.05621474192138674]\n",
            "Model Trained at Step 1200: loss [0.05465677657882793]\n",
            "Model Trained at Step 1220: loss [0.052881220134928]\n",
            "Model Trained at Step 1240: loss [0.05111713739011907]\n",
            "Model Trained at Step 1260: loss [0.049454903560549004]\n",
            "Model Trained at Step 1280: loss [0.04790971112478004]\n",
            "Model Trained at Step 1300: loss [0.04647617742503679]\n",
            "Model Trained at Step 1320: loss [0.04515363459639084]\n",
            "Model Trained at Step 1340: loss [0.04394776263903853]\n",
            "Model Trained at Step 1360: loss [0.04286653018313684]\n",
            "Model Trained at Step 1380: loss [0.04191413974451129]\n",
            "Model Trained at Step 1400: loss [0.04110763556840283]\n",
            "Model Trained at Step 1420: loss [0.040480199160732075]\n",
            "Model Trained at Step 1440: loss [0.040051386971981275]\n",
            "Model Trained at Step 1460: loss [0.039738241045564296]\n",
            "Model Trained at Step 1480: loss [0.039448591892480234]\n",
            "Model Trained at Step 1500: loss [0.039399680348898194]\n",
            "Model Trained at Step 1520: loss [0.039656335515755835]\n",
            "Model Trained at Step 1540: loss [0.03997316884559314]\n",
            "Model Trained at Step 1560: loss [0.04045953455019875]\n",
            "Model Trained at Step 1580: loss [0.041149089991157045]\n",
            "Model Trained at Step 1600: loss [0.041935489344546995]\n",
            "Model Trained at Step 1620: loss [0.04284933230869114]\n",
            "Model Trained at Step 1640: loss [0.04391872890847697]\n",
            "Model Trained at Step 1660: loss [0.0450319104272668]\n",
            "Model Trained at Step 1680: loss [0.046349892879359554]\n",
            "Model Trained at Step 1700: loss [0.04770524776746784]\n",
            "Model Trained at Step 1720: loss [0.04911194284839844]\n",
            "Model Trained at Step 1740: loss [0.05065861649995977]\n",
            "Model Trained at Step 1760: loss [0.05274022885902692]\n",
            "Model Trained at Step 1780: loss [0.054682670039956326]\n",
            "Model Trained at Step 1800: loss [0.050851612461086625]\n",
            "Model Trained at Step 1820: loss [0.054526243951437146]\n",
            "Model Trained at Step 1840: loss [0.05047532758187191]\n",
            "Model Trained at Step 1860: loss [0.0560120088331451]\n",
            "Model Trained at Step 1880: loss [0.052646767727443845]\n",
            "Model Trained at Step 1900: loss [0.04943237575795904]\n",
            "Model Trained at Step 1920: loss [0.047531185372511844]\n",
            "Model Trained at Step 1940: loss [0.046288507318445915]\n",
            "Model Trained at Step 1960: loss [0.04555217560977605]\n",
            "Model Trained at Step 1980: loss [0.04535406084161027]\n",
            "Model Trained at Step 2000: loss [0.04580836367778904]\n",
            "Model Trained at Step 2020: loss [0.04736639217907761]\n",
            "Model Trained at Step 2040: loss [0.04963458653106771]\n",
            "Model Trained at Step 2060: loss [0.04877170252438109]\n",
            "Model Trained at Step 2080: loss [0.04826468608895637]\n",
            "Model Trained at Step 2100: loss [0.04869108758321479]\n",
            "Model Trained at Step 2120: loss [0.04957622015262981]\n",
            "Model Trained at Step 2140: loss [0.05073161689648224]\n",
            "Model Trained at Step 2160: loss [0.055503071957207405]\n",
            "Model Trained at Step 2180: loss [0.053216968471416735]\n",
            "Model Trained at Step 2200: loss [0.05421284599039874]\n",
            "Model Trained at Step 2220: loss [0.05201877523834986]\n",
            "Model Trained at Step 2240: loss [0.05111512990703706]\n",
            "Model Trained at Step 2260: loss [0.04949391790731118]\n",
            "Model Trained at Step 2280: loss [0.04865303572450011]\n",
            "Model Trained at Step 2300: loss [0.047619303403419584]\n",
            "Model Trained at Step 2320: loss [0.046475041959884146]\n",
            "Model Trained at Step 2340: loss [0.04547565910222888]\n",
            "Model Trained at Step 2360: loss [0.044621018239792976]\n",
            "Model Trained at Step 2380: loss [0.04396545869019074]\n",
            "Model Trained at Step 2400: loss [0.04342627109664748]\n",
            "Model Trained at Step 2420: loss [0.042954442698620116]\n",
            "Model Trained at Step 2440: loss [0.0424595904201633]\n",
            "Model Trained at Step 2460: loss [0.042211406289363734]\n",
            "Model Trained at Step 2480: loss [0.04221651412724915]\n",
            "Model Trained at Step 2500: loss [0.042209175991804904]\n",
            "Model Trained at Step 2520: loss [0.04227696650862096]\n",
            "Model Trained at Step 2540: loss [0.04264486057161269]\n",
            "Model Trained at Step 2560: loss [0.043007355136660305]\n",
            "Model Trained at Step 2580: loss [0.04352736140674055]\n",
            "Model Trained at Step 2600: loss [0.044198591613713756]\n",
            "Model Trained at Step 2620: loss [0.044882506547612]\n",
            "Model Trained at Step 2640: loss [0.0457609296246863]\n",
            "Model Trained at Step 2660: loss [0.046765994099002964]\n",
            "Model Trained at Step 2680: loss [0.04727124154106964]\n",
            "Model Trained at Step 2700: loss [0.04831787098623528]\n",
            "Model Trained at Step 2720: loss [0.048841198996199396]\n",
            "Model Trained at Step 2740: loss [0.04993638616108863]\n",
            "Model Trained at Step 2760: loss [0.050894643619453006]\n",
            "Model Trained at Step 2780: loss [0.051623383784263975]\n",
            "Model Trained at Step 2800: loss [0.04682818391096051]\n",
            "Model Trained at Step 2820: loss [0.05350485436077594]\n",
            "Model Trained at Step 2840: loss [0.04949875942749776]\n",
            "Model Trained at Step 2860: loss [0.058353226020169055]\n",
            "Model Trained at Step 2880: loss [0.055601072040207776]\n",
            "Model Trained at Step 2900: loss [0.053304254133771954]\n",
            "Model Trained at Step 2920: loss [0.05229987728786739]\n",
            "Model Trained at Step 2940: loss [0.05176859360869105]\n",
            "Model Trained at Step 2960: loss [0.05105906440664245]\n",
            "Model Trained at Step 2980: loss [0.05007298619796265]\n",
            "Model Trained at Step 3000: loss [0.04942638407880382]\n",
            "Model Trained at Step 3020: loss [0.04962699615621041]\n",
            "Model Trained at Step 3040: loss [0.05060778429485711]\n",
            "Model Trained at Step 3060: loss [0.05699775922831975]\n",
            "Model Trained at Step 3080: loss [0.04932286891016681]\n",
            "Model Trained at Step 3100: loss [0.052712381472640214]\n",
            "Model Trained at Step 3120: loss [0.04927724416432511]\n",
            "Model Trained at Step 3140: loss [0.053127774216665334]\n",
            "Model Trained at Step 3160: loss [0.04680176822098841]\n",
            "Model Trained at Step 3180: loss [0.05102688439443557]\n",
            "Model Trained at Step 3200: loss [0.046794688887343706]\n",
            "Model Trained at Step 3220: loss [0.049178700359068385]\n",
            "Model Trained at Step 3240: loss [0.04845211754439431]\n",
            "Model Trained at Step 3260: loss [0.047767284359515556]\n",
            "Model Trained at Step 3280: loss [0.04685641690787756]\n",
            "Model Trained at Step 3300: loss [0.046963727999674534]\n",
            "Model Trained at Step 3320: loss [0.04634910075933234]\n",
            "Model Trained at Step 3340: loss [0.04592825893316489]\n",
            "Model Trained at Step 3360: loss [0.04543396051832954]\n",
            "Model Trained at Step 3380: loss [0.04509265523259086]\n",
            "Model Trained at Step 3400: loss [0.044766359803296504]\n",
            "Model Trained at Step 3420: loss [0.044522803794435246]\n",
            "Model Trained at Step 3440: loss [0.04438850718673058]\n",
            "Model Trained at Step 3460: loss [0.044250514662783104]\n",
            "Model Trained at Step 3480: loss [0.04414340960491332]\n",
            "Model Trained at Step 3500: loss [0.04418173506202528]\n",
            "Model Trained at Step 3520: loss [0.04425176600947833]\n",
            "Model Trained at Step 3540: loss [0.0442959860759625]\n",
            "Model Trained at Step 3560: loss [0.04451035643217401]\n",
            "Model Trained at Step 3580: loss [0.04477998242656594]\n",
            "Model Trained at Step 3600: loss [0.04510026437767785]\n",
            "Model Trained at Step 3620: loss [0.04536276157901189]\n",
            "Model Trained at Step 3640: loss [0.045791998061681896]\n",
            "Model Trained at Step 3660: loss [0.04627653595372417]\n",
            "Model Trained at Step 3680: loss [0.04690272543930777]\n",
            "Model Trained at Step 3700: loss [0.04698556152566332]\n",
            "Model Trained at Step 3720: loss [0.047430386329454746]\n",
            "Model Trained at Step 3740: loss [0.047678989699511565]\n",
            "Model Trained at Step 3760: loss [0.04803538419633392]\n",
            "Model Trained at Step 3780: loss [0.04834278377349984]\n",
            "Model Trained at Step 3800: loss [0.04915685454538858]\n",
            "Model Trained at Step 3820: loss [0.05055867729556238]\n",
            "Model Trained at Step 3840: loss [0.04739678253424556]\n",
            "Model Trained at Step 3860: loss [0.0500217822340608]\n",
            "Model Trained at Step 3880: loss [0.04782592410996184]\n",
            "Model Trained at Step 3900: loss [0.05048186074977279]\n",
            "Model Trained at Step 3920: loss [0.04555742651182339]\n",
            "Model Trained at Step 3940: loss [0.0476718913709975]\n",
            "Model Trained at Step 3960: loss [0.054494608015855796]\n",
            "Model Trained at Step 3980: loss [0.044097467010813116]\n",
            "Model Trained at Step 4000: loss [0.04544654296109839]\n",
            "Model Trained at Step 4020: loss [0.047083857212548756]\n",
            "Model Trained at Step 4040: loss [0.04914704321516439]\n",
            "Model Trained at Step 4060: loss [0.05458177501449689]\n",
            "Model Trained at Step 4080: loss [0.04784577681969311]\n",
            "Model Trained at Step 4100: loss [0.05227857632958667]\n",
            "Model Trained at Step 4120: loss [0.0481130332320823]\n",
            "Model Trained at Step 4140: loss [0.05217291252835267]\n",
            "Model Trained at Step 4160: loss [0.04439524371806798]\n",
            "Model Trained at Step 4180: loss [0.047514287294564875]\n",
            "Model Trained at Step 4200: loss [0.05043404217534571]\n",
            "Model Trained at Step 4220: loss [0.045003351843162294]\n",
            "Model Trained at Step 4240: loss [0.048770932888815115]\n",
            "Model Trained at Step 4260: loss [0.04693524094859129]\n",
            "Model Trained at Step 4280: loss [0.04828191450059433]\n",
            "Model Trained at Step 4300: loss [0.04793418678133239]\n",
            "Model Trained at Step 4320: loss [0.04666436596205107]\n",
            "Model Trained at Step 4340: loss [0.04738993455955766]\n",
            "Model Trained at Step 4360: loss [0.04700038186339382]\n",
            "Model Trained at Step 4380: loss [0.046828993536551886]\n",
            "Model Trained at Step 4400: loss [0.04650014095718015]\n",
            "Model Trained at Step 4420: loss [0.04619514691997309]\n",
            "Model Trained at Step 4440: loss [0.04601096731444945]\n",
            "Model Trained at Step 4460: loss [0.04604208450274246]\n",
            "Model Trained at Step 4480: loss [0.04599813207800657]\n",
            "Model Trained at Step 4500: loss [0.0460591505143839]\n",
            "Model Trained at Step 4520: loss [0.0460363995743092]\n",
            "Model Trained at Step 4540: loss [0.04612927971866456]\n",
            "Model Trained at Step 4560: loss [0.04632881516329098]\n",
            "Model Trained at Step 4580: loss [0.04661311670794631]\n",
            "Model Trained at Step 4600: loss [0.04690751778562674]\n",
            "Model Trained at Step 4620: loss [0.046633590947138204]\n",
            "Model Trained at Step 4640: loss [0.046915305422563815]\n",
            "Model Trained at Step 4660: loss [0.04730155707840565]\n",
            "Model Trained at Step 4680: loss [0.04734950155245636]\n",
            "Model Trained at Step 4700: loss [0.04734636730366735]\n",
            "Model Trained at Step 4720: loss [0.047783993166540854]\n",
            "Model Trained at Step 4740: loss [0.047899760478920375]\n",
            "Model Trained at Step 4760: loss [0.048011840667103026]\n",
            "Model Trained at Step 4780: loss [0.04871707193030874]\n",
            "Model Trained at Step 4800: loss [0.04910727905227126]\n",
            "Model Trained at Step 4820: loss [0.0480882490834184]\n",
            "Model Trained at Step 4840: loss [0.04896937000126485]\n",
            "Model Trained at Step 4860: loss [0.047936989529572364]\n",
            "Model Trained at Step 4880: loss [0.04933420063663088]\n",
            "Model Trained at Step 4900: loss [0.04718481680747747]\n",
            "Model Trained at Step 4920: loss [0.049479940127912195]\n",
            "Model Trained at Step 4940: loss [0.05045509906355459]\n",
            "Model Trained at Step 4960: loss [0.046209736200198984]\n",
            "Model Trained at Step 4980: loss [0.05030482464905904]\n",
            "Model Trained at Step 5000: loss [0.04873125015840861]\n",
            "Simulation wave training 10 done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ev_mod = Sequential([Conv2D(1, kernel_size=11, padding='same', padding_mode='wrap', activation='relu', input_size=(500,500,1))],\n",
        "                    loss='mse', lr=1e-7)\n",
        "\n",
        "for i in range(10):\n",
        "    sim = SchroSim()\n",
        "    sim.add_proton(pos=randint(-2, 3, 2))\n",
        "    sim.add_electron(p=randint(-20, 21, 2), pos=randint(-2, 3, 2))\n",
        "    sim.add_electron(p=randint(-20, 21, 2), pos=randint(-2, 3, 2))\n",
        "\n",
        "    mod = EVModel(model=ev_mod, sim=sim)\n",
        "    sim.simulate(dims=(5,5), dau=1e-2, time_step=1e-1, steps=5000, model=mod, train_model=20, ev_samp_rate='full')\n",
        "    ev_mod.save(path='./ev1_checks/', name=f'ev1_{i+1}')\n",
        "    print(f'Simulation ev training {i+1} done')\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n-6Nm9Ek6n-",
        "outputId": "356ed8d4-f92d-4257-dd38-43c15171eb76"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Trained at Step 20: loss [3.9983947431408766]\n",
            "Model Trained at Step 40: loss [2.745314880293292]\n",
            "Model Trained at Step 60: loss [2.799250393596618]\n",
            "Model Trained at Step 80: loss [2.7922966345275437]\n",
            "Model Trained at Step 100: loss [2.786835999285194]\n",
            "Model Trained at Step 120: loss [2.779938272861137]\n",
            "Model Trained at Step 140: loss [2.765143389508612]\n",
            "Model Trained at Step 160: loss [2.71893718776563]\n",
            "Model Trained at Step 180: loss [2.682455635856931]\n",
            "Model Trained at Step 200: loss [2.642248631482377]\n",
            "Model Trained at Step 220: loss [2.6012314392497604]\n",
            "Model Trained at Step 240: loss [2.571808219183864]\n",
            "Model Trained at Step 260: loss [2.5406557582179503]\n",
            "Model Trained at Step 280: loss [2.503498400301546]\n",
            "Model Trained at Step 300: loss [2.453588776650002]\n",
            "Model Trained at Step 320: loss [2.4043153217564552]\n",
            "Model Trained at Step 340: loss [2.353846437463404]\n",
            "Model Trained at Step 360: loss [2.3039906507417554]\n",
            "Model Trained at Step 380: loss [2.245162548368134]\n",
            "Model Trained at Step 400: loss [2.2018854744135337]\n",
            "Model Trained at Step 420: loss [2.1356312391700185]\n",
            "Model Trained at Step 440: loss [2.06556546754424]\n",
            "Model Trained at Step 460: loss [2.06646405515195]\n",
            "Model Trained at Step 480: loss [2.015107227246358]\n",
            "Model Trained at Step 500: loss [1.9301425672960988]\n",
            "Model Trained at Step 520: loss [1.8963521817958227]\n",
            "Model Trained at Step 540: loss [1.8232432513662835]\n",
            "Model Trained at Step 560: loss [1.7626120204475533]\n",
            "Model Trained at Step 580: loss [1.696964842021395]\n",
            "Model Trained at Step 600: loss [1.669826854151752]\n",
            "Model Trained at Step 620: loss [1.6372655676845338]\n",
            "Model Trained at Step 640: loss [1.5857582037577407]\n",
            "Model Trained at Step 660: loss [1.5730423269113456]\n",
            "Model Trained at Step 680: loss [1.594563265132398]\n",
            "Model Trained at Step 700: loss [1.5889232028544968]\n",
            "Model Trained at Step 720: loss [1.6149080055197884]\n",
            "Model Trained at Step 740: loss [1.6530188084764006]\n",
            "Model Trained at Step 760: loss [1.6188170949305623]\n",
            "Model Trained at Step 780: loss [1.604717646291266]\n",
            "Model Trained at Step 800: loss [1.6205740927182983]\n",
            "Model Trained at Step 820: loss [1.471612341555946]\n",
            "Model Trained at Step 840: loss [1.3497799814719316]\n",
            "Model Trained at Step 860: loss [1.4074834252454163]\n",
            "Model Trained at Step 880: loss [1.4620578209581452]\n",
            "Model Trained at Step 900: loss [1.5204862101080319]\n",
            "Model Trained at Step 920: loss [1.5614705197160066]\n",
            "Model Trained at Step 940: loss [1.5881366373307773]\n",
            "Model Trained at Step 960: loss [1.616044384713301]\n",
            "Model Trained at Step 980: loss [1.6633696981866755]\n",
            "Model Trained at Step 1000: loss [1.6995762183414747]\n",
            "Model Trained at Step 1020: loss [1.728365624364733]\n",
            "Model Trained at Step 1040: loss [1.7425236186820245]\n",
            "Model Trained at Step 1060: loss [1.7675073879365215]\n",
            "Model Trained at Step 1080: loss [1.8029273076081764]\n",
            "Model Trained at Step 1100: loss [1.834901678108888]\n",
            "Model Trained at Step 1120: loss [1.8646125490630205]\n",
            "Model Trained at Step 1140: loss [1.8941200497476953]\n",
            "Model Trained at Step 1160: loss [1.918542396218097]\n",
            "Model Trained at Step 1180: loss [1.940473400928898]\n",
            "Model Trained at Step 1200: loss [1.9593824645118079]\n",
            "Model Trained at Step 1220: loss [1.9728515440001833]\n",
            "Model Trained at Step 1240: loss [1.9631869462065379]\n",
            "Model Trained at Step 1260: loss [1.998885102499959]\n",
            "Model Trained at Step 1280: loss [1.9866340801847198]\n",
            "Model Trained at Step 1300: loss [2.094553979116781]\n",
            "Model Trained at Step 1320: loss [2.1435751053182535]\n",
            "Model Trained at Step 1340: loss [2.172737733332468]\n",
            "Model Trained at Step 1360: loss [2.200917457341072]\n",
            "Model Trained at Step 1380: loss [2.2269660892528798]\n",
            "Model Trained at Step 1400: loss [2.251216223508175]\n",
            "Model Trained at Step 1420: loss [2.2737508956247217]\n",
            "Model Trained at Step 1440: loss [2.293996206310174]\n",
            "Model Trained at Step 1460: loss [2.310546551197521]\n",
            "Model Trained at Step 1480: loss [2.33166159424306]\n",
            "Model Trained at Step 1500: loss [2.3475935490917044]\n",
            "Model Trained at Step 1520: loss [2.3607293603289285]\n",
            "Model Trained at Step 1540: loss [2.3675216994982735]\n",
            "Model Trained at Step 1560: loss [2.368467646053266]\n",
            "Model Trained at Step 1580: loss [2.3678589314272713]\n",
            "Model Trained at Step 1600: loss [2.3624956327780384]\n",
            "Model Trained at Step 1620: loss [2.3577816170421557]\n",
            "Model Trained at Step 1640: loss [2.350295651433676]\n",
            "Model Trained at Step 1660: loss [2.3372036630429425]\n",
            "Model Trained at Step 1680: loss [2.322342497496506]\n",
            "Model Trained at Step 1700: loss [2.3025400196087027]\n",
            "Model Trained at Step 1720: loss [2.2826632755013785]\n",
            "Model Trained at Step 1740: loss [2.2618283179508314]\n",
            "Model Trained at Step 1760: loss [2.238374044935746]\n",
            "Model Trained at Step 1780: loss [2.2149938424787643]\n",
            "Model Trained at Step 1800: loss [2.1885900622096868]\n",
            "Model Trained at Step 1820: loss [2.163007925262182]\n",
            "Model Trained at Step 1840: loss [2.133517768646388]\n",
            "Model Trained at Step 1860: loss [2.103305280453871]\n",
            "Model Trained at Step 1880: loss [2.0823015006179686]\n",
            "Model Trained at Step 1900: loss [2.0650007232724326]\n",
            "Model Trained at Step 1920: loss [2.043245941618695]\n",
            "Model Trained at Step 1940: loss [2.031067459347315]\n",
            "Model Trained at Step 1960: loss [2.0149866932229314]\n",
            "Model Trained at Step 1980: loss [2.0077534023718084]\n",
            "Model Trained at Step 2000: loss [2.010636388300608]\n",
            "Model Trained at Step 2020: loss [2.013983827252967]\n",
            "Model Trained at Step 2040: loss [2.016464156320002]\n",
            "Model Trained at Step 2060: loss [2.024772820538449]\n",
            "Model Trained at Step 2080: loss [2.039811821831447]\n",
            "Model Trained at Step 2100: loss [2.0508667385224073]\n",
            "Model Trained at Step 2120: loss [2.065147290084278]\n",
            "Model Trained at Step 2140: loss [2.0918438449284267]\n",
            "Model Trained at Step 2160: loss [2.111868895734304]\n",
            "Model Trained at Step 2180: loss [2.1487578113582337]\n",
            "Model Trained at Step 2200: loss [2.1517356664830767]\n",
            "Model Trained at Step 2220: loss [2.164453857078413]\n",
            "Model Trained at Step 2240: loss [2.1517015494399914]\n",
            "Model Trained at Step 2260: loss [2.117128625503549]\n",
            "Model Trained at Step 2280: loss [2.0558229329630513]\n",
            "Model Trained at Step 2300: loss [2.0321116134588473]\n",
            "Model Trained at Step 2320: loss [1.9780607664173964]\n",
            "Model Trained at Step 2340: loss [1.9322040662451645]\n",
            "Model Trained at Step 2360: loss [1.9326965122034252]\n",
            "Model Trained at Step 2380: loss [1.8893770429201062]\n",
            "Model Trained at Step 2400: loss [1.8839049677991084]\n",
            "Model Trained at Step 2420: loss [1.8484136893336882]\n",
            "Model Trained at Step 2440: loss [1.8306586461706054]\n",
            "Model Trained at Step 2460: loss [1.782659342603592]\n",
            "Model Trained at Step 2480: loss [1.7893920881766676]\n",
            "Model Trained at Step 2500: loss [1.7678463178486694]\n",
            "Model Trained at Step 2520: loss [1.800952033251629]\n",
            "Model Trained at Step 2540: loss [1.7848442757144127]\n",
            "Model Trained at Step 2560: loss [1.759529661205145]\n",
            "Model Trained at Step 2580: loss [1.75345869270948]\n",
            "Model Trained at Step 2600: loss [1.756971269072805]\n",
            "Model Trained at Step 2620: loss [1.7590850727179443]\n",
            "Model Trained at Step 2640: loss [1.7671787345673546]\n",
            "Model Trained at Step 2660: loss [1.7766530535363259]\n",
            "Model Trained at Step 2680: loss [1.7829293174364167]\n",
            "Model Trained at Step 2700: loss [1.842582330012658]\n",
            "Model Trained at Step 2720: loss [1.9601963498721218]\n",
            "Model Trained at Step 2740: loss [2.0075047098824568]\n",
            "Model Trained at Step 2760: loss [1.9944941332920352]\n",
            "Model Trained at Step 2780: loss [2.029114205827871]\n",
            "Model Trained at Step 2800: loss [2.1093999609429197]\n",
            "Model Trained at Step 2820: loss [2.119155165985982]\n",
            "Model Trained at Step 2840: loss [2.1049778201675133]\n",
            "Model Trained at Step 2860: loss [2.0296397249012004]\n",
            "Model Trained at Step 2880: loss [2.1724030889559964]\n",
            "Model Trained at Step 2900: loss [2.18338428838293]\n",
            "Model Trained at Step 2920: loss [2.1603468555461087]\n",
            "Model Trained at Step 2940: loss [2.251052677672794]\n",
            "Model Trained at Step 2960: loss [2.2267914716509223]\n",
            "Model Trained at Step 2980: loss [2.2165056634893676]\n",
            "Model Trained at Step 3000: loss [2.3696905728275026]\n",
            "Model Trained at Step 3020: loss [2.397729682784289]\n",
            "Model Trained at Step 3040: loss [2.417137416059979]\n",
            "Model Trained at Step 3060: loss [2.434492615378045]\n",
            "Model Trained at Step 3080: loss [2.4528459211301206]\n",
            "Model Trained at Step 3100: loss [2.4685885946145625]\n",
            "Model Trained at Step 3120: loss [2.478612724364966]\n",
            "Model Trained at Step 3140: loss [2.481707568354488]\n",
            "Model Trained at Step 3160: loss [2.473531359374236]\n",
            "Model Trained at Step 3180: loss [2.465675793712147]\n",
            "Model Trained at Step 3200: loss [2.4552509610121724]\n",
            "Model Trained at Step 3220: loss [2.442151624695537]\n",
            "Model Trained at Step 3240: loss [2.4237730787533147]\n",
            "Model Trained at Step 3260: loss [2.400216388511061]\n",
            "Model Trained at Step 3280: loss [2.3765069207174214]\n",
            "Model Trained at Step 3300: loss [2.3437932358330147]\n",
            "Model Trained at Step 3320: loss [2.308838314978746]\n",
            "Model Trained at Step 3340: loss [2.2777622804474875]\n",
            "Model Trained at Step 3360: loss [2.2474619146622565]\n",
            "Model Trained at Step 3380: loss [2.2178486010187886]\n",
            "Model Trained at Step 3400: loss [2.2356159159957256]\n",
            "Model Trained at Step 3420: loss [2.2028565566499316]\n",
            "Model Trained at Step 3440: loss [2.168035769292728]\n",
            "Model Trained at Step 3460: loss [2.1447450434670907]\n",
            "Model Trained at Step 3480: loss [2.1025989759262993]\n",
            "Model Trained at Step 3500: loss [2.07805160946151]\n",
            "Model Trained at Step 3520: loss [2.11403957324191]\n",
            "Model Trained at Step 3540: loss [2.092239804504396]\n",
            "Model Trained at Step 3560: loss [2.066945099460738]\n",
            "Model Trained at Step 3580: loss [2.044262107810937]\n",
            "Model Trained at Step 3600: loss [2.029511372282054]\n",
            "Model Trained at Step 3620: loss [2.0131793497604478]\n",
            "Model Trained at Step 3640: loss [1.9962822035403298]\n",
            "Model Trained at Step 3660: loss [1.9945098502361496]\n",
            "Model Trained at Step 3680: loss [1.9955422047469054]\n",
            "Model Trained at Step 3700: loss [1.9953019800749676]\n",
            "Model Trained at Step 3720: loss [2.0113004318151306]\n",
            "Model Trained at Step 3740: loss [2.024048386717001]\n",
            "Model Trained at Step 3760: loss [2.03078647802781]\n",
            "Model Trained at Step 3780: loss [2.0423194568126903]\n",
            "Model Trained at Step 3800: loss [2.057400320122465]\n",
            "Model Trained at Step 3820: loss [2.0700843884290565]\n",
            "Model Trained at Step 3840: loss [2.0814851866565576]\n",
            "Model Trained at Step 3860: loss [2.086629706073648]\n",
            "Model Trained at Step 3880: loss [2.097356397550043]\n",
            "Model Trained at Step 3900: loss [2.0846562830014173]\n",
            "Model Trained at Step 3920: loss [2.0769784202683796]\n",
            "Model Trained at Step 3940: loss [2.051454217457858]\n",
            "Model Trained at Step 3960: loss [2.018667580530027]\n",
            "Model Trained at Step 3980: loss [1.9867859079831476]\n",
            "Model Trained at Step 4000: loss [1.962281620209057]\n",
            "Model Trained at Step 4020: loss [1.9463117066224587]\n",
            "Model Trained at Step 4040: loss [1.960825206650604]\n",
            "Model Trained at Step 4060: loss [1.9360827437485728]\n",
            "Model Trained at Step 4080: loss [1.9308127935780544]\n",
            "Model Trained at Step 4100: loss [1.9167709525593075]\n",
            "Model Trained at Step 4120: loss [1.9669159443053055]\n",
            "Model Trained at Step 4140: loss [2.004408485797527]\n",
            "Model Trained at Step 4160: loss [1.9391594658548503]\n",
            "Model Trained at Step 4180: loss [1.9789130394206302]\n",
            "Model Trained at Step 4200: loss [1.918505839481877]\n",
            "Model Trained at Step 4220: loss [1.9425893023687704]\n",
            "Model Trained at Step 4240: loss [1.9018273023190517]\n",
            "Model Trained at Step 4260: loss [2.00426980611886]\n",
            "Model Trained at Step 4280: loss [2.0725531064980953]\n",
            "Model Trained at Step 4300: loss [2.0327293015349257]\n",
            "Model Trained at Step 4320: loss [2.098261360497598]\n",
            "Model Trained at Step 4340: loss [2.1721436357953445]\n",
            "Model Trained at Step 4360: loss [2.1877780638978193]\n",
            "Model Trained at Step 4380: loss [2.1704964322971314]\n",
            "Model Trained at Step 4400: loss [2.2175705976370175]\n",
            "Model Trained at Step 4420: loss [2.281731039074332]\n",
            "Model Trained at Step 4440: loss [2.3669769878755345]\n",
            "Model Trained at Step 4460: loss [2.477941592193508]\n",
            "Model Trained at Step 4480: loss [2.511776208945805]\n",
            "Model Trained at Step 4500: loss [2.535614471923342]\n",
            "Model Trained at Step 4520: loss [2.528137392653601]\n",
            "Model Trained at Step 4540: loss [2.4749801074516244]\n",
            "Model Trained at Step 4560: loss [2.4399506420625325]\n",
            "Model Trained at Step 4580: loss [2.497085833479465]\n",
            "Model Trained at Step 4600: loss [2.6210832616810373]\n",
            "Model Trained at Step 4620: loss [2.6200853700559614]\n",
            "Model Trained at Step 4640: loss [2.524391210508564]\n",
            "Model Trained at Step 4660: loss [2.6796476734371986]\n",
            "Model Trained at Step 4680: loss [2.819480598099773]\n",
            "Model Trained at Step 4700: loss [2.8311770559372853]\n",
            "Model Trained at Step 4720: loss [2.84443355155073]\n",
            "Model Trained at Step 4740: loss [2.8401396222364386]\n",
            "Model Trained at Step 4760: loss [2.83023824897083]\n",
            "Model Trained at Step 4780: loss [2.8248243233701658]\n",
            "Model Trained at Step 4800: loss [2.819580680026146]\n",
            "Model Trained at Step 4820: loss [2.8158480404235067]\n",
            "Model Trained at Step 4840: loss [2.812051159517493]\n",
            "Model Trained at Step 4860: loss [2.798777073137686]\n",
            "Model Trained at Step 4880: loss [2.785233217869687]\n",
            "Model Trained at Step 4900: loss [2.7660017807126307]\n",
            "Model Trained at Step 4920: loss [2.7495689209689047]\n",
            "Model Trained at Step 4940: loss [2.72814242166796]\n",
            "Model Trained at Step 4960: loss [2.712935673653363]\n",
            "Model Trained at Step 4980: loss [2.6965975224529912]\n",
            "Model Trained at Step 5000: loss [2.6683419268136617]\n",
            "Simulation ev training 1 done\n",
            "Model Trained at Step 20: loss [2.3593475488283993]\n",
            "Model Trained at Step 40: loss [2.1126522082516956]\n",
            "Model Trained at Step 60: loss [2.0984147447547086]\n",
            "Model Trained at Step 80: loss [2.0316565920374594]\n",
            "Model Trained at Step 100: loss [2.0223013489872197]\n",
            "Model Trained at Step 120: loss [1.9973674966592025]\n",
            "Model Trained at Step 140: loss [1.9784723603445005]\n",
            "Model Trained at Step 160: loss [1.950533351385681]\n",
            "Model Trained at Step 180: loss [1.9340342643030854]\n",
            "Model Trained at Step 200: loss [1.9167776559386671]\n",
            "Model Trained at Step 220: loss [1.9010257151344656]\n",
            "Model Trained at Step 240: loss [1.8894136242799704]\n",
            "Model Trained at Step 260: loss [1.8574009231461381]\n",
            "Model Trained at Step 280: loss [1.8726678109739088]\n",
            "Model Trained at Step 300: loss [1.8614797994421894]\n",
            "Model Trained at Step 320: loss [1.851104523673761]\n",
            "Model Trained at Step 340: loss [1.8647778401376993]\n",
            "Model Trained at Step 360: loss [1.8917850562932048]\n",
            "Model Trained at Step 380: loss [1.9056021571599597]\n",
            "Model Trained at Step 400: loss [1.9115955131215432]\n",
            "Model Trained at Step 420: loss [1.9085736541838216]\n",
            "Model Trained at Step 440: loss [1.8986821885309708]\n",
            "Model Trained at Step 460: loss [1.8885276362136978]\n",
            "Model Trained at Step 480: loss [1.877322487234305]\n",
            "Model Trained at Step 500: loss [1.8697946476870295]\n",
            "Model Trained at Step 520: loss [1.8830684680929752]\n",
            "Model Trained at Step 540: loss [1.8948088811761057]\n",
            "Model Trained at Step 560: loss [1.9027050180723073]\n",
            "Model Trained at Step 580: loss [1.9094494433322549]\n",
            "Model Trained at Step 600: loss [1.9161029172587916]\n",
            "Model Trained at Step 620: loss [1.927354099052355]\n",
            "Model Trained at Step 640: loss [1.9446757439805988]\n",
            "Model Trained at Step 660: loss [1.960092087083067]\n",
            "Model Trained at Step 680: loss [1.9936286159943666]\n",
            "Model Trained at Step 700: loss [2.0393594547510565]\n",
            "Model Trained at Step 720: loss [2.07794978216384]\n",
            "Model Trained at Step 740: loss [2.124217604648814]\n",
            "Model Trained at Step 760: loss [2.1710066413473923]\n",
            "Model Trained at Step 780: loss [2.2325665749352135]\n",
            "Model Trained at Step 800: loss [2.2943203699794648]\n",
            "Model Trained at Step 820: loss [2.3329733944624786]\n",
            "Model Trained at Step 840: loss [2.369748377851628]\n",
            "Model Trained at Step 860: loss [2.401471160496297]\n",
            "Model Trained at Step 880: loss [2.428464497838094]\n",
            "Model Trained at Step 900: loss [2.4525704704948477]\n",
            "Model Trained at Step 920: loss [2.4586643606013254]\n",
            "Model Trained at Step 940: loss [2.4662355805363907]\n",
            "Model Trained at Step 960: loss [2.4531834480421884]\n",
            "Model Trained at Step 980: loss [2.4275445870728354]\n",
            "Model Trained at Step 1000: loss [2.405665967061231]\n",
            "Model Trained at Step 1020: loss [2.379323028779071]\n",
            "Model Trained at Step 1040: loss [2.349534724958393]\n",
            "Model Trained at Step 1060: loss [2.3209674320056295]\n",
            "Model Trained at Step 1080: loss [2.2900025290906063]\n",
            "Model Trained at Step 1100: loss [2.266211785168435]\n",
            "Model Trained at Step 1120: loss [2.2380180282431086]\n",
            "Model Trained at Step 1140: loss [2.206928768140216]\n",
            "Model Trained at Step 1160: loss [2.1779017593293624]\n",
            "Model Trained at Step 1180: loss [2.153715141166095]\n",
            "Model Trained at Step 1200: loss [2.1211678024659175]\n",
            "Model Trained at Step 1220: loss [2.0829242054248667]\n",
            "Model Trained at Step 1240: loss [2.0629955773411575]\n",
            "Model Trained at Step 1260: loss [2.0427781544242536]\n",
            "Model Trained at Step 1280: loss [2.0188010812414428]\n",
            "Model Trained at Step 1300: loss [1.9969909693181882]\n",
            "Model Trained at Step 1320: loss [2.0051030636849463]\n",
            "Model Trained at Step 1340: loss [2.0201492980550073]\n",
            "Model Trained at Step 1360: loss [2.0472651654297107]\n",
            "Model Trained at Step 1380: loss [2.042184685980819]\n",
            "Model Trained at Step 1400: loss [2.090800397648414]\n",
            "Model Trained at Step 1420: loss [2.072343103079567]\n",
            "Model Trained at Step 1440: loss [2.038543282470993]\n",
            "Model Trained at Step 1460: loss [2.1333698370648513]\n",
            "Model Trained at Step 1480: loss [2.1345786038384644]\n",
            "Model Trained at Step 1500: loss [2.115332085276416]\n",
            "Model Trained at Step 1520: loss [2.0662120441881684]\n",
            "Model Trained at Step 1540: loss [1.9892512047308462]\n",
            "Model Trained at Step 1560: loss [2.0258083438539174]\n",
            "Model Trained at Step 1580: loss [2.164160460747767]\n",
            "Model Trained at Step 1600: loss [2.174901358820509]\n",
            "Model Trained at Step 1620: loss [2.175318562758927]\n",
            "Model Trained at Step 1640: loss [2.171966070444127]\n",
            "Model Trained at Step 1660: loss [2.1641558294227363]\n",
            "Model Trained at Step 1680: loss [2.155735427596186]\n",
            "Model Trained at Step 1700: loss [2.1350095452940976]\n",
            "Model Trained at Step 1720: loss [2.102812579649465]\n",
            "Model Trained at Step 1740: loss [2.0274242553469812]\n",
            "Model Trained at Step 1760: loss [2.0129275603231465]\n",
            "Model Trained at Step 1780: loss [2.020064033739354]\n",
            "Model Trained at Step 1800: loss [2.09331979561633]\n",
            "Model Trained at Step 1820: loss [2.142918438315646]\n",
            "Model Trained at Step 1840: loss [2.147814351224882]\n",
            "Model Trained at Step 1860: loss [2.1546065662829816]\n",
            "Model Trained at Step 1880: loss [2.1511984921824823]\n",
            "Model Trained at Step 1900: loss [2.0967009155900445]\n",
            "Model Trained at Step 1920: loss [2.158045796199059]\n",
            "Model Trained at Step 1940: loss [2.3181769934930445]\n",
            "Model Trained at Step 1960: loss [2.3545379905980637]\n",
            "Model Trained at Step 1980: loss [2.3772244724218767]\n",
            "Model Trained at Step 2000: loss [2.394497026180087]\n",
            "Model Trained at Step 2020: loss [2.409540924664763]\n",
            "Model Trained at Step 2040: loss [2.4204215202590924]\n",
            "Model Trained at Step 2060: loss [2.4221167998549435]\n",
            "Model Trained at Step 2080: loss [2.3632730946165785]\n",
            "Model Trained at Step 2100: loss [2.458215972786026]\n",
            "Model Trained at Step 2120: loss [2.6604303390546535]\n",
            "Model Trained at Step 2140: loss [2.6976338074443382]\n",
            "Model Trained at Step 2160: loss [2.724829981522288]\n",
            "Model Trained at Step 2180: loss [2.7552224105191967]\n",
            "Model Trained at Step 2200: loss [2.784796431886084]\n",
            "Model Trained at Step 2220: loss [2.816335327281849]\n",
            "Model Trained at Step 2240: loss [2.8396407868841]\n",
            "Model Trained at Step 2260: loss [2.861791788405402]\n",
            "Model Trained at Step 2280: loss [2.8785800721936403]\n",
            "Model Trained at Step 2300: loss [2.8967097739339405]\n",
            "Model Trained at Step 2320: loss [2.912015659188257]\n",
            "Model Trained at Step 2340: loss [2.922697017066281]\n",
            "Model Trained at Step 2360: loss [2.9379352399600336]\n",
            "Model Trained at Step 2380: loss [2.946349966877818]\n",
            "Model Trained at Step 2400: loss [2.95366061115765]\n",
            "Model Trained at Step 2420: loss [2.9616948811133996]\n",
            "Model Trained at Step 2440: loss [2.9729175273430952]\n",
            "Model Trained at Step 2460: loss [2.9794794814120706]\n",
            "Model Trained at Step 2480: loss [2.9814786764129955]\n",
            "Model Trained at Step 2500: loss [2.98614065587942]\n",
            "Model Trained at Step 2520: loss [2.9812120360988006]\n",
            "Model Trained at Step 2540: loss [2.973790631561286]\n",
            "Model Trained at Step 2560: loss [2.9536155419826735]\n",
            "Model Trained at Step 2580: loss [2.921485613902837]\n",
            "Model Trained at Step 2600: loss [2.885164795707242]\n",
            "Model Trained at Step 2620: loss [2.8394637024535467]\n",
            "Model Trained at Step 2640: loss [2.78214973165638]\n",
            "Model Trained at Step 2660: loss [2.7147161398813546]\n",
            "Model Trained at Step 2680: loss [2.6094728418527877]\n",
            "Model Trained at Step 2700: loss [2.4891787186991086]\n",
            "Model Trained at Step 2720: loss [2.430944749703857]\n",
            "Model Trained at Step 2740: loss [2.3961067421048328]\n",
            "Model Trained at Step 2760: loss [2.4288100056803796]\n",
            "Model Trained at Step 2780: loss [2.4696581694559376]\n",
            "Model Trained at Step 2800: loss [2.459727301619779]\n",
            "Model Trained at Step 2820: loss [2.4897602350717207]\n",
            "Model Trained at Step 2840: loss [2.6623173560177515]\n",
            "Model Trained at Step 2860: loss [2.664404122765952]\n",
            "Model Trained at Step 2880: loss [2.5640699897550725]\n",
            "Model Trained at Step 2900: loss [2.5758491308935856]\n",
            "Model Trained at Step 2920: loss [2.6307818426586094]\n",
            "Model Trained at Step 2940: loss [2.7108574609734033]\n",
            "Model Trained at Step 2960: loss [2.6239285840085573]\n",
            "Model Trained at Step 2980: loss [2.827902276602228]\n",
            "Model Trained at Step 3000: loss [2.873553268171799]\n",
            "Model Trained at Step 3020: loss [2.706941520419817]\n",
            "Model Trained at Step 3040: loss [2.8941129537108976]\n",
            "Model Trained at Step 3060: loss [2.7653911905363655]\n",
            "Model Trained at Step 3080: loss [2.849111108525629]\n",
            "Model Trained at Step 3100: loss [3.083761367291545]\n",
            "Model Trained at Step 3120: loss [2.9059727781892875]\n",
            "Model Trained at Step 3140: loss [3.0093563984603184]\n",
            "Model Trained at Step 3160: loss [3.1777654767917505]\n",
            "Model Trained at Step 3180: loss [2.9845610007003187]\n",
            "Model Trained at Step 3200: loss [3.0028605540798097]\n",
            "Model Trained at Step 3220: loss [3.0271344911738227]\n",
            "Model Trained at Step 3240: loss [2.9858181386170637]\n",
            "Model Trained at Step 3260: loss [2.857922794997422]\n",
            "Model Trained at Step 3280: loss [2.828882084855976]\n",
            "Model Trained at Step 3300: loss [3.004356312774449]\n",
            "Model Trained at Step 3320: loss [2.8267209187915183]\n",
            "Model Trained at Step 3340: loss [2.825524792896507]\n",
            "Model Trained at Step 3360: loss [3.0399528963086104]\n",
            "Model Trained at Step 3380: loss [2.8796332159852724]\n",
            "Model Trained at Step 3400: loss [2.8496413950959414]\n",
            "Model Trained at Step 3420: loss [2.7940901789946886]\n",
            "Model Trained at Step 3440: loss [2.9380918537101617]\n",
            "Model Trained at Step 3460: loss [3.1629761543909076]\n",
            "Model Trained at Step 3480: loss [2.9544602634326105]\n",
            "Model Trained at Step 3500: loss [2.8091381301025016]\n",
            "Model Trained at Step 3520: loss [2.853680420858553]\n",
            "Model Trained at Step 3540: loss [2.725355955498472]\n",
            "Model Trained at Step 3560: loss [2.81586955508141]\n",
            "Model Trained at Step 3580: loss [2.6995358802765517]\n",
            "Model Trained at Step 3600: loss [2.801061123306123]\n",
            "Model Trained at Step 3620: loss [2.7843863252873233]\n",
            "Model Trained at Step 3640: loss [2.6202657267708744]\n",
            "Model Trained at Step 3660: loss [2.600061800777211]\n",
            "Model Trained at Step 3680: loss [2.7757179750423298]\n",
            "Model Trained at Step 3700: loss [2.6309458021384216]\n",
            "Model Trained at Step 3720: loss [2.625993848935574]\n",
            "Model Trained at Step 3740: loss [2.648096330419391]\n",
            "Model Trained at Step 3760: loss [2.5214091426051]\n",
            "Model Trained at Step 3780: loss [2.5372830149869485]\n",
            "Model Trained at Step 3800: loss [2.4816298973518047]\n",
            "Model Trained at Step 3820: loss [2.3915883405517735]\n",
            "Model Trained at Step 3840: loss [2.6566818723315837]\n",
            "Model Trained at Step 3860: loss [2.5501523440637848]\n",
            "Model Trained at Step 3880: loss [2.3776773428779374]\n",
            "Model Trained at Step 3900: loss [2.4106841884290784]\n",
            "Model Trained at Step 3920: loss [2.37830692513927]\n",
            "Model Trained at Step 3940: loss [2.392800210030263]\n",
            "Model Trained at Step 3960: loss [2.3599059682246972]\n",
            "Model Trained at Step 3980: loss [2.384037511493497]\n",
            "Model Trained at Step 4000: loss [2.2743564819470365]\n",
            "Model Trained at Step 4020: loss [2.3164976384918736]\n",
            "Model Trained at Step 4040: loss [2.2740273077739355]\n",
            "Model Trained at Step 4060: loss [2.3121873673510844]\n",
            "Model Trained at Step 4080: loss [2.232820392828752]\n",
            "Model Trained at Step 4100: loss [2.168694409095402]\n",
            "Model Trained at Step 4120: loss [2.22155432485259]\n",
            "Model Trained at Step 4140: loss [2.224213112494736]\n",
            "Model Trained at Step 4160: loss [2.2290932627589917]\n",
            "Model Trained at Step 4180: loss [2.2362595794563527]\n",
            "Model Trained at Step 4200: loss [2.25936608561117]\n",
            "Model Trained at Step 4220: loss [2.2118709716677456]\n",
            "Model Trained at Step 4240: loss [2.215518555047754]\n",
            "Model Trained at Step 4260: loss [2.208877016080764]\n",
            "Model Trained at Step 4280: loss [2.16190609956864]\n",
            "Model Trained at Step 4300: loss [2.1704809059590233]\n",
            "Model Trained at Step 4320: loss [2.193619318735416]\n",
            "Model Trained at Step 4340: loss [2.1037944769289116]\n",
            "Model Trained at Step 4360: loss [2.2054292154899313]\n",
            "Model Trained at Step 4380: loss [2.176204199801802]\n",
            "Model Trained at Step 4400: loss [2.1403745169751835]\n",
            "Model Trained at Step 4420: loss [2.1624184044843355]\n",
            "Model Trained at Step 4440: loss [2.115151818394496]\n",
            "Model Trained at Step 4460: loss [2.1144347273058854]\n",
            "Model Trained at Step 4480: loss [2.1359860195226856]\n",
            "Model Trained at Step 4500: loss [2.154641324598576]\n",
            "Model Trained at Step 4520: loss [2.121121699824795]\n",
            "Model Trained at Step 4540: loss [2.0961981209110894]\n",
            "Model Trained at Step 4560: loss [2.041810410272864]\n",
            "Model Trained at Step 4580: loss [2.1231847517625857]\n",
            "Model Trained at Step 4600: loss [2.0249207813466823]\n",
            "Model Trained at Step 4620: loss [2.058405624697825]\n",
            "Model Trained at Step 4640: loss [2.138812971410353]\n",
            "Model Trained at Step 4660: loss [2.064093976895567]\n",
            "Model Trained at Step 4680: loss [2.055007174315108]\n",
            "Model Trained at Step 4700: loss [2.0524368057777336]\n",
            "Model Trained at Step 4720: loss [2.0603702527998253]\n",
            "Model Trained at Step 4740: loss [2.060431591791249]\n",
            "Model Trained at Step 4760: loss [2.0326344041270117]\n",
            "Model Trained at Step 4780: loss [2.0387454607692894]\n",
            "Model Trained at Step 4800: loss [2.0603791265817177]\n",
            "Model Trained at Step 4820: loss [2.0655379301212604]\n",
            "Model Trained at Step 4840: loss [2.0385022519700042]\n",
            "Model Trained at Step 4860: loss [2.0597931355611734]\n",
            "Model Trained at Step 4880: loss [2.0680807311256557]\n",
            "Model Trained at Step 4900: loss [2.0882530662627348]\n",
            "Model Trained at Step 4920: loss [2.0898147631102075]\n",
            "Model Trained at Step 4940: loss [2.096214112992658]\n",
            "Model Trained at Step 4960: loss [2.087173612106978]\n",
            "Model Trained at Step 4980: loss [2.0993758969203284]\n",
            "Model Trained at Step 5000: loss [2.083440935439767]\n",
            "Simulation ev training 2 done\n",
            "Model Trained at Step 20: loss [2.2614998032463247]\n",
            "Model Trained at Step 40: loss [1.9881829348110511]\n",
            "Model Trained at Step 60: loss [1.9752833707390445]\n",
            "Model Trained at Step 80: loss [1.9802500592910852]\n",
            "Model Trained at Step 100: loss [1.9711499954098062]\n",
            "Model Trained at Step 120: loss [1.9529761283208305]\n",
            "Model Trained at Step 140: loss [1.9463064185695835]\n",
            "Model Trained at Step 160: loss [1.9438689447761426]\n",
            "Model Trained at Step 180: loss [1.9337359594883705]\n",
            "Model Trained at Step 200: loss [1.9200746298514422]\n",
            "Model Trained at Step 220: loss [1.9088047053193296]\n",
            "Model Trained at Step 240: loss [1.9000340827276125]\n",
            "Model Trained at Step 260: loss [1.8890957738049834]\n",
            "Model Trained at Step 280: loss [1.8763926466592586]\n",
            "Model Trained at Step 300: loss [1.864732870838942]\n",
            "Model Trained at Step 320: loss [1.861040626312508]\n",
            "Model Trained at Step 340: loss [1.8502478210462923]\n",
            "Model Trained at Step 360: loss [1.834780001477744]\n",
            "Model Trained at Step 380: loss [1.8250002618681613]\n",
            "Model Trained at Step 400: loss [1.818761302311898]\n",
            "Model Trained at Step 420: loss [1.8080406082398617]\n",
            "Model Trained at Step 440: loss [1.8031404471852788]\n",
            "Model Trained at Step 460: loss [1.8042397984873293]\n",
            "Model Trained at Step 480: loss [1.8064262128219308]\n",
            "Model Trained at Step 500: loss [1.8041790560686337]\n",
            "Model Trained at Step 520: loss [1.8092666533697757]\n",
            "Model Trained at Step 540: loss [1.8163735501888507]\n",
            "Model Trained at Step 560: loss [1.826697803027694]\n",
            "Model Trained at Step 580: loss [1.8350353885690804]\n",
            "Model Trained at Step 600: loss [1.8415977053620076]\n",
            "Model Trained at Step 620: loss [1.8539217258151517]\n",
            "Model Trained at Step 640: loss [1.8677058053974236]\n",
            "Model Trained at Step 660: loss [1.8811399346700164]\n",
            "Model Trained at Step 680: loss [1.8952176979785513]\n",
            "Model Trained at Step 700: loss [1.9150546729090976]\n",
            "Model Trained at Step 720: loss [1.9339733976119533]\n",
            "Model Trained at Step 740: loss [1.9484114106462542]\n",
            "Model Trained at Step 760: loss [1.964275869410376]\n",
            "Model Trained at Step 780: loss [1.9797007231184967]\n",
            "Model Trained at Step 800: loss [1.9957691880638517]\n",
            "Model Trained at Step 820: loss [2.011032990712157]\n",
            "Model Trained at Step 840: loss [2.0265791851751778]\n",
            "Model Trained at Step 860: loss [2.046671098288049]\n",
            "Model Trained at Step 880: loss [2.0649334234477754]\n",
            "Model Trained at Step 900: loss [2.0818672649397425]\n",
            "Model Trained at Step 920: loss [2.0972686210798357]\n",
            "Model Trained at Step 940: loss [2.1147071873806706]\n",
            "Model Trained at Step 960: loss [2.133782437725178]\n",
            "Model Trained at Step 980: loss [2.152626038070414]\n",
            "Model Trained at Step 1000: loss [2.1714230260027727]\n",
            "Model Trained at Step 1020: loss [2.188359085093448]\n",
            "Model Trained at Step 1040: loss [2.20492312583493]\n",
            "Model Trained at Step 1060: loss [2.2192066022006545]\n",
            "Model Trained at Step 1080: loss [2.2328065996875788]\n",
            "Model Trained at Step 1100: loss [2.2447698369661055]\n",
            "Model Trained at Step 1120: loss [2.2550074181071795]\n",
            "Model Trained at Step 1140: loss [2.2645462547999786]\n",
            "Model Trained at Step 1160: loss [2.2701400883351655]\n",
            "Model Trained at Step 1180: loss [2.2716551923259902]\n",
            "Model Trained at Step 1200: loss [2.2718755017175254]\n",
            "Model Trained at Step 1220: loss [2.268854681358282]\n",
            "Model Trained at Step 1240: loss [2.266027764582214]\n",
            "Model Trained at Step 1260: loss [2.2586087509239454]\n",
            "Model Trained at Step 1280: loss [2.251822639532115]\n",
            "Model Trained at Step 1300: loss [2.244065214123711]\n",
            "Model Trained at Step 1320: loss [2.236438204113763]\n",
            "Model Trained at Step 1340: loss [2.22668776191251]\n",
            "Model Trained at Step 1360: loss [2.2162161744820907]\n",
            "Model Trained at Step 1380: loss [2.203974445620216]\n",
            "Model Trained at Step 1400: loss [2.190665807126668]\n",
            "Model Trained at Step 1420: loss [2.183117377298999]\n",
            "Model Trained at Step 1440: loss [2.175820926023298]\n",
            "Model Trained at Step 1460: loss [2.1704574339098595]\n",
            "Model Trained at Step 1480: loss [2.157268678265725]\n",
            "Model Trained at Step 1500: loss [2.1457120625297095]\n",
            "Model Trained at Step 1520: loss [2.13762795302564]\n",
            "Model Trained at Step 1540: loss [2.1331334642304696]\n",
            "Model Trained at Step 1560: loss [2.126952920949005]\n",
            "Model Trained at Step 1580: loss [2.1170797224326448]\n",
            "Model Trained at Step 1600: loss [2.1042579683358795]\n",
            "Model Trained at Step 1620: loss [2.094404362801701]\n",
            "Model Trained at Step 1640: loss [2.09033003226342]\n",
            "Model Trained at Step 1660: loss [2.0816434329571374]\n",
            "Model Trained at Step 1680: loss [2.0755031176138483]\n",
            "Model Trained at Step 1700: loss [2.07002008392821]\n",
            "Model Trained at Step 1720: loss [2.0627162356124518]\n",
            "Model Trained at Step 1740: loss [2.055982232820654]\n",
            "Model Trained at Step 1760: loss [2.0471699527174914]\n",
            "Model Trained at Step 1780: loss [2.038632682681777]\n",
            "Model Trained at Step 1800: loss [2.0340210714940428]\n",
            "Model Trained at Step 1820: loss [2.0275796728420823]\n",
            "Model Trained at Step 1840: loss [2.024264715976301]\n",
            "Model Trained at Step 1860: loss [2.023043044572842]\n",
            "Model Trained at Step 1880: loss [2.0208191295000293]\n",
            "Model Trained at Step 1900: loss [2.0162985769501485]\n",
            "Model Trained at Step 1920: loss [2.0130879041328416]\n",
            "Model Trained at Step 1940: loss [2.011254965684782]\n",
            "Model Trained at Step 1960: loss [2.0111388821530602]\n",
            "Model Trained at Step 1980: loss [2.0115101060488683]\n",
            "Model Trained at Step 2000: loss [2.0119247630289205]\n",
            "Model Trained at Step 2020: loss [2.0119216877428037]\n",
            "Model Trained at Step 2040: loss [2.01302734538602]\n",
            "Model Trained at Step 2060: loss [2.0171702354932126]\n",
            "Model Trained at Step 2080: loss [2.022563669005879]\n",
            "Model Trained at Step 2100: loss [2.03143775981754]\n",
            "Model Trained at Step 2120: loss [2.037515914294417]\n",
            "Model Trained at Step 2140: loss [2.0479407126804703]\n",
            "Model Trained at Step 2160: loss [2.0624984038225036]\n",
            "Model Trained at Step 2180: loss [2.077663361101025]\n",
            "Model Trained at Step 2200: loss [2.0953325452030396]\n",
            "Model Trained at Step 2220: loss [2.1067043946315493]\n",
            "Model Trained at Step 2240: loss [2.120574089101585]\n",
            "Model Trained at Step 2260: loss [2.1403491905571865]\n",
            "Model Trained at Step 2280: loss [2.162752758635394]\n",
            "Model Trained at Step 2300: loss [2.190760448328583]\n",
            "Model Trained at Step 2320: loss [2.2177388131210614]\n",
            "Model Trained at Step 2340: loss [2.2501462612521363]\n",
            "Model Trained at Step 2360: loss [2.281976932600982]\n",
            "Model Trained at Step 2380: loss [2.312081075522103]\n",
            "Model Trained at Step 2400: loss [2.3393363047755615]\n",
            "Model Trained at Step 2420: loss [2.362529174103615]\n",
            "Model Trained at Step 2440: loss [2.3844923633695148]\n",
            "Model Trained at Step 2460: loss [2.4068009772405587]\n",
            "Model Trained at Step 2480: loss [2.430540504386215]\n",
            "Model Trained at Step 2500: loss [2.454516540418499]\n",
            "Model Trained at Step 2520: loss [2.4763388298389866]\n",
            "Model Trained at Step 2540: loss [2.500913011924303]\n",
            "Model Trained at Step 2560: loss [2.5286508786693895]\n",
            "Model Trained at Step 2580: loss [2.555774717170073]\n",
            "Model Trained at Step 2600: loss [2.5836503653291327]\n",
            "Model Trained at Step 2620: loss [2.612548327623521]\n",
            "Model Trained at Step 2640: loss [2.636600267240587]\n",
            "Model Trained at Step 2660: loss [2.6598626603754796]\n",
            "Model Trained at Step 2680: loss [2.679753945700539]\n",
            "Model Trained at Step 2700: loss [2.7006622280302177]\n",
            "Model Trained at Step 2720: loss [2.718287916856301]\n",
            "Model Trained at Step 2740: loss [2.7354928838999344]\n",
            "Model Trained at Step 2760: loss [2.7550640027152027]\n",
            "Model Trained at Step 2780: loss [2.7687894813318157]\n",
            "Model Trained at Step 2800: loss [2.782041246484764]\n",
            "Model Trained at Step 2820: loss [2.798016414704726]\n",
            "Model Trained at Step 2840: loss [2.8098422630086985]\n",
            "Model Trained at Step 2860: loss [2.8185210017480777]\n",
            "Model Trained at Step 2880: loss [2.834035445688129]\n",
            "Model Trained at Step 2900: loss [2.8446294828233087]\n",
            "Model Trained at Step 2920: loss [2.859184390538653]\n",
            "Model Trained at Step 2940: loss [2.867718558247166]\n",
            "Model Trained at Step 2960: loss [2.881231499431324]\n",
            "Model Trained at Step 2980: loss [2.8800129259160556]\n",
            "Model Trained at Step 3000: loss [2.892078165425563]\n",
            "Model Trained at Step 3020: loss [2.9012541717310683]\n",
            "Model Trained at Step 3040: loss [2.913546299155602]\n",
            "Model Trained at Step 3060: loss [2.9206169167623486]\n",
            "Model Trained at Step 3080: loss [2.9264500352396836]\n",
            "Model Trained at Step 3100: loss [2.9193889996701174]\n",
            "Model Trained at Step 3120: loss [2.9117699701344657]\n",
            "Model Trained at Step 3140: loss [2.897400366193862]\n",
            "Model Trained at Step 3160: loss [2.8781639920942776]\n",
            "Model Trained at Step 3180: loss [2.8503422339888855]\n",
            "Model Trained at Step 3200: loss [2.8201507806166592]\n",
            "Model Trained at Step 3220: loss [2.7916684115299892]\n",
            "Model Trained at Step 3240: loss [2.77148231713585]\n",
            "Model Trained at Step 3260: loss [2.7512709056040867]\n",
            "Model Trained at Step 3280: loss [2.7326673773331036]\n",
            "Model Trained at Step 3300: loss [2.7149068328144397]\n",
            "Model Trained at Step 3320: loss [2.6970474052141253]\n",
            "Model Trained at Step 3340: loss [2.679472361647348]\n",
            "Model Trained at Step 3360: loss [2.665984781989296]\n",
            "Model Trained at Step 3380: loss [2.654892581741213]\n",
            "Model Trained at Step 3400: loss [2.6496968506276253]\n",
            "Model Trained at Step 3420: loss [2.646436240226109]\n",
            "Model Trained at Step 3440: loss [2.641762805854961]\n",
            "Model Trained at Step 3460: loss [2.640669429284563]\n",
            "Model Trained at Step 3480: loss [2.6471104974212283]\n",
            "Model Trained at Step 3500: loss [2.6544093205735306]\n",
            "Model Trained at Step 3520: loss [2.6561395225888917]\n",
            "Model Trained at Step 3540: loss [2.6598776557413757]\n",
            "Model Trained at Step 3560: loss [2.6611533137042085]\n",
            "Model Trained at Step 3580: loss [2.661532687156952]\n",
            "Model Trained at Step 3600: loss [2.6634162875415717]\n",
            "Model Trained at Step 3620: loss [2.6628751549865233]\n",
            "Model Trained at Step 3640: loss [2.6636131455023007]\n",
            "Model Trained at Step 3660: loss [2.661999455329841]\n",
            "Model Trained at Step 3680: loss [2.66167676142116]\n",
            "Model Trained at Step 3700: loss [2.6622698714822812]\n",
            "Model Trained at Step 3720: loss [2.6637388829991258]\n",
            "Model Trained at Step 3740: loss [2.6612828936805246]\n",
            "Model Trained at Step 3760: loss [2.661131438550157]\n",
            "Model Trained at Step 3780: loss [2.656099033152396]\n",
            "Model Trained at Step 3800: loss [2.6518639264871826]\n",
            "Model Trained at Step 3820: loss [2.6511841707029786]\n",
            "Model Trained at Step 3840: loss [2.6439661389973823]\n",
            "Model Trained at Step 3860: loss [2.6463061446504623]\n",
            "Model Trained at Step 3880: loss [2.6494439378139063]\n",
            "Model Trained at Step 3900: loss [2.65328614617481]\n",
            "Model Trained at Step 3920: loss [2.662152538705266]\n",
            "Model Trained at Step 3940: loss [2.672631650586296]\n",
            "Model Trained at Step 3960: loss [2.677574121212275]\n",
            "Model Trained at Step 3980: loss [2.6834498141456904]\n",
            "Model Trained at Step 4000: loss [2.6847251541368355]\n",
            "Model Trained at Step 4020: loss [2.687498686812945]\n",
            "Model Trained at Step 4040: loss [2.6909875972996558]\n",
            "Model Trained at Step 4060: loss [2.7036342590794002]\n",
            "Model Trained at Step 4080: loss [2.72089036434337]\n",
            "Model Trained at Step 4100: loss [2.7473138989541805]\n",
            "Model Trained at Step 4120: loss [2.7705265917891575]\n",
            "Model Trained at Step 4140: loss [2.797424943728635]\n",
            "Model Trained at Step 4160: loss [2.8228412566619907]\n",
            "Model Trained at Step 4180: loss [2.840011854541944]\n",
            "Model Trained at Step 4200: loss [2.849264366677725]\n",
            "Model Trained at Step 4220: loss [2.8619070193247724]\n",
            "Model Trained at Step 4240: loss [2.8667385132030185]\n",
            "Model Trained at Step 4260: loss [2.8782186432877586]\n",
            "Model Trained at Step 4280: loss [2.89016808338532]\n",
            "Model Trained at Step 4300: loss [2.9001574543381365]\n",
            "Model Trained at Step 4320: loss [2.9123380525779203]\n",
            "Model Trained at Step 4340: loss [2.9179606537661775]\n",
            "Model Trained at Step 4360: loss [2.9125823102746646]\n",
            "Model Trained at Step 4380: loss [2.9139673860443396]\n",
            "Model Trained at Step 4400: loss [2.9073713098452485]\n",
            "Model Trained at Step 4420: loss [2.894403911954623]\n",
            "Model Trained at Step 4440: loss [2.867594120900785]\n",
            "Model Trained at Step 4460: loss [2.8346803809571695]\n",
            "Model Trained at Step 4480: loss [2.7978980643146207]\n",
            "Model Trained at Step 4500: loss [2.7230442997325697]\n",
            "Model Trained at Step 4520: loss [2.604182483976803]\n",
            "Model Trained at Step 4540: loss [2.5239086471342045]\n",
            "Model Trained at Step 4560: loss [2.6046002533065042]\n",
            "Model Trained at Step 4580: loss [2.750290575465683]\n",
            "Model Trained at Step 4600: loss [2.677132883810569]\n",
            "Model Trained at Step 4620: loss [2.842169217563963]\n",
            "Model Trained at Step 4640: loss [2.951790880519236]\n",
            "Model Trained at Step 4660: loss [2.771598437325213]\n",
            "Model Trained at Step 4680: loss [2.983447714077541]\n",
            "Model Trained at Step 4700: loss [2.7366417381635224]\n",
            "Model Trained at Step 4720: loss [3.039004635939712]\n",
            "Model Trained at Step 4740: loss [2.7787534422149944]\n",
            "Model Trained at Step 4760: loss [2.9683339281668895]\n",
            "Model Trained at Step 4780: loss [2.8437793688363042]\n",
            "Model Trained at Step 4800: loss [2.5998647841077602]\n",
            "Model Trained at Step 4820: loss [2.982714861394242]\n",
            "Model Trained at Step 4840: loss [2.7932242879548754]\n",
            "Model Trained at Step 4860: loss [3.0962492410199873]\n",
            "Model Trained at Step 4880: loss [2.881170277299826]\n",
            "Model Trained at Step 4900: loss [2.827290392473485]\n",
            "Model Trained at Step 4920: loss [2.7717066115517714]\n",
            "Model Trained at Step 4940: loss [2.86230338617945]\n",
            "Model Trained at Step 4960: loss [2.8139522808434405]\n",
            "Model Trained at Step 4980: loss [2.858792099045515]\n",
            "Model Trained at Step 5000: loss [2.8623280264795783]\n",
            "Simulation ev training 3 done\n",
            "Model Trained at Step 20: loss [2.7940395403394964]\n",
            "Model Trained at Step 40: loss [2.6610446543707837]\n",
            "Model Trained at Step 60: loss [2.6548887243502053]\n",
            "Model Trained at Step 80: loss [2.6483666237748174]\n",
            "Model Trained at Step 100: loss [2.637677239830618]\n",
            "Model Trained at Step 120: loss [2.6252794529405783]\n",
            "Model Trained at Step 140: loss [2.615636125739411]\n",
            "Model Trained at Step 160: loss [2.587231629491457]\n",
            "Model Trained at Step 180: loss [2.5419121886204326]\n",
            "Model Trained at Step 200: loss [2.5245749551613472]\n",
            "Model Trained at Step 220: loss [2.497193728420438]\n",
            "Model Trained at Step 240: loss [2.4780867497414767]\n",
            "Model Trained at Step 260: loss [2.4637040443687805]\n",
            "Model Trained at Step 280: loss [2.446911758794152]\n",
            "Model Trained at Step 300: loss [2.4391560828179815]\n",
            "Model Trained at Step 320: loss [2.446720546648323]\n",
            "Model Trained at Step 340: loss [2.425483622164488]\n",
            "Model Trained at Step 360: loss [2.397837319892461]\n",
            "Model Trained at Step 380: loss [2.3703369994979737]\n",
            "Model Trained at Step 400: loss [2.331341577217063]\n",
            "Model Trained at Step 420: loss [2.2971346872253]\n",
            "Model Trained at Step 440: loss [2.2614245310399923]\n",
            "Model Trained at Step 460: loss [2.220690943576332]\n",
            "Model Trained at Step 480: loss [2.190995478118059]\n",
            "Model Trained at Step 500: loss [2.1715246990182466]\n",
            "Model Trained at Step 520: loss [2.1604239953788236]\n",
            "Model Trained at Step 540: loss [2.135514750648641]\n",
            "Model Trained at Step 560: loss [2.10874253496107]\n",
            "Model Trained at Step 580: loss [2.086593947584686]\n",
            "Model Trained at Step 600: loss [2.0641610435066937]\n",
            "Model Trained at Step 620: loss [2.0553226834085434]\n",
            "Model Trained at Step 640: loss [2.0455255443546303]\n",
            "Model Trained at Step 660: loss [2.045993446877323]\n",
            "Model Trained at Step 680: loss [2.046568815781781]\n",
            "Model Trained at Step 700: loss [2.041580492695086]\n",
            "Model Trained at Step 720: loss [2.0449335912610516]\n",
            "Model Trained at Step 740: loss [2.0445879122696793]\n",
            "Model Trained at Step 760: loss [2.0433389048907946]\n",
            "Model Trained at Step 780: loss [2.0430412394905577]\n",
            "Model Trained at Step 800: loss [2.0389014977086863]\n",
            "Model Trained at Step 820: loss [2.0306291361176925]\n",
            "Model Trained at Step 840: loss [2.0259603746969934]\n",
            "Model Trained at Step 860: loss [2.0154915138917717]\n",
            "Model Trained at Step 880: loss [2.005769813980172]\n",
            "Model Trained at Step 900: loss [1.9969064064085429]\n",
            "Model Trained at Step 920: loss [1.9917374446462186]\n",
            "Model Trained at Step 940: loss [1.9821241791863655]\n",
            "Model Trained at Step 960: loss [1.975399946271526]\n",
            "Model Trained at Step 980: loss [1.9654784986715363]\n",
            "Model Trained at Step 1000: loss [1.9605077128396893]\n",
            "Model Trained at Step 1020: loss [1.953548302750249]\n",
            "Model Trained at Step 1040: loss [1.948523791033751]\n",
            "Model Trained at Step 1060: loss [1.9423072333681717]\n",
            "Model Trained at Step 1080: loss [1.9426981808454804]\n",
            "Model Trained at Step 1100: loss [1.945332905814014]\n",
            "Model Trained at Step 1120: loss [1.944795485316419]\n",
            "Model Trained at Step 1140: loss [1.9497214806715966]\n",
            "Model Trained at Step 1160: loss [1.9562276089566866]\n",
            "Model Trained at Step 1180: loss [1.964683773383279]\n",
            "Model Trained at Step 1200: loss [1.974072454569462]\n",
            "Model Trained at Step 1220: loss [1.981943716873463]\n",
            "Model Trained at Step 1240: loss [1.993350367558253]\n",
            "Model Trained at Step 1260: loss [2.0036810818742525]\n",
            "Model Trained at Step 1280: loss [2.0143930190276422]\n",
            "Model Trained at Step 1300: loss [2.0248129986976897]\n",
            "Model Trained at Step 1320: loss [2.035217137805179]\n",
            "Model Trained at Step 1340: loss [2.045953614554553]\n",
            "Model Trained at Step 1360: loss [2.0544263761756545]\n",
            "Model Trained at Step 1380: loss [2.0625903693041368]\n",
            "Model Trained at Step 1400: loss [2.0722903506883283]\n",
            "Model Trained at Step 1420: loss [2.0808889545154265]\n",
            "Model Trained at Step 1440: loss [2.0903876583852323]\n",
            "Model Trained at Step 1460: loss [2.09951070478916]\n",
            "Model Trained at Step 1480: loss [2.108831229022849]\n",
            "Model Trained at Step 1500: loss [2.11876511385639]\n",
            "Model Trained at Step 1520: loss [2.126972609263229]\n",
            "Model Trained at Step 1540: loss [2.1411575720556817]\n",
            "Model Trained at Step 1560: loss [2.1534602695629865]\n",
            "Model Trained at Step 1580: loss [2.169047490889072]\n",
            "Model Trained at Step 1600: loss [2.182672328323229]\n",
            "Model Trained at Step 1620: loss [2.1940813868667046]\n",
            "Model Trained at Step 1640: loss [2.2064204721509135]\n",
            "Model Trained at Step 1660: loss [2.2188309651134785]\n",
            "Model Trained at Step 1680: loss [2.2311254354532406]\n",
            "Model Trained at Step 1700: loss [2.2448364380758763]\n",
            "Model Trained at Step 1720: loss [2.261416704711542]\n",
            "Model Trained at Step 1740: loss [2.2771100332889738]\n",
            "Model Trained at Step 1760: loss [2.2919799284507953]\n",
            "Model Trained at Step 1780: loss [2.3098536737181576]\n",
            "Model Trained at Step 1800: loss [2.3298698786777132]\n",
            "Model Trained at Step 1820: loss [2.347587381761165]\n",
            "Model Trained at Step 1840: loss [2.3695672426753682]\n",
            "Model Trained at Step 1860: loss [2.3866473629365794]\n",
            "Model Trained at Step 1880: loss [2.398309242917062]\n",
            "Model Trained at Step 1900: loss [2.4105712363264535]\n",
            "Model Trained at Step 1920: loss [2.419514746104763]\n",
            "Model Trained at Step 1940: loss [2.429342449234702]\n",
            "Model Trained at Step 1960: loss [2.435505105880415]\n",
            "Model Trained at Step 1980: loss [2.440263264108963]\n",
            "Model Trained at Step 2000: loss [2.445141301491957]\n",
            "Model Trained at Step 2020: loss [2.446267721392728]\n",
            "Model Trained at Step 2040: loss [2.445782809413518]\n",
            "Model Trained at Step 2060: loss [2.445750505353015]\n",
            "Model Trained at Step 2080: loss [2.447534696938581]\n",
            "Model Trained at Step 2100: loss [2.4473349616891493]\n",
            "Model Trained at Step 2120: loss [2.445549621722993]\n",
            "Model Trained at Step 2140: loss [2.4416414732400513]\n",
            "Model Trained at Step 2160: loss [2.437395625649787]\n",
            "Model Trained at Step 2180: loss [2.431519061292603]\n",
            "Model Trained at Step 2200: loss [2.4295515177802676]\n",
            "Model Trained at Step 2220: loss [2.429385814582595]\n",
            "Model Trained at Step 2240: loss [2.4279663572661354]\n",
            "Model Trained at Step 2260: loss [2.4281562899609033]\n",
            "Model Trained at Step 2280: loss [2.4274385810679817]\n",
            "Model Trained at Step 2300: loss [2.432233307436975]\n",
            "Model Trained at Step 2320: loss [2.4382588527140117]\n",
            "Model Trained at Step 2340: loss [2.4451653888917217]\n",
            "Model Trained at Step 2360: loss [2.45272860973193]\n",
            "Model Trained at Step 2380: loss [2.460296714630311]\n",
            "Model Trained at Step 2400: loss [2.4703560440870076]\n",
            "Model Trained at Step 2420: loss [2.480123285689063]\n",
            "Model Trained at Step 2440: loss [2.491239839016165]\n",
            "Model Trained at Step 2460: loss [2.5041424644847927]\n",
            "Model Trained at Step 2480: loss [2.51732991337455]\n",
            "Model Trained at Step 2500: loss [2.5294189577382498]\n",
            "Model Trained at Step 2520: loss [2.547783606934913]\n",
            "Model Trained at Step 2540: loss [2.5576598567642774]\n",
            "Model Trained at Step 2560: loss [2.563137419743131]\n",
            "Model Trained at Step 2580: loss [2.560946073242791]\n",
            "Model Trained at Step 2600: loss [2.549941612580695]\n",
            "Model Trained at Step 2620: loss [2.5437347255371625]\n",
            "Model Trained at Step 2640: loss [2.5399747816632496]\n",
            "Model Trained at Step 2660: loss [2.5368364510839347]\n",
            "Model Trained at Step 2680: loss [2.538088023338616]\n",
            "Model Trained at Step 2700: loss [2.5409655791862455]\n",
            "Model Trained at Step 2720: loss [2.537292653024882]\n",
            "Model Trained at Step 2740: loss [2.5177250379322302]\n",
            "Model Trained at Step 2760: loss [2.488861335151634]\n",
            "Model Trained at Step 2780: loss [2.456770120479832]\n",
            "Model Trained at Step 2800: loss [2.483759450032384]\n",
            "Model Trained at Step 2820: loss [2.465271843859515]\n",
            "Model Trained at Step 2840: loss [2.681252943402368]\n",
            "Model Trained at Step 2860: loss [2.758650358069869]\n",
            "Model Trained at Step 2880: loss [2.7486542735510477]\n",
            "Model Trained at Step 2900: loss [2.7345888693289586]\n",
            "Model Trained at Step 2920: loss [2.718035814827968]\n",
            "Model Trained at Step 2940: loss [2.6961917568385414]\n",
            "Model Trained at Step 2960: loss [2.664431947410963]\n",
            "Model Trained at Step 2980: loss [2.624149904177604]\n",
            "Model Trained at Step 3000: loss [2.5724718398969393]\n",
            "Model Trained at Step 3020: loss [2.490042001645974]\n",
            "Model Trained at Step 3040: loss [2.32606026105175]\n",
            "Model Trained at Step 3060: loss [2.233223004518152]\n",
            "Model Trained at Step 3080: loss [2.2899894170092554]\n",
            "Model Trained at Step 3100: loss [2.3169296936826727]\n",
            "Model Trained at Step 3120: loss [2.3487375197720866]\n",
            "Model Trained at Step 3140: loss [2.494564149020031]\n",
            "Model Trained at Step 3160: loss [2.4380469769273434]\n",
            "Model Trained at Step 3180: loss [2.333402528043457]\n",
            "Model Trained at Step 3200: loss [2.232931573197142]\n",
            "Model Trained at Step 3220: loss [2.3736617082978957]\n",
            "Model Trained at Step 3240: loss [2.308355762041427]\n",
            "Model Trained at Step 3260: loss [2.3184186543846548]\n",
            "Model Trained at Step 3280: loss [2.368638237615503]\n",
            "Model Trained at Step 3300: loss [2.267909969379158]\n",
            "Model Trained at Step 3320: loss [2.3820678959117894]\n",
            "Model Trained at Step 3340: loss [2.417147543038624]\n",
            "Model Trained at Step 3360: loss [2.3634787079297537]\n",
            "Model Trained at Step 3380: loss [2.2532995123591473]\n",
            "Model Trained at Step 3400: loss [2.4615537482326606]\n",
            "Model Trained at Step 3420: loss [2.531279363595616]\n",
            "Model Trained at Step 3440: loss [2.51642037979392]\n",
            "Model Trained at Step 3460: loss [2.4944457271283724]\n",
            "Model Trained at Step 3480: loss [2.470060512458942]\n",
            "Model Trained at Step 3500: loss [2.44417804452039]\n",
            "Model Trained at Step 3520: loss [2.3950654989134987]\n",
            "Model Trained at Step 3540: loss [2.3301455729158747]\n",
            "Model Trained at Step 3560: loss [2.307905788262068]\n",
            "Model Trained at Step 3580: loss [2.5914604574189335]\n",
            "Model Trained at Step 3600: loss [2.6170837514506706]\n",
            "Model Trained at Step 3620: loss [2.6076031051708735]\n",
            "Model Trained at Step 3640: loss [2.5947225096170063]\n",
            "Model Trained at Step 3660: loss [2.580607871137063]\n",
            "Model Trained at Step 3680: loss [2.554236700395582]\n",
            "Model Trained at Step 3700: loss [2.5114868532716406]\n",
            "Model Trained at Step 3720: loss [2.429956888623212]\n",
            "Model Trained at Step 3740: loss [2.316434732041672]\n",
            "Model Trained at Step 3760: loss [2.649338306881674]\n",
            "Model Trained at Step 3780: loss [2.9081153804742526]\n",
            "Model Trained at Step 3800: loss [2.927864236204095]\n",
            "Model Trained at Step 3820: loss [2.9367771152157474]\n",
            "Model Trained at Step 3840: loss [2.9355956063861894]\n",
            "Model Trained at Step 3860: loss [2.9376686397726]\n",
            "Model Trained at Step 3880: loss [2.9322275393956447]\n",
            "Model Trained at Step 3900: loss [2.933089222295645]\n",
            "Model Trained at Step 3920: loss [2.9392861888104687]\n",
            "Model Trained at Step 3940: loss [2.932243471777033]\n",
            "Model Trained at Step 3960: loss [2.931061902038207]\n",
            "Model Trained at Step 3980: loss [2.9223125089523414]\n",
            "Model Trained at Step 4000: loss [2.9070844923828805]\n",
            "Model Trained at Step 4020: loss [2.887243328037668]\n",
            "Model Trained at Step 4040: loss [2.8556678212797957]\n",
            "Model Trained at Step 4060: loss [2.802911683793673]\n",
            "Model Trained at Step 4080: loss [2.725891247503458]\n",
            "Model Trained at Step 4100: loss [2.6101062947441154]\n",
            "Model Trained at Step 4120: loss [2.652065672787602]\n",
            "Model Trained at Step 4140: loss [3.1003281527506608]\n",
            "Model Trained at Step 4160: loss [3.133348577412227]\n",
            "Model Trained at Step 4180: loss [3.113769176947053]\n",
            "Model Trained at Step 4200: loss [3.0877577598156494]\n",
            "Model Trained at Step 4220: loss [3.0701742863397317]\n",
            "Model Trained at Step 4240: loss [3.0296061356184785]\n",
            "Model Trained at Step 4260: loss [2.9686994632272548]\n",
            "Model Trained at Step 4280: loss [2.88918755003034]\n",
            "Model Trained at Step 4300: loss [2.7876239196993713]\n",
            "Model Trained at Step 4320: loss [2.7839976485593865]\n",
            "Model Trained at Step 4340: loss [2.9475448793177996]\n",
            "Model Trained at Step 4360: loss [2.8813138000176606]\n",
            "Model Trained at Step 4380: loss [2.946203332848847]\n",
            "Model Trained at Step 4400: loss [3.0900802348977106]\n",
            "Model Trained at Step 4420: loss [3.035179377494792]\n",
            "Model Trained at Step 4440: loss [2.9753834427294965]\n",
            "Model Trained at Step 4460: loss [2.8650996980410794]\n",
            "Model Trained at Step 4480: loss [2.8199489546389884]\n",
            "Model Trained at Step 4500: loss [3.030266853592974]\n",
            "Model Trained at Step 4520: loss [3.113057356115964]\n",
            "Model Trained at Step 4540: loss [3.0875895192247618]\n",
            "Model Trained at Step 4560: loss [3.048978150270125]\n",
            "Model Trained at Step 4580: loss [2.9944799993149522]\n",
            "Model Trained at Step 4600: loss [2.8976058560617766]\n",
            "Model Trained at Step 4620: loss [2.894653243250798]\n",
            "Model Trained at Step 4640: loss [3.1055282274720435]\n",
            "Model Trained at Step 4660: loss [3.1087143539732427]\n",
            "Model Trained at Step 4680: loss [3.071852930050592]\n",
            "Model Trained at Step 4700: loss [3.0144963430473184]\n",
            "Model Trained at Step 4720: loss [2.923541619157393]\n",
            "Model Trained at Step 4740: loss [2.9926869387714286]\n",
            "Model Trained at Step 4760: loss [3.140151308517646]\n",
            "Model Trained at Step 4780: loss [3.1252457523136266]\n",
            "Model Trained at Step 4800: loss [3.0859576447989383]\n",
            "Model Trained at Step 4820: loss [3.0242156026069367]\n",
            "Model Trained at Step 4840: loss [2.88845345619454]\n",
            "Model Trained at Step 4860: loss [3.1224708383723514]\n",
            "Model Trained at Step 4880: loss [3.2415757152757676]\n",
            "Model Trained at Step 4900: loss [3.240552039618212]\n",
            "Model Trained at Step 4920: loss [3.2250350192532955]\n",
            "Model Trained at Step 4940: loss [3.205164852287357]\n",
            "Model Trained at Step 4960: loss [3.1784055384406384]\n",
            "Model Trained at Step 4980: loss [3.154075972119808]\n",
            "Model Trained at Step 5000: loss [3.129922060681485]\n",
            "Simulation ev training 4 done\n",
            "Model Trained at Step 20: loss [2.3365405097158267]\n",
            "Model Trained at Step 40: loss [2.134752658436897]\n",
            "Model Trained at Step 60: loss [2.092669061827703]\n",
            "Model Trained at Step 80: loss [2.0300851631850874]\n",
            "Model Trained at Step 100: loss [2.0041487704696292]\n",
            "Model Trained at Step 120: loss [1.986438187251204]\n",
            "Model Trained at Step 140: loss [1.987307168431482]\n",
            "Model Trained at Step 160: loss [1.9633274634069706]\n",
            "Model Trained at Step 180: loss [1.9495487745616906]\n",
            "Model Trained at Step 200: loss [1.9265886423848326]\n",
            "Model Trained at Step 220: loss [1.9029365206058677]\n",
            "Model Trained at Step 240: loss [1.8905490279986314]\n",
            "Model Trained at Step 260: loss [1.8673553215044705]\n",
            "Model Trained at Step 280: loss [1.8460960577166687]\n",
            "Model Trained at Step 300: loss [1.8382431710915117]\n",
            "Model Trained at Step 320: loss [1.8420951552748086]\n",
            "Model Trained at Step 340: loss [1.8422237781753812]\n",
            "Model Trained at Step 360: loss [1.8502551590642526]\n",
            "Model Trained at Step 380: loss [1.8661918577041017]\n",
            "Model Trained at Step 400: loss [1.8867677267983705]\n",
            "Model Trained at Step 420: loss [1.8893262316456876]\n",
            "Model Trained at Step 440: loss [1.8816812010515052]\n",
            "Model Trained at Step 460: loss [1.8692648006263706]\n",
            "Model Trained at Step 480: loss [1.850132785055155]\n",
            "Model Trained at Step 500: loss [1.843338359075697]\n",
            "Model Trained at Step 520: loss [1.8524485179620414]\n",
            "Model Trained at Step 540: loss [1.8634588134920975]\n",
            "Model Trained at Step 560: loss [1.8701262485110686]\n",
            "Model Trained at Step 580: loss [1.8728856267417568]\n",
            "Model Trained at Step 600: loss [1.8774678238790543]\n",
            "Model Trained at Step 620: loss [1.8912909910579447]\n",
            "Model Trained at Step 640: loss [1.9019206941438445]\n",
            "Model Trained at Step 660: loss [1.9193490866906788]\n",
            "Model Trained at Step 680: loss [1.9433994129724765]\n",
            "Model Trained at Step 700: loss [1.9742255059097598]\n",
            "Model Trained at Step 720: loss [2.008469769540291]\n",
            "Model Trained at Step 740: loss [2.0475848968219608]\n",
            "Model Trained at Step 760: loss [2.0839289730940416]\n",
            "Model Trained at Step 780: loss [2.123110538920875]\n",
            "Model Trained at Step 800: loss [2.167026591304347]\n",
            "Model Trained at Step 820: loss [2.2158493804015973]\n",
            "Model Trained at Step 840: loss [2.2658734405726038]\n",
            "Model Trained at Step 860: loss [2.3192021908442664]\n",
            "Model Trained at Step 880: loss [2.356068628795908]\n",
            "Model Trained at Step 900: loss [2.3904273217450474]\n",
            "Model Trained at Step 920: loss [2.4219030555240146]\n",
            "Model Trained at Step 940: loss [2.4305359057211264]\n",
            "Model Trained at Step 960: loss [2.455581454705335]\n",
            "Model Trained at Step 980: loss [2.4505158135569864]\n",
            "Model Trained at Step 1000: loss [2.4753786981298074]\n",
            "Model Trained at Step 1020: loss [2.466112643508962]\n",
            "Model Trained at Step 1040: loss [2.4701895248208983]\n",
            "Model Trained at Step 1060: loss [2.438866864770444]\n",
            "Model Trained at Step 1080: loss [2.4397817209672223]\n",
            "Model Trained at Step 1100: loss [2.476134157149228]\n",
            "Model Trained at Step 1120: loss [2.457857178066247]\n",
            "Model Trained at Step 1140: loss [2.434925099469327]\n",
            "Model Trained at Step 1160: loss [2.4024842525673002]\n",
            "Model Trained at Step 1180: loss [2.3762048520527115]\n",
            "Model Trained at Step 1200: loss [2.3399079450664386]\n",
            "Model Trained at Step 1220: loss [2.302198127210704]\n",
            "Model Trained at Step 1240: loss [2.2770125276305975]\n",
            "Model Trained at Step 1260: loss [2.2621228722180446]\n",
            "Model Trained at Step 1280: loss [2.2369513311002107]\n",
            "Model Trained at Step 1300: loss [2.237991777506189]\n",
            "Model Trained at Step 1320: loss [2.2406586563877537]\n",
            "Model Trained at Step 1340: loss [2.2514877765890042]\n",
            "Model Trained at Step 1360: loss [2.251499318610105]\n",
            "Model Trained at Step 1380: loss [2.2479338883807287]\n",
            "Model Trained at Step 1400: loss [2.2482868191619003]\n",
            "Model Trained at Step 1420: loss [2.2539453231376823]\n",
            "Model Trained at Step 1440: loss [2.2601150029049712]\n",
            "Model Trained at Step 1460: loss [2.264905263536523]\n",
            "Model Trained at Step 1480: loss [2.2717898437743136]\n",
            "Model Trained at Step 1500: loss [2.276760153832972]\n",
            "Model Trained at Step 1520: loss [2.2823556915448577]\n",
            "Model Trained at Step 1540: loss [2.284743147077809]\n",
            "Model Trained at Step 1560: loss [2.2853886866742616]\n",
            "Model Trained at Step 1580: loss [2.293269600629164]\n",
            "Model Trained at Step 1600: loss [2.3054709813737952]\n",
            "Model Trained at Step 1620: loss [2.3153574151118894]\n",
            "Model Trained at Step 1640: loss [2.3223475892711876]\n",
            "Model Trained at Step 1660: loss [2.3270541293071525]\n",
            "Model Trained at Step 1680: loss [2.3280256682992304]\n",
            "Model Trained at Step 1700: loss [2.3236050979793013]\n",
            "Model Trained at Step 1720: loss [2.315808611451793]\n",
            "Model Trained at Step 1740: loss [2.3052295692324494]\n",
            "Model Trained at Step 1760: loss [2.297808952356335]\n",
            "Model Trained at Step 1780: loss [2.293078138848595]\n",
            "Model Trained at Step 1800: loss [2.297278378768066]\n",
            "Model Trained at Step 1820: loss [2.3113672834072663]\n",
            "Model Trained at Step 1840: loss [2.327315134878428]\n",
            "Model Trained at Step 1860: loss [2.3495685741343166]\n",
            "Model Trained at Step 1880: loss [2.3772395674168667]\n",
            "Model Trained at Step 1900: loss [2.4104223488633347]\n",
            "Model Trained at Step 1920: loss [2.4365837335934866]\n",
            "Model Trained at Step 1940: loss [2.4715848626965586]\n",
            "Model Trained at Step 1960: loss [2.507077440295352]\n",
            "Model Trained at Step 1980: loss [2.538269888997561]\n",
            "Model Trained at Step 2000: loss [2.5650797008316544]\n",
            "Model Trained at Step 2020: loss [2.593724322228751]\n",
            "Model Trained at Step 2040: loss [2.611457580270431]\n",
            "Model Trained at Step 2060: loss [2.6256203149354165]\n",
            "Model Trained at Step 2080: loss [2.64923176763937]\n",
            "Model Trained at Step 2100: loss [2.6704446332250704]\n",
            "Model Trained at Step 2120: loss [2.6945713420335125]\n",
            "Model Trained at Step 2140: loss [2.718185017466427]\n",
            "Model Trained at Step 2160: loss [2.726937197069201]\n",
            "Model Trained at Step 2180: loss [2.7344036925155066]\n",
            "Model Trained at Step 2200: loss [2.746169764931745]\n",
            "Model Trained at Step 2220: loss [2.7532705637950894]\n",
            "Model Trained at Step 2240: loss [2.7677540067355055]\n",
            "Model Trained at Step 2260: loss [2.76369469265405]\n",
            "Model Trained at Step 2280: loss [2.7682337303162523]\n",
            "Model Trained at Step 2300: loss [2.7844430857265934]\n",
            "Model Trained at Step 2320: loss [2.8005218670744205]\n",
            "Model Trained at Step 2340: loss [2.800208186202777]\n",
            "Model Trained at Step 2360: loss [2.809987941170873]\n",
            "Model Trained at Step 2380: loss [2.8014513327358856]\n",
            "Model Trained at Step 2400: loss [2.783585250421957]\n",
            "Model Trained at Step 2420: loss [2.7638668063410186]\n",
            "Model Trained at Step 2440: loss [2.7408870724421255]\n",
            "Model Trained at Step 2460: loss [2.713754844882635]\n",
            "Model Trained at Step 2480: loss [2.6833973486189295]\n",
            "Model Trained at Step 2500: loss [2.6488507577503717]\n",
            "Model Trained at Step 2520: loss [2.6079367378296103]\n",
            "Model Trained at Step 2540: loss [2.5632946275853405]\n",
            "Model Trained at Step 2560: loss [2.5066614009682118]\n",
            "Model Trained at Step 2580: loss [2.445969039990456]\n",
            "Model Trained at Step 2600: loss [2.3779233914804214]\n",
            "Model Trained at Step 2620: loss [2.289508361106521]\n",
            "Model Trained at Step 2640: loss [2.3194049858481165]\n",
            "Model Trained at Step 2660: loss [2.466073546325603]\n",
            "Model Trained at Step 2680: loss [2.3077834835503714]\n",
            "Model Trained at Step 2700: loss [2.431263435079326]\n",
            "Model Trained at Step 2720: loss [2.3327135366903606]\n",
            "Model Trained at Step 2740: loss [2.4163906083965165]\n",
            "Model Trained at Step 2760: loss [2.2336172804600194]\n",
            "Model Trained at Step 2780: loss [2.422038197037297]\n",
            "Model Trained at Step 2800: loss [2.5958401920154204]\n",
            "Model Trained at Step 2820: loss [2.585437988278212]\n",
            "Model Trained at Step 2840: loss [2.556710837552274]\n",
            "Model Trained at Step 2860: loss [2.508310910446919]\n",
            "Model Trained at Step 2880: loss [2.423980970166291]\n",
            "Model Trained at Step 2900: loss [2.2053644106520114]\n",
            "Model Trained at Step 2920: loss [2.187360428788998]\n",
            "Model Trained at Step 2940: loss [2.2452788820243037]\n",
            "Model Trained at Step 2960: loss [2.3530195257800957]\n",
            "Model Trained at Step 2980: loss [2.231969018653979]\n",
            "Model Trained at Step 3000: loss [2.128878419377614]\n",
            "Model Trained at Step 3020: loss [2.161941045445864]\n",
            "Model Trained at Step 3040: loss [2.2289610323701874]\n",
            "Model Trained at Step 3060: loss [2.144536817581473]\n",
            "Model Trained at Step 3080: loss [2.132505185418663]\n",
            "Model Trained at Step 3100: loss [2.1244776577757145]\n",
            "Model Trained at Step 3120: loss [2.070006649479821]\n",
            "Model Trained at Step 3140: loss [2.1553535667744095]\n",
            "Model Trained at Step 3160: loss [2.3910370222946775]\n",
            "Model Trained at Step 3180: loss [2.2695937990228323]\n",
            "Model Trained at Step 3200: loss [2.048324312403175]\n",
            "Model Trained at Step 3220: loss [2.122271067158633]\n",
            "Model Trained at Step 3240: loss [2.0248155129161742]\n",
            "Model Trained at Step 3260: loss [2.09214005182809]\n",
            "Model Trained at Step 3280: loss [2.049376626589396]\n",
            "Model Trained at Step 3300: loss [2.0896960232697572]\n",
            "Model Trained at Step 3320: loss [2.067409567533204]\n",
            "Model Trained at Step 3340: loss [2.0317044331564]\n",
            "Model Trained at Step 3360: loss [2.118984836998826]\n",
            "Model Trained at Step 3380: loss [2.077038417396854]\n",
            "Model Trained at Step 3400: loss [2.0278710064461567]\n",
            "Model Trained at Step 3420: loss [2.0062816750505723]\n",
            "Model Trained at Step 3440: loss [2.063778102512718]\n",
            "Model Trained at Step 3460: loss [2.0249477526309168]\n",
            "Model Trained at Step 3480: loss [2.0299782506066046]\n",
            "Model Trained at Step 3500: loss [2.056116629013659]\n",
            "Model Trained at Step 3520: loss [2.0252202456993276]\n",
            "Model Trained at Step 3540: loss [2.0695400256630805]\n",
            "Model Trained at Step 3560: loss [2.0367201105165993]\n",
            "Model Trained at Step 3580: loss [2.0605002041541924]\n",
            "Model Trained at Step 3600: loss [2.07848430326827]\n",
            "Model Trained at Step 3620: loss [2.0317923592605376]\n",
            "Model Trained at Step 3640: loss [2.110962405733718]\n",
            "Model Trained at Step 3660: loss [2.051077701353457]\n",
            "Model Trained at Step 3680: loss [2.0530882637623113]\n",
            "Model Trained at Step 3700: loss [2.0534418460819657]\n",
            "Model Trained at Step 3720: loss [2.1165535142299126]\n",
            "Model Trained at Step 3740: loss [2.046047279193117]\n",
            "Model Trained at Step 3760: loss [2.0783535101970076]\n",
            "Model Trained at Step 3780: loss [2.080150363353728]\n",
            "Model Trained at Step 3800: loss [2.0313456753279597]\n",
            "Model Trained at Step 3820: loss [2.1063717290599513]\n",
            "Model Trained at Step 3840: loss [1.9779657651903022]\n",
            "Model Trained at Step 3860: loss [2.0401072719946263]\n",
            "Model Trained at Step 3880: loss [2.0692819974207155]\n",
            "Model Trained at Step 3900: loss [2.0527529108702818]\n",
            "Model Trained at Step 3920: loss [2.1174074649216963]\n",
            "Model Trained at Step 3940: loss [2.081397326466004]\n",
            "Model Trained at Step 3960: loss [2.076630472565886]\n",
            "Model Trained at Step 3980: loss [2.1085841538377754]\n",
            "Model Trained at Step 4000: loss [2.095037366253662]\n",
            "Model Trained at Step 4020: loss [2.113796625209732]\n",
            "Model Trained at Step 4040: loss [2.1628384697303047]\n",
            "Model Trained at Step 4060: loss [2.108626294496111]\n",
            "Model Trained at Step 4080: loss [2.0960059935612128]\n",
            "Model Trained at Step 4100: loss [2.10671770790461]\n",
            "Model Trained at Step 4120: loss [2.0923223822736587]\n",
            "Model Trained at Step 4140: loss [2.173531733358229]\n",
            "Model Trained at Step 4160: loss [2.157560116057071]\n",
            "Model Trained at Step 4180: loss [2.1164313620676376]\n",
            "Model Trained at Step 4200: loss [2.0906927967270486]\n",
            "Model Trained at Step 4220: loss [2.13468169499584]\n",
            "Model Trained at Step 4240: loss [2.1098840178991765]\n",
            "Model Trained at Step 4260: loss [2.1198898170011105]\n",
            "Model Trained at Step 4280: loss [2.082351996508056]\n",
            "Model Trained at Step 4300: loss [2.0966312752137157]\n",
            "Model Trained at Step 4320: loss [2.125020218083193]\n",
            "Model Trained at Step 4340: loss [2.088717068289532]\n",
            "Model Trained at Step 4360: loss [2.1103293099657354]\n",
            "Model Trained at Step 4380: loss [2.124305331750964]\n",
            "Model Trained at Step 4400: loss [2.0915069483338193]\n",
            "Model Trained at Step 4420: loss [2.191119723357045]\n",
            "Model Trained at Step 4440: loss [2.196266038803379]\n",
            "Model Trained at Step 4460: loss [2.205753141487947]\n",
            "Model Trained at Step 4480: loss [2.2394701056855055]\n",
            "Model Trained at Step 4500: loss [2.217788586420057]\n",
            "Model Trained at Step 4520: loss [2.209004328567149]\n",
            "Model Trained at Step 4540: loss [2.2396220858064737]\n",
            "Model Trained at Step 4560: loss [2.194434458715663]\n",
            "Model Trained at Step 4580: loss [2.165937494067182]\n",
            "Model Trained at Step 4600: loss [2.0138939662426663]\n",
            "Model Trained at Step 4620: loss [2.2577749807591383]\n",
            "Model Trained at Step 4640: loss [2.1581577936941927]\n",
            "Model Trained at Step 4660: loss [2.214316282496635]\n",
            "Model Trained at Step 4680: loss [2.097745794829251]\n",
            "Model Trained at Step 4700: loss [2.2298956416308906]\n",
            "Model Trained at Step 4720: loss [2.1621014326182255]\n",
            "Model Trained at Step 4740: loss [2.1603359299095506]\n",
            "Model Trained at Step 4760: loss [2.1530184773674366]\n",
            "Model Trained at Step 4780: loss [2.113437362378213]\n",
            "Model Trained at Step 4800: loss [2.1137578351316066]\n",
            "Model Trained at Step 4820: loss [2.052377207558561]\n",
            "Model Trained at Step 4840: loss [2.0731786815738147]\n",
            "Model Trained at Step 4860: loss [2.082459697730008]\n",
            "Model Trained at Step 4880: loss [2.020410087853822]\n",
            "Model Trained at Step 4900: loss [2.0872154684160504]\n",
            "Model Trained at Step 4920: loss [2.073428270351983]\n",
            "Model Trained at Step 4940: loss [2.0775075924502993]\n",
            "Model Trained at Step 4960: loss [2.056670511314591]\n",
            "Model Trained at Step 4980: loss [2.069309198007736]\n",
            "Model Trained at Step 5000: loss [2.045749617390782]\n",
            "Simulation ev training 5 done\n",
            "Model Trained at Step 20: loss [2.304373945121418]\n",
            "Model Trained at Step 40: loss [2.155594630936987]\n",
            "Model Trained at Step 60: loss [2.173822569583411]\n",
            "Model Trained at Step 80: loss [2.1725852381661612]\n",
            "Model Trained at Step 100: loss [2.1511021232890815]\n",
            "Model Trained at Step 120: loss [2.1346039030762323]\n",
            "Model Trained at Step 140: loss [2.1219426078003387]\n",
            "Model Trained at Step 160: loss [2.0990651957402156]\n",
            "Model Trained at Step 180: loss [2.072708614051235]\n",
            "Model Trained at Step 200: loss [2.04397677304945]\n",
            "Model Trained at Step 220: loss [2.0158411921003156]\n",
            "Model Trained at Step 240: loss [1.9843438042296282]\n",
            "Model Trained at Step 260: loss [1.9406856818229126]\n",
            "Model Trained at Step 280: loss [1.903075363319988]\n",
            "Model Trained at Step 300: loss [1.874959450216617]\n",
            "Model Trained at Step 320: loss [1.838657096842455]\n",
            "Model Trained at Step 340: loss [1.8182759365485897]\n",
            "Model Trained at Step 360: loss [1.8083289322701979]\n",
            "Model Trained at Step 380: loss [1.803258664097767]\n",
            "Model Trained at Step 400: loss [1.793931284037971]\n",
            "Model Trained at Step 420: loss [1.7838216196575636]\n",
            "Model Trained at Step 440: loss [1.776144981067381]\n",
            "Model Trained at Step 460: loss [1.77183382819391]\n",
            "Model Trained at Step 480: loss [1.7696617165442432]\n",
            "Model Trained at Step 500: loss [1.7682933705171298]\n",
            "Model Trained at Step 520: loss [1.7744836535639021]\n",
            "Model Trained at Step 540: loss [1.7851986989239244]\n",
            "Model Trained at Step 560: loss [1.788127207240161]\n",
            "Model Trained at Step 580: loss [1.7742283849869533]\n",
            "Model Trained at Step 600: loss [1.758787872978243]\n",
            "Model Trained at Step 620: loss [1.7633825237849554]\n",
            "Model Trained at Step 640: loss [1.7776343128806515]\n",
            "Model Trained at Step 660: loss [1.7950256412783738]\n",
            "Model Trained at Step 680: loss [1.81316961066773]\n",
            "Model Trained at Step 700: loss [1.8355631681109608]\n",
            "Model Trained at Step 720: loss [1.8610708960960238]\n",
            "Model Trained at Step 740: loss [1.8890913345724383]\n",
            "Model Trained at Step 760: loss [1.9202641267268512]\n",
            "Model Trained at Step 780: loss [1.9465195561585937]\n",
            "Model Trained at Step 800: loss [1.9703525820370562]\n",
            "Model Trained at Step 820: loss [1.994901087211891]\n",
            "Model Trained at Step 840: loss [2.0145008002868496]\n",
            "Model Trained at Step 860: loss [2.0328954466625238]\n",
            "Model Trained at Step 880: loss [2.0522849904379177]\n",
            "Model Trained at Step 900: loss [2.0685843404720794]\n",
            "Model Trained at Step 920: loss [2.0820809728891647]\n",
            "Model Trained at Step 940: loss [2.098074471917598]\n",
            "Model Trained at Step 960: loss [2.1183577505795808]\n",
            "Model Trained at Step 980: loss [2.143571349591176]\n",
            "Model Trained at Step 1000: loss [2.1705039528346473]\n",
            "Model Trained at Step 1020: loss [2.19851902772613]\n",
            "Model Trained at Step 1040: loss [2.2252491429892074]\n",
            "Model Trained at Step 1060: loss [2.25501821104049]\n",
            "Model Trained at Step 1080: loss [2.2796892782020124]\n",
            "Model Trained at Step 1100: loss [2.306834731474744]\n",
            "Model Trained at Step 1120: loss [2.3360892168099023]\n",
            "Model Trained at Step 1140: loss [2.3623011114151775]\n",
            "Model Trained at Step 1160: loss [2.3835440066358946]\n",
            "Model Trained at Step 1180: loss [2.4084325373818714]\n",
            "Model Trained at Step 1200: loss [2.425051905920765]\n",
            "Model Trained at Step 1220: loss [2.443859357742923]\n",
            "Model Trained at Step 1240: loss [2.464215186475587]\n",
            "Model Trained at Step 1260: loss [2.4852879317244296]\n",
            "Model Trained at Step 1280: loss [2.5100738835057927]\n",
            "Model Trained at Step 1300: loss [2.5327554499195215]\n",
            "Model Trained at Step 1320: loss [2.551108165644031]\n",
            "Model Trained at Step 1340: loss [2.5635202603883993]\n",
            "Model Trained at Step 1360: loss [2.5759350888338237]\n",
            "Model Trained at Step 1380: loss [2.587864153562776]\n",
            "Model Trained at Step 1400: loss [2.5937351125966073]\n",
            "Model Trained at Step 1420: loss [2.6029032927740046]\n",
            "Model Trained at Step 1440: loss [2.605628625434714]\n",
            "Model Trained at Step 1460: loss [2.600672367131867]\n",
            "Model Trained at Step 1480: loss [2.601414871037866]\n",
            "Model Trained at Step 1500: loss [2.6096598806035054]\n",
            "Model Trained at Step 1520: loss [2.6211647446885906]\n",
            "Model Trained at Step 1540: loss [2.6342662945858324]\n",
            "Model Trained at Step 1560: loss [2.6455698477879643]\n",
            "Model Trained at Step 1580: loss [2.652751841985308]\n",
            "Model Trained at Step 1600: loss [2.6692933978078957]\n",
            "Model Trained at Step 1620: loss [2.691717177392531]\n",
            "Model Trained at Step 1640: loss [2.709331781394215]\n",
            "Model Trained at Step 1660: loss [2.7114675302808005]\n",
            "Model Trained at Step 1680: loss [2.723082915753544]\n",
            "Model Trained at Step 1700: loss [2.736400938042002]\n",
            "Model Trained at Step 1720: loss [2.7504511633021584]\n",
            "Model Trained at Step 1740: loss [2.7611998163131175]\n",
            "Model Trained at Step 1760: loss [2.7636974200103945]\n",
            "Model Trained at Step 1780: loss [2.7743428633252116]\n",
            "Model Trained at Step 1800: loss [2.7791598990328135]\n",
            "Model Trained at Step 1820: loss [2.7778416188233463]\n",
            "Model Trained at Step 1840: loss [2.7714774099128343]\n",
            "Model Trained at Step 1860: loss [2.7632695436299017]\n",
            "Model Trained at Step 1880: loss [2.708327096781827]\n",
            "Model Trained at Step 1900: loss [2.830378490330897]\n",
            "Model Trained at Step 1920: loss [2.939012882298802]\n",
            "Model Trained at Step 1940: loss [2.9499142367484934]\n",
            "Model Trained at Step 1960: loss [2.9520216214759665]\n",
            "Model Trained at Step 1980: loss [2.948661819006437]\n",
            "Model Trained at Step 2000: loss [2.9414720598373103]\n",
            "Model Trained at Step 2020: loss [2.937944353135011]\n",
            "Model Trained at Step 2040: loss [2.934699800151267]\n",
            "Model Trained at Step 2060: loss [2.9252567737026114]\n",
            "Model Trained at Step 2080: loss [2.9185149269950172]\n",
            "Model Trained at Step 2100: loss [2.910170538068114]\n",
            "Model Trained at Step 2120: loss [2.901358356674483]\n",
            "Model Trained at Step 2140: loss [2.8915273241100983]\n",
            "Model Trained at Step 2160: loss [2.8781191385718996]\n",
            "Model Trained at Step 2180: loss [2.865295045538484]\n",
            "Model Trained at Step 2200: loss [2.8533615920538096]\n",
            "Model Trained at Step 2220: loss [2.8400685149462643]\n",
            "Model Trained at Step 2240: loss [2.8214963003552054]\n",
            "Model Trained at Step 2260: loss [2.804274248089423]\n",
            "Model Trained at Step 2280: loss [2.7870929543914373]\n",
            "Model Trained at Step 2300: loss [2.768785986593257]\n",
            "Model Trained at Step 2320: loss [2.751445340952004]\n",
            "Model Trained at Step 2340: loss [2.7311326888545393]\n",
            "Model Trained at Step 2360: loss [2.710637457580785]\n",
            "Model Trained at Step 2380: loss [2.6875300512895666]\n",
            "Model Trained at Step 2400: loss [2.6680844617755404]\n",
            "Model Trained at Step 2420: loss [2.647438700923751]\n",
            "Model Trained at Step 2440: loss [2.6225137373242857]\n",
            "Model Trained at Step 2460: loss [2.599469527718569]\n",
            "Model Trained at Step 2480: loss [2.5756433929219757]\n",
            "Model Trained at Step 2500: loss [2.5572455697021335]\n",
            "Model Trained at Step 2520: loss [2.537107203959571]\n",
            "Model Trained at Step 2540: loss [2.5172227216771024]\n",
            "Model Trained at Step 2560: loss [2.496505737918617]\n",
            "Model Trained at Step 2580: loss [2.4771779289501046]\n",
            "Model Trained at Step 2600: loss [2.4595555709748487]\n",
            "Model Trained at Step 2620: loss [2.4411957002279663]\n",
            "Model Trained at Step 2640: loss [2.424471135662153]\n",
            "Model Trained at Step 2660: loss [2.40721893270766]\n",
            "Model Trained at Step 2680: loss [2.392306870495073]\n",
            "Model Trained at Step 2700: loss [2.3764039015081346]\n",
            "Model Trained at Step 2720: loss [2.3636435127063407]\n",
            "Model Trained at Step 2740: loss [2.350014093386556]\n",
            "Model Trained at Step 2760: loss [2.335780070370531]\n",
            "Model Trained at Step 2780: loss [2.3205718377804754]\n",
            "Model Trained at Step 2800: loss [2.3084332362873248]\n",
            "Model Trained at Step 2820: loss [2.2939049060452392]\n",
            "Model Trained at Step 2840: loss [2.2807186161295547]\n",
            "Model Trained at Step 2860: loss [2.2642745345267907]\n",
            "Model Trained at Step 2880: loss [2.245585987619518]\n",
            "Model Trained at Step 2900: loss [2.2281414872511953]\n",
            "Model Trained at Step 2920: loss [2.2029773348641335]\n",
            "Model Trained at Step 2940: loss [2.1714466801842796]\n",
            "Model Trained at Step 2960: loss [2.1361971258919232]\n",
            "Model Trained at Step 2980: loss [2.0945901943148066]\n",
            "Model Trained at Step 3000: loss [2.046486991896468]\n",
            "Model Trained at Step 3020: loss [1.999694550009913]\n",
            "Model Trained at Step 3040: loss [1.9582525351125972]\n",
            "Model Trained at Step 3060: loss [1.9215321709617446]\n",
            "Model Trained at Step 3080: loss [1.893870403844658]\n",
            "Model Trained at Step 3100: loss [1.8666673094982826]\n",
            "Model Trained at Step 3120: loss [1.839252459992321]\n",
            "Model Trained at Step 3140: loss [1.81354734531454]\n",
            "Model Trained at Step 3160: loss [1.7878614919925908]\n",
            "Model Trained at Step 3180: loss [1.76706464619547]\n",
            "Model Trained at Step 3200: loss [1.8286545607970397]\n",
            "Model Trained at Step 3220: loss [1.7679257521867313]\n",
            "Model Trained at Step 3240: loss [1.8886323403717278]\n",
            "Model Trained at Step 3260: loss [1.8028378672207694]\n",
            "Model Trained at Step 3280: loss [1.7885251845548311]\n",
            "Model Trained at Step 3300: loss [1.8068571022576196]\n",
            "Model Trained at Step 3320: loss [1.7788832726815849]\n",
            "Model Trained at Step 3340: loss [1.7318332081389094]\n",
            "Model Trained at Step 3360: loss [1.812883477223459]\n",
            "Model Trained at Step 3380: loss [1.9448767684257569]\n",
            "Model Trained at Step 3400: loss [1.7909121889699908]\n",
            "Model Trained at Step 3420: loss [1.7422432147773546]\n",
            "Model Trained at Step 3440: loss [1.9165947317577121]\n",
            "Model Trained at Step 3460: loss [1.8031524715802743]\n",
            "Model Trained at Step 3480: loss [1.6803263873291314]\n",
            "Model Trained at Step 3500: loss [1.697002755444807]\n",
            "Model Trained at Step 3520: loss [1.715851997091924]\n",
            "Model Trained at Step 3540: loss [1.704135526080599]\n",
            "Model Trained at Step 3560: loss [1.6700233255755936]\n",
            "Model Trained at Step 3580: loss [1.6846636472471233]\n",
            "Model Trained at Step 3600: loss [1.675017715300232]\n",
            "Model Trained at Step 3620: loss [1.71706125678264]\n",
            "Model Trained at Step 3640: loss [1.6790548615225636]\n",
            "Model Trained at Step 3660: loss [1.652956070857782]\n",
            "Model Trained at Step 3680: loss [1.6850001150988259]\n",
            "Model Trained at Step 3700: loss [1.6856582863747587]\n",
            "Model Trained at Step 3720: loss [1.63629961439395]\n",
            "Model Trained at Step 3740: loss [1.6316161543562422]\n",
            "Model Trained at Step 3760: loss [1.6243919468406365]\n",
            "Model Trained at Step 3780: loss [1.6345731805398427]\n",
            "Model Trained at Step 3800: loss [1.6305666617049888]\n",
            "Model Trained at Step 3820: loss [1.6363463750332854]\n",
            "Model Trained at Step 3840: loss [1.6335409965103012]\n",
            "Model Trained at Step 3860: loss [1.6392766668777832]\n",
            "Model Trained at Step 3880: loss [1.6343576872781675]\n",
            "Model Trained at Step 3900: loss [1.6410946358187082]\n",
            "Model Trained at Step 3920: loss [1.6372176839311874]\n",
            "Model Trained at Step 3940: loss [1.640632460459075]\n",
            "Model Trained at Step 3960: loss [1.6413631240162458]\n",
            "Model Trained at Step 3980: loss [1.699704057333193]\n",
            "Model Trained at Step 4000: loss [1.69941179993318]\n",
            "Model Trained at Step 4020: loss [1.676535402231541]\n",
            "Model Trained at Step 4040: loss [1.6715992336279144]\n",
            "Model Trained at Step 4060: loss [1.6685447792925128]\n",
            "Model Trained at Step 4080: loss [1.726362622729706]\n",
            "Model Trained at Step 4100: loss [1.7490663463943004]\n",
            "Model Trained at Step 4120: loss [1.7406412524990984]\n",
            "Model Trained at Step 4140: loss [1.7453307139251941]\n",
            "Model Trained at Step 4160: loss [1.7060760098454046]\n",
            "Model Trained at Step 4180: loss [1.790596321813651]\n",
            "Model Trained at Step 4200: loss [1.7498048123075503]\n",
            "Model Trained at Step 4220: loss [1.7662540169439456]\n",
            "Model Trained at Step 4240: loss [1.8448561276249182]\n",
            "Model Trained at Step 4260: loss [1.8098130090590856]\n",
            "Model Trained at Step 4280: loss [1.8390808808135]\n",
            "Model Trained at Step 4300: loss [1.800468458362435]\n",
            "Model Trained at Step 4320: loss [1.7940418758942738]\n",
            "Model Trained at Step 4340: loss [1.7831520738345639]\n",
            "Model Trained at Step 4360: loss [1.8290790796016676]\n",
            "Model Trained at Step 4380: loss [1.8653621594498482]\n",
            "Model Trained at Step 4400: loss [1.903921968508343]\n",
            "Model Trained at Step 4420: loss [1.8735871105291395]\n",
            "Model Trained at Step 4440: loss [1.8217035074240713]\n",
            "Model Trained at Step 4460: loss [1.8530200953274878]\n",
            "Model Trained at Step 4480: loss [1.868032689255394]\n",
            "Model Trained at Step 4500: loss [1.9312899704218345]\n",
            "Model Trained at Step 4520: loss [1.940615075253359]\n",
            "Model Trained at Step 4540: loss [1.9441229704611331]\n",
            "Model Trained at Step 4560: loss [2.02657087114992]\n",
            "Model Trained at Step 4580: loss [2.0618082795061934]\n",
            "Model Trained at Step 4600: loss [1.980259497023073]\n",
            "Model Trained at Step 4620: loss [1.9941100402459593]\n",
            "Model Trained at Step 4640: loss [2.0477935040132067]\n",
            "Model Trained at Step 4660: loss [2.0721885898559522]\n",
            "Model Trained at Step 4680: loss [2.127163080289086]\n",
            "Model Trained at Step 4700: loss [2.1451664692632164]\n",
            "Model Trained at Step 4720: loss [2.0897178537579975]\n",
            "Model Trained at Step 4740: loss [2.1740531963880505]\n",
            "Model Trained at Step 4760: loss [2.1355606954079804]\n",
            "Model Trained at Step 4780: loss [2.1447564358503395]\n",
            "Model Trained at Step 4800: loss [2.1347841023207454]\n",
            "Model Trained at Step 4820: loss [2.2562132223214424]\n",
            "Model Trained at Step 4840: loss [2.1842149962790294]\n",
            "Model Trained at Step 4860: loss [2.1404847616141036]\n",
            "Model Trained at Step 4880: loss [2.1611326677126623]\n",
            "Model Trained at Step 4900: loss [2.196705310747335]\n",
            "Model Trained at Step 4920: loss [2.194658072567159]\n",
            "Model Trained at Step 4940: loss [2.182341287150528]\n",
            "Model Trained at Step 4960: loss [2.2433969537351732]\n",
            "Model Trained at Step 4980: loss [2.269596834885239]\n",
            "Model Trained at Step 5000: loss [2.1883166762603814]\n",
            "Simulation ev training 6 done\n",
            "Model Trained at Step 20: loss [2.34058669923811]\n",
            "Model Trained at Step 40: loss [2.1565082370478335]\n",
            "Model Trained at Step 60: loss [2.1598444689904857]\n",
            "Model Trained at Step 80: loss [2.1564429259572986]\n",
            "Model Trained at Step 100: loss [2.152759097367187]\n",
            "Model Trained at Step 120: loss [2.1343218953788874]\n",
            "Model Trained at Step 140: loss [2.12004318080179]\n",
            "Model Trained at Step 160: loss [2.1008438352327206]\n",
            "Model Trained at Step 180: loss [2.0532592050364733]\n",
            "Model Trained at Step 200: loss [2.0214722851325915]\n",
            "Model Trained at Step 220: loss [1.9988345675706227]\n",
            "Model Trained at Step 240: loss [1.9679954906577994]\n",
            "Model Trained at Step 260: loss [1.932549692942764]\n",
            "Model Trained at Step 280: loss [1.891983746810007]\n",
            "Model Trained at Step 300: loss [1.8721829068985953]\n",
            "Model Trained at Step 320: loss [1.8585011230044421]\n",
            "Model Trained at Step 340: loss [1.8451202048748994]\n",
            "Model Trained at Step 360: loss [1.8232344347315999]\n",
            "Model Trained at Step 380: loss [1.8119323274036865]\n",
            "Model Trained at Step 400: loss [1.8078412749541173]\n",
            "Model Trained at Step 420: loss [1.8015025612702513]\n",
            "Model Trained at Step 440: loss [1.7877442882683767]\n",
            "Model Trained at Step 460: loss [1.7769738257514962]\n",
            "Model Trained at Step 480: loss [1.7741094535598811]\n",
            "Model Trained at Step 500: loss [1.7732158446223338]\n",
            "Model Trained at Step 520: loss [1.7750469299831082]\n",
            "Model Trained at Step 540: loss [1.7630663754318594]\n",
            "Model Trained at Step 560: loss [1.7524159757525055]\n",
            "Model Trained at Step 580: loss [1.7538758706851774]\n",
            "Model Trained at Step 600: loss [1.7657370618347379]\n",
            "Model Trained at Step 620: loss [1.7816493403391926]\n",
            "Model Trained at Step 640: loss [1.7999567492259583]\n",
            "Model Trained at Step 660: loss [1.8197160260422787]\n",
            "Model Trained at Step 680: loss [1.843223512329444]\n",
            "Model Trained at Step 700: loss [1.8697849642115245]\n",
            "Model Trained at Step 720: loss [1.8949196149570475]\n",
            "Model Trained at Step 740: loss [1.9188359432562918]\n",
            "Model Trained at Step 760: loss [1.9426355296841424]\n",
            "Model Trained at Step 780: loss [1.9631282692461915]\n",
            "Model Trained at Step 800: loss [1.9829152329807829]\n",
            "Model Trained at Step 820: loss [1.998956528175374]\n",
            "Model Trained at Step 840: loss [2.01567095502624]\n",
            "Model Trained at Step 860: loss [2.0304541868085924]\n",
            "Model Trained at Step 880: loss [2.0455682585426054]\n",
            "Model Trained at Step 900: loss [2.063191168310664]\n",
            "Model Trained at Step 920: loss [2.0798061375232755]\n",
            "Model Trained at Step 940: loss [2.100197881819381]\n",
            "Model Trained at Step 960: loss [2.120444348287765]\n",
            "Model Trained at Step 980: loss [2.1413541333220336]\n",
            "Model Trained at Step 1000: loss [2.1623319020520593]\n",
            "Model Trained at Step 1020: loss [2.183936083958283]\n",
            "Model Trained at Step 1040: loss [2.2061140510723956]\n",
            "Model Trained at Step 1060: loss [2.2337424089688382]\n",
            "Model Trained at Step 1080: loss [2.2638293637509577]\n",
            "Model Trained at Step 1100: loss [2.2903478798978805]\n",
            "Model Trained at Step 1120: loss [2.3174458641056286]\n",
            "Model Trained at Step 1140: loss [2.336123796026957]\n",
            "Model Trained at Step 1160: loss [2.357215074510899]\n",
            "Model Trained at Step 1180: loss [2.3744884070469348]\n",
            "Model Trained at Step 1200: loss [2.391380559280319]\n",
            "Model Trained at Step 1220: loss [2.4056182940635926]\n",
            "Model Trained at Step 1240: loss [2.422461306846068]\n",
            "Model Trained at Step 1260: loss [2.437992752147091]\n",
            "Model Trained at Step 1280: loss [2.447561471777152]\n",
            "Model Trained at Step 1300: loss [2.4616573232831347]\n",
            "Model Trained at Step 1320: loss [2.474408185828513]\n",
            "Model Trained at Step 1340: loss [2.4884279757027565]\n",
            "Model Trained at Step 1360: loss [2.500287188616503]\n",
            "Model Trained at Step 1380: loss [2.512756191552941]\n",
            "Model Trained at Step 1400: loss [2.5183052534367847]\n",
            "Model Trained at Step 1420: loss [2.529390256303765]\n",
            "Model Trained at Step 1440: loss [2.537658321042341]\n",
            "Model Trained at Step 1460: loss [2.539216114492914]\n",
            "Model Trained at Step 1480: loss [2.540361146286415]\n",
            "Model Trained at Step 1500: loss [2.54735743378203]\n",
            "Model Trained at Step 1520: loss [2.55439278192566]\n",
            "Model Trained at Step 1540: loss [2.569686743291981]\n",
            "Model Trained at Step 1560: loss [2.5905610763277247]\n",
            "Model Trained at Step 1580: loss [2.610722274092942]\n",
            "Model Trained at Step 1600: loss [2.628411200038962]\n",
            "Model Trained at Step 1620: loss [2.645743334144752]\n",
            "Model Trained at Step 1640: loss [2.6709665152839235]\n",
            "Model Trained at Step 1660: loss [2.699267525060732]\n",
            "Model Trained at Step 1680: loss [2.7317438366966744]\n",
            "Model Trained at Step 1700: loss [2.753310901633667]\n",
            "Model Trained at Step 1720: loss [2.772532935514492]\n",
            "Model Trained at Step 1740: loss [2.801598910966552]\n",
            "Model Trained at Step 1760: loss [2.8301474432174567]\n",
            "Model Trained at Step 1780: loss [2.859989422097269]\n",
            "Model Trained at Step 1800: loss [2.888189795679154]\n",
            "Model Trained at Step 1820: loss [2.9163060846548476]\n",
            "Model Trained at Step 1840: loss [2.94471972035036]\n",
            "Model Trained at Step 1860: loss [2.9712946980257704]\n",
            "Model Trained at Step 1880: loss [2.9924170609286986]\n",
            "Model Trained at Step 1900: loss [3.0094225485676542]\n",
            "Model Trained at Step 1920: loss [3.0221885575032768]\n",
            "Model Trained at Step 1940: loss [3.0318662756708745]\n",
            "Model Trained at Step 1960: loss [3.039424326595037]\n",
            "Model Trained at Step 1980: loss [3.04197341388231]\n",
            "Model Trained at Step 2000: loss [3.0388757827077493]\n",
            "Model Trained at Step 2020: loss [3.029717135329605]\n",
            "Model Trained at Step 2040: loss [3.0175472207836695]\n",
            "Model Trained at Step 2060: loss [3.001266180120124]\n",
            "Model Trained at Step 2080: loss [2.986693996527606]\n",
            "Model Trained at Step 2100: loss [2.967056423290921]\n",
            "Model Trained at Step 2120: loss [2.9465064057668173]\n",
            "Model Trained at Step 2140: loss [2.9205483135795784]\n",
            "Model Trained at Step 2160: loss [2.8959136143049102]\n",
            "Model Trained at Step 2180: loss [2.871854816628979]\n",
            "Model Trained at Step 2200: loss [2.8489946556321133]\n",
            "Model Trained at Step 2220: loss [2.826799577291834]\n",
            "Model Trained at Step 2240: loss [2.8090765336288017]\n",
            "Model Trained at Step 2260: loss [2.7949456431925377]\n",
            "Model Trained at Step 2280: loss [2.781382644769921]\n",
            "Model Trained at Step 2300: loss [2.7694959164492174]\n",
            "Model Trained at Step 2320: loss [2.758779374249431]\n",
            "Model Trained at Step 2340: loss [2.7470388459091537]\n",
            "Model Trained at Step 2360: loss [2.7328488667135598]\n",
            "Model Trained at Step 2380: loss [2.718692606556316]\n",
            "Model Trained at Step 2400: loss [2.709574294603079]\n",
            "Model Trained at Step 2420: loss [2.702527327034731]\n",
            "Model Trained at Step 2440: loss [2.697744941305193]\n",
            "Model Trained at Step 2460: loss [2.6941285123529677]\n",
            "Model Trained at Step 2480: loss [2.6907578149310942]\n",
            "Model Trained at Step 2500: loss [2.692835591727356]\n",
            "Model Trained at Step 2520: loss [2.696130109932222]\n",
            "Model Trained at Step 2540: loss [2.7008935795913174]\n",
            "Model Trained at Step 2560: loss [2.7063552713058163]\n",
            "Model Trained at Step 2580: loss [2.713940213627472]\n",
            "Model Trained at Step 2600: loss [2.7209364570892545]\n",
            "Model Trained at Step 2620: loss [2.7291897655420767]\n",
            "Model Trained at Step 2640: loss [2.7359079312711003]\n",
            "Model Trained at Step 2660: loss [2.750101522428296]\n",
            "Model Trained at Step 2680: loss [2.7621178667128574]\n",
            "Model Trained at Step 2700: loss [2.7765277553163217]\n",
            "Model Trained at Step 2720: loss [2.7901525149761763]\n",
            "Model Trained at Step 2740: loss [2.8040721347951623]\n",
            "Model Trained at Step 2760: loss [2.8207528241074766]\n",
            "Model Trained at Step 2780: loss [2.841372097542293]\n",
            "Model Trained at Step 2800: loss [2.8615489932699036]\n",
            "Model Trained at Step 2820: loss [2.8821574229662015]\n",
            "Model Trained at Step 2840: loss [2.9051483810249463]\n",
            "Model Trained at Step 2860: loss [2.9289458306334306]\n",
            "Model Trained at Step 2880: loss [2.948121589753953]\n",
            "Model Trained at Step 2900: loss [2.9663363443334263]\n",
            "Model Trained at Step 2920: loss [2.981650976082581]\n",
            "Model Trained at Step 2940: loss [2.9962980806632378]\n",
            "Model Trained at Step 2960: loss [3.013189332072637]\n",
            "Model Trained at Step 2980: loss [3.0273736301601866]\n",
            "Model Trained at Step 3000: loss [3.0417563579886813]\n",
            "Model Trained at Step 3020: loss [3.0534482563699377]\n",
            "Model Trained at Step 3040: loss [3.0612096751394615]\n",
            "Model Trained at Step 3060: loss [3.067670596144199]\n",
            "Model Trained at Step 3080: loss [3.07201521142376]\n",
            "Model Trained at Step 3100: loss [3.0784895200657183]\n",
            "Model Trained at Step 3120: loss [3.081714439040721]\n",
            "Model Trained at Step 3140: loss [3.0858492368163697]\n",
            "Model Trained at Step 3160: loss [3.0860206823869256]\n",
            "Model Trained at Step 3180: loss [3.0863748700804385]\n",
            "Model Trained at Step 3200: loss [3.088399734435561]\n",
            "Model Trained at Step 3220: loss [3.0942299248944005]\n",
            "Model Trained at Step 3240: loss [3.101109115031705]\n",
            "Model Trained at Step 3260: loss [3.1065778709234353]\n",
            "Model Trained at Step 3280: loss [3.1119261580811264]\n",
            "Model Trained at Step 3300: loss [3.116704706809614]\n",
            "Model Trained at Step 3320: loss [3.1223801400652533]\n",
            "Model Trained at Step 3340: loss [3.1298007602850983]\n",
            "Model Trained at Step 3360: loss [3.138150315146221]\n",
            "Model Trained at Step 3380: loss [3.1476062862172185]\n",
            "Model Trained at Step 3400: loss [3.1554904808545223]\n",
            "Model Trained at Step 3420: loss [3.1647768069726308]\n",
            "Model Trained at Step 3440: loss [3.1751769735614617]\n",
            "Model Trained at Step 3460: loss [3.1860826169906]\n",
            "Model Trained at Step 3480: loss [3.199510792377411]\n",
            "Model Trained at Step 3500: loss [3.219076884362637]\n",
            "Model Trained at Step 3520: loss [3.2460322802805406]\n",
            "Model Trained at Step 3540: loss [3.27439945442015]\n",
            "Model Trained at Step 3560: loss [3.304775342008348]\n",
            "Model Trained at Step 3580: loss [3.3332615028575696]\n",
            "Model Trained at Step 3600: loss [3.362174645210324]\n",
            "Model Trained at Step 3620: loss [3.388637396432371]\n",
            "Model Trained at Step 3640: loss [3.414899921320793]\n",
            "Model Trained at Step 3660: loss [3.4382188158602474]\n",
            "Model Trained at Step 3680: loss [3.464747935326661]\n",
            "Model Trained at Step 3700: loss [3.4930421533958396]\n",
            "Model Trained at Step 3720: loss [3.5172031460288222]\n",
            "Model Trained at Step 3740: loss [3.544198430831191]\n",
            "Model Trained at Step 3760: loss [3.572488040090105]\n",
            "Model Trained at Step 3780: loss [3.599408907857511]\n",
            "Model Trained at Step 3800: loss [3.63017023435117]\n",
            "Model Trained at Step 3820: loss [3.662387846142218]\n",
            "Model Trained at Step 3840: loss [3.6989668488113567]\n",
            "Model Trained at Step 3860: loss [3.732290158922337]\n",
            "Model Trained at Step 3880: loss [3.768308823174094]\n",
            "Model Trained at Step 3900: loss [3.8019278563186902]\n",
            "Model Trained at Step 3920: loss [3.832244915621472]\n",
            "Model Trained at Step 3940: loss [3.86202903883826]\n",
            "Model Trained at Step 3960: loss [3.8868857060184845]\n",
            "Model Trained at Step 3980: loss [3.911216953198059]\n",
            "Model Trained at Step 4000: loss [3.931789278698614]\n",
            "Model Trained at Step 4020: loss [3.9517322051248867]\n",
            "Model Trained at Step 4040: loss [3.970612691865256]\n",
            "Model Trained at Step 4060: loss [3.9875867421466666]\n",
            "Model Trained at Step 4080: loss [3.9983333648725923]\n",
            "Model Trained at Step 4100: loss [4.0050186958673795]\n",
            "Model Trained at Step 4120: loss [4.005974195218249]\n",
            "Model Trained at Step 4140: loss [4.004591772380837]\n",
            "Model Trained at Step 4160: loss [4.002894106399227]\n",
            "Model Trained at Step 4180: loss [4.001477212758476]\n",
            "Model Trained at Step 4200: loss [3.996935128289368]\n",
            "Model Trained at Step 4220: loss [3.9939975061282196]\n",
            "Model Trained at Step 4240: loss [3.991030226862513]\n",
            "Model Trained at Step 4260: loss [3.9928197170213964]\n",
            "Model Trained at Step 4280: loss [3.992882047516292]\n",
            "Model Trained at Step 4300: loss [3.9865774773641904]\n",
            "Model Trained at Step 4320: loss [3.9825648935851126]\n",
            "Model Trained at Step 4340: loss [3.975314967458034]\n",
            "Model Trained at Step 4360: loss [3.9713839018238026]\n",
            "Model Trained at Step 4380: loss [3.968236701226761]\n",
            "Model Trained at Step 4400: loss [3.9665047721014375]\n",
            "Model Trained at Step 4420: loss [3.963161670818713]\n",
            "Model Trained at Step 4440: loss [3.9552358540794863]\n",
            "Model Trained at Step 4460: loss [3.94576838067037]\n",
            "Model Trained at Step 4480: loss [3.9314862052223765]\n",
            "Model Trained at Step 4500: loss [3.917670613247215]\n",
            "Model Trained at Step 4520: loss [3.89703379765767]\n",
            "Model Trained at Step 4540: loss [3.8739945289074753]\n",
            "Model Trained at Step 4560: loss [3.850637137948513]\n",
            "Model Trained at Step 4580: loss [3.824430854267871]\n",
            "Model Trained at Step 4600: loss [3.8005593969755154]\n",
            "Model Trained at Step 4620: loss [3.775347216491361]\n",
            "Model Trained at Step 4640: loss [3.7508383671091883]\n",
            "Model Trained at Step 4660: loss [3.7240187275662793]\n",
            "Model Trained at Step 4680: loss [3.6919562278970064]\n",
            "Model Trained at Step 4700: loss [3.6506684550972937]\n",
            "Model Trained at Step 4720: loss [3.610172984682406]\n",
            "Model Trained at Step 4740: loss [3.5612627459054607]\n",
            "Model Trained at Step 4760: loss [3.5093170739016983]\n",
            "Model Trained at Step 4780: loss [3.452966155870846]\n",
            "Model Trained at Step 4800: loss [3.3884091035107753]\n",
            "Model Trained at Step 4820: loss [3.318472719027034]\n",
            "Model Trained at Step 4840: loss [3.241728433815857]\n",
            "Model Trained at Step 4860: loss [3.126196409804762]\n",
            "Model Trained at Step 4880: loss [2.9383223915488355]\n",
            "Model Trained at Step 4900: loss [2.7247838237388464]\n",
            "Model Trained at Step 4920: loss [2.5714624773570116]\n",
            "Model Trained at Step 4940: loss [2.4579926895585933]\n",
            "Model Trained at Step 4960: loss [2.375954952198007]\n",
            "Model Trained at Step 4980: loss [2.298819348089978]\n",
            "Model Trained at Step 5000: loss [2.234667026765282]\n",
            "Simulation ev training 7 done\n",
            "Model Trained at Step 20: loss [2.536034108767045]\n",
            "Model Trained at Step 40: loss [2.0226312476242176]\n",
            "Model Trained at Step 60: loss [2.0558552405452324]\n",
            "Model Trained at Step 80: loss [2.065787284417651]\n",
            "Model Trained at Step 100: loss [2.0422652821288767]\n",
            "Model Trained at Step 120: loss [2.0513610244845424]\n",
            "Model Trained at Step 140: loss [2.030507744592729]\n",
            "Model Trained at Step 160: loss [1.976909879080034]\n",
            "Model Trained at Step 180: loss [1.9371771070424164]\n",
            "Model Trained at Step 200: loss [1.899131464415855]\n",
            "Model Trained at Step 220: loss [1.8649161266072398]\n",
            "Model Trained at Step 240: loss [1.8520086166031944]\n",
            "Model Trained at Step 260: loss [1.823942169195735]\n",
            "Model Trained at Step 280: loss [1.8035880489634692]\n",
            "Model Trained at Step 300: loss [1.7896135062058156]\n",
            "Model Trained at Step 320: loss [1.7643118388268761]\n",
            "Model Trained at Step 340: loss [1.7461867640579136]\n",
            "Model Trained at Step 360: loss [1.7326059451005524]\n",
            "Model Trained at Step 380: loss [1.7222841068369033]\n",
            "Model Trained at Step 400: loss [1.7149363851351853]\n",
            "Model Trained at Step 420: loss [1.705433661318357]\n",
            "Model Trained at Step 440: loss [1.6977499692813052]\n",
            "Model Trained at Step 460: loss [1.6908719249659594]\n",
            "Model Trained at Step 480: loss [1.6836069359860417]\n",
            "Model Trained at Step 500: loss [1.6757950757748674]\n",
            "Model Trained at Step 520: loss [1.6673945695393357]\n",
            "Model Trained at Step 540: loss [1.6613259916315477]\n",
            "Model Trained at Step 560: loss [1.6587551973539267]\n",
            "Model Trained at Step 580: loss [1.6562715676824038]\n",
            "Model Trained at Step 600: loss [1.65353396988423]\n",
            "Model Trained at Step 620: loss [1.6476054475596555]\n",
            "Model Trained at Step 640: loss [1.6409348728873592]\n",
            "Model Trained at Step 660: loss [1.6336945237149223]\n",
            "Model Trained at Step 680: loss [1.6293341302328748]\n",
            "Model Trained at Step 700: loss [1.6237147004802814]\n",
            "Model Trained at Step 720: loss [1.6258853362032446]\n",
            "Model Trained at Step 740: loss [1.6232995634117373]\n",
            "Model Trained at Step 760: loss [1.6134506152538688]\n",
            "Model Trained at Step 780: loss [1.5943255225130397]\n",
            "Model Trained at Step 800: loss [1.580445940621739]\n",
            "Model Trained at Step 820: loss [1.5711580726742969]\n",
            "Model Trained at Step 840: loss [1.5616253817758452]\n",
            "Model Trained at Step 860: loss [1.5454999407809702]\n",
            "Model Trained at Step 880: loss [1.5357324724157806]\n",
            "Model Trained at Step 900: loss [1.5337175789310336]\n",
            "Model Trained at Step 920: loss [1.5260922062358018]\n",
            "Model Trained at Step 940: loss [1.5114799599490334]\n",
            "Model Trained at Step 960: loss [1.50275167035864]\n",
            "Model Trained at Step 980: loss [1.495138232788179]\n",
            "Model Trained at Step 1000: loss [1.4945238824600788]\n",
            "Model Trained at Step 1020: loss [1.4962623343212607]\n",
            "Model Trained at Step 1040: loss [1.493057240011462]\n",
            "Model Trained at Step 1060: loss [1.4911257717317066]\n",
            "Model Trained at Step 1080: loss [1.491123093185295]\n",
            "Model Trained at Step 1100: loss [1.4996929966401804]\n",
            "Model Trained at Step 1120: loss [1.5037280017147874]\n",
            "Model Trained at Step 1140: loss [1.510138984065218]\n",
            "Model Trained at Step 1160: loss [1.5160612775400137]\n",
            "Model Trained at Step 1180: loss [1.5207197097299956]\n",
            "Model Trained at Step 1200: loss [1.5295657525675002]\n",
            "Model Trained at Step 1220: loss [1.539938079617015]\n",
            "Model Trained at Step 1240: loss [1.547803715324975]\n",
            "Model Trained at Step 1260: loss [1.5567521948743464]\n",
            "Model Trained at Step 1280: loss [1.568671432999734]\n",
            "Model Trained at Step 1300: loss [1.5739966584197822]\n",
            "Model Trained at Step 1320: loss [1.5805654505009468]\n",
            "Model Trained at Step 1340: loss [1.589483138307449]\n",
            "Model Trained at Step 1360: loss [1.5994412230511839]\n",
            "Model Trained at Step 1380: loss [1.6164551044399253]\n",
            "Model Trained at Step 1400: loss [1.6325865912244584]\n",
            "Model Trained at Step 1420: loss [1.6398395456945365]\n",
            "Model Trained at Step 1440: loss [1.6531770380719302]\n",
            "Model Trained at Step 1460: loss [1.7203810734041611]\n",
            "Model Trained at Step 1480: loss [1.7641019291909124]\n",
            "Model Trained at Step 1500: loss [1.78318932824453]\n",
            "Model Trained at Step 1520: loss [1.8041818364719575]\n",
            "Model Trained at Step 1540: loss [1.8220661451225812]\n",
            "Model Trained at Step 1560: loss [1.8387889451947868]\n",
            "Model Trained at Step 1580: loss [1.8543656049287816]\n",
            "Model Trained at Step 1600: loss [1.864081835964217]\n",
            "Model Trained at Step 1620: loss [1.8725131999365996]\n",
            "Model Trained at Step 1640: loss [1.8762427911393793]\n",
            "Model Trained at Step 1660: loss [1.868567582977919]\n",
            "Model Trained at Step 1680: loss [1.9060629317821842]\n",
            "Model Trained at Step 1700: loss [1.9337089550220496]\n",
            "Model Trained at Step 1720: loss [1.9322318700661216]\n",
            "Model Trained at Step 1740: loss [1.9010617230791915]\n",
            "Model Trained at Step 1760: loss [1.924228505307206]\n",
            "Model Trained at Step 1780: loss [1.9145327949251807]\n",
            "Model Trained at Step 1800: loss [1.9568909325360373]\n",
            "Model Trained at Step 1820: loss [1.9687365677952307]\n",
            "Model Trained at Step 1840: loss [1.9649316334565974]\n",
            "Model Trained at Step 1860: loss [1.960355754196915]\n",
            "Model Trained at Step 1880: loss [1.9579339306416057]\n",
            "Model Trained at Step 1900: loss [1.9485017470095265]\n",
            "Model Trained at Step 1920: loss [1.9399644200628916]\n",
            "Model Trained at Step 1940: loss [1.9323776711796044]\n",
            "Model Trained at Step 1960: loss [1.9229470806433295]\n",
            "Model Trained at Step 1980: loss [1.9220188072280664]\n",
            "Model Trained at Step 2000: loss [1.9021395021564849]\n",
            "Model Trained at Step 2020: loss [1.8922986466453082]\n",
            "Model Trained at Step 2040: loss [1.8817742780530697]\n",
            "Model Trained at Step 2060: loss [1.8628372837515013]\n",
            "Model Trained at Step 2080: loss [1.8571066514950523]\n",
            "Model Trained at Step 2100: loss [1.8883968280785055]\n",
            "Model Trained at Step 2120: loss [1.886901222700838]\n",
            "Model Trained at Step 2140: loss [1.8787844591447989]\n",
            "Model Trained at Step 2160: loss [1.8747728202131158]\n",
            "Model Trained at Step 2180: loss [1.8646547049654583]\n",
            "Model Trained at Step 2200: loss [1.8484299260172463]\n",
            "Model Trained at Step 2220: loss [1.8445846146437503]\n",
            "Model Trained at Step 2240: loss [1.919596737858063]\n",
            "Model Trained at Step 2260: loss [1.9323646726847372]\n",
            "Model Trained at Step 2280: loss [1.9343774335405306]\n",
            "Model Trained at Step 2300: loss [1.934823938369611]\n",
            "Model Trained at Step 2320: loss [1.9343819216179274]\n",
            "Model Trained at Step 2340: loss [1.938906662478334]\n",
            "Model Trained at Step 2360: loss [1.9347664320804945]\n",
            "Model Trained at Step 2380: loss [1.9258728868557842]\n",
            "Model Trained at Step 2400: loss [1.9085371166530714]\n",
            "Model Trained at Step 2420: loss [1.8708488990629513]\n",
            "Model Trained at Step 2440: loss [1.856154810597647]\n",
            "Model Trained at Step 2460: loss [1.9763018249561661]\n",
            "Model Trained at Step 2480: loss [1.991368032598182]\n",
            "Model Trained at Step 2500: loss [1.9935093516109503]\n",
            "Model Trained at Step 2520: loss [1.9938794743342885]\n",
            "Model Trained at Step 2540: loss [1.9990915921967196]\n",
            "Model Trained at Step 2560: loss [2.005762577678025]\n",
            "Model Trained at Step 2580: loss [2.0014973730388914]\n",
            "Model Trained at Step 2600: loss [1.9924007268918629]\n",
            "Model Trained at Step 2620: loss [1.9349484540564608]\n",
            "Model Trained at Step 2640: loss [1.9775850458603474]\n",
            "Model Trained at Step 2660: loss [2.083405530932739]\n",
            "Model Trained at Step 2680: loss [2.106194196747745]\n",
            "Model Trained at Step 2700: loss [2.1061154128167288]\n",
            "Model Trained at Step 2720: loss [2.1095727407650373]\n",
            "Model Trained at Step 2740: loss [2.114675353307162]\n",
            "Model Trained at Step 2760: loss [2.1059158110315055]\n",
            "Model Trained at Step 2780: loss [2.1011857669995004]\n",
            "Model Trained at Step 2800: loss [2.101229826465425]\n",
            "Model Trained at Step 2820: loss [2.09380430319968]\n",
            "Model Trained at Step 2840: loss [2.087372547788464]\n",
            "Model Trained at Step 2860: loss [2.0504369053812033]\n",
            "Model Trained at Step 2880: loss [2.1227004142200174]\n",
            "Model Trained at Step 2900: loss [2.152344291075033]\n",
            "Model Trained at Step 2920: loss [2.1534925186663934]\n",
            "Model Trained at Step 2940: loss [2.1532861852752068]\n",
            "Model Trained at Step 2960: loss [2.149687369839217]\n",
            "Model Trained at Step 2980: loss [2.1446454068050422]\n",
            "Model Trained at Step 3000: loss [2.121949922222158]\n",
            "Model Trained at Step 3020: loss [2.06615369868846]\n",
            "Model Trained at Step 3040: loss [2.018793121753595]\n",
            "Model Trained at Step 3060: loss [2.1727471111993113]\n",
            "Model Trained at Step 3080: loss [2.167117952387337]\n",
            "Model Trained at Step 3100: loss [2.1360935804011283]\n",
            "Model Trained at Step 3120: loss [2.0706879430304426]\n",
            "Model Trained at Step 3140: loss [2.112336141784562]\n",
            "Model Trained at Step 3160: loss [2.1991372406229113]\n",
            "Model Trained at Step 3180: loss [2.196154029568508]\n",
            "Model Trained at Step 3200: loss [2.184961353265357]\n",
            "Model Trained at Step 3220: loss [2.17536310593698]\n",
            "Model Trained at Step 3240: loss [2.1562132450320117]\n",
            "Model Trained at Step 3260: loss [2.1336655698715914]\n",
            "Model Trained at Step 3280: loss [2.105360130197237]\n",
            "Model Trained at Step 3300: loss [2.06742803738632]\n",
            "Model Trained at Step 3320: loss [2.0170008291446297]\n",
            "Model Trained at Step 3340: loss [2.1184681617496723]\n",
            "Model Trained at Step 3360: loss [2.10270136499859]\n",
            "Model Trained at Step 3380: loss [2.0226058960112296]\n",
            "Model Trained at Step 3400: loss [2.034148247800368]\n",
            "Model Trained at Step 3420: loss [2.0910806479144792]\n",
            "Model Trained at Step 3440: loss [2.050116413266818]\n",
            "Model Trained at Step 3460: loss [2.036837346473787]\n",
            "Model Trained at Step 3480: loss [2.099226553068398]\n",
            "Model Trained at Step 3500: loss [2.093359915486072]\n",
            "Model Trained at Step 3520: loss [2.08023526818285]\n",
            "Model Trained at Step 3540: loss [2.064137406890699]\n",
            "Model Trained at Step 3560: loss [2.049121762425555]\n",
            "Model Trained at Step 3580: loss [2.025390235690895]\n",
            "Model Trained at Step 3600: loss [1.9815150286171463]\n",
            "Model Trained at Step 3620: loss [2.0186009634543796]\n",
            "Model Trained at Step 3640: loss [2.017782494447239]\n",
            "Model Trained at Step 3660: loss [1.9423178892223123]\n",
            "Model Trained at Step 3680: loss [1.9742077988624451]\n",
            "Model Trained at Step 3700: loss [2.0516284840914563]\n",
            "Model Trained at Step 3720: loss [2.0962850483860285]\n",
            "Model Trained at Step 3740: loss [2.0964768513519276]\n",
            "Model Trained at Step 3760: loss [2.097415908628274]\n",
            "Model Trained at Step 3780: loss [2.093326531620531]\n",
            "Model Trained at Step 3800: loss [2.0961920006732937]\n",
            "Model Trained at Step 3820: loss [2.0923327045759157]\n",
            "Model Trained at Step 3840: loss [2.0891483286512513]\n",
            "Model Trained at Step 3860: loss [2.081839780687921]\n",
            "Model Trained at Step 3880: loss [2.0730964334576827]\n",
            "Model Trained at Step 3900: loss [2.06222019199875]\n",
            "Model Trained at Step 3920: loss [2.041910622750963]\n",
            "Model Trained at Step 3940: loss [2.0129382285529265]\n",
            "Model Trained at Step 3960: loss [1.9200676146175322]\n",
            "Model Trained at Step 3980: loss [2.016853344582072]\n",
            "Model Trained at Step 4000: loss [2.0379767459123554]\n",
            "Model Trained at Step 4020: loss [2.006566308490254]\n",
            "Model Trained at Step 4040: loss [1.8981984857014187]\n",
            "Model Trained at Step 4060: loss [1.948184872335545]\n",
            "Model Trained at Step 4080: loss [2.014142000697023]\n",
            "Model Trained at Step 4100: loss [2.0361321926055913]\n",
            "Model Trained at Step 4120: loss [2.0242732551361735]\n",
            "Model Trained at Step 4140: loss [2.0027045874317286]\n",
            "Model Trained at Step 4160: loss [1.9490171132921446]\n",
            "Model Trained at Step 4180: loss [2.0056492165293105]\n",
            "Model Trained at Step 4200: loss [2.010751491286453]\n",
            "Model Trained at Step 4220: loss [1.9748964137148803]\n",
            "Model Trained at Step 4240: loss [1.9526527277649521]\n",
            "Model Trained at Step 4260: loss [1.9970251031048112]\n",
            "Model Trained at Step 4280: loss [1.9797997588630092]\n",
            "Model Trained at Step 4300: loss [1.9157744502187977]\n",
            "Model Trained at Step 4320: loss [1.9504038025627786]\n",
            "Model Trained at Step 4340: loss [1.9369325502860444]\n",
            "Model Trained at Step 4360: loss [1.9088286350744863]\n",
            "Model Trained at Step 4380: loss [1.9429895964028543]\n",
            "Model Trained at Step 4400: loss [1.8208093086720716]\n",
            "Model Trained at Step 4420: loss [1.9381391096955192]\n",
            "Model Trained at Step 4440: loss [1.9467218576323997]\n",
            "Model Trained at Step 4460: loss [1.8885547255506143]\n",
            "Model Trained at Step 4480: loss [1.8694536229583973]\n",
            "Model Trained at Step 4500: loss [1.8276331479241918]\n",
            "Model Trained at Step 4520: loss [1.882908389470293]\n",
            "Model Trained at Step 4540: loss [1.9215167870072272]\n",
            "Model Trained at Step 4560: loss [1.941389266370766]\n",
            "Model Trained at Step 4580: loss [1.9373585495600874]\n",
            "Model Trained at Step 4600: loss [1.9029366146835751]\n",
            "Model Trained at Step 4620: loss [1.816687489667722]\n",
            "Model Trained at Step 4640: loss [1.8707404537362038]\n",
            "Model Trained at Step 4660: loss [1.921538383138896]\n",
            "Model Trained at Step 4680: loss [1.8778077465616778]\n",
            "Model Trained at Step 4700: loss [1.9275445683779775]\n",
            "Model Trained at Step 4720: loss [1.9369707150199826]\n",
            "Model Trained at Step 4740: loss [1.9138653572994826]\n",
            "Model Trained at Step 4760: loss [1.8370061582366533]\n",
            "Model Trained at Step 4780: loss [1.9135208154551773]\n",
            "Model Trained at Step 4800: loss [1.9083524806530339]\n",
            "Model Trained at Step 4820: loss [1.872190916687429]\n",
            "Model Trained at Step 4840: loss [1.8531500832975212]\n",
            "Model Trained at Step 4860: loss [1.9370155277051009]\n",
            "Model Trained at Step 4880: loss [1.9533768415938937]\n",
            "Model Trained at Step 4900: loss [1.949264312305362]\n",
            "Model Trained at Step 4920: loss [1.9455513165694556]\n",
            "Model Trained at Step 4940: loss [1.9373233926148419]\n",
            "Model Trained at Step 4960: loss [1.9374982822821387]\n",
            "Model Trained at Step 4980: loss [1.8875570318777826]\n",
            "Model Trained at Step 5000: loss [1.8884710251167451]\n",
            "Simulation ev training 8 done\n",
            "Model Trained at Step 20: loss [1.6866912912949092]\n",
            "Model Trained at Step 40: loss [1.619965017321055]\n",
            "Model Trained at Step 60: loss [1.5955723600729899]\n",
            "Model Trained at Step 80: loss [1.5971300018619659]\n",
            "Model Trained at Step 100: loss [1.5707880107738037]\n",
            "Model Trained at Step 120: loss [1.5691480439137364]\n",
            "Model Trained at Step 140: loss [1.5602272598485905]\n",
            "Model Trained at Step 160: loss [1.5452250306554869]\n",
            "Model Trained at Step 180: loss [1.532504808904805]\n",
            "Model Trained at Step 200: loss [1.5243090806658401]\n",
            "Model Trained at Step 220: loss [1.5240917344134186]\n",
            "Model Trained at Step 240: loss [1.5218539262234967]\n",
            "Model Trained at Step 260: loss [1.5210566119955735]\n",
            "Model Trained at Step 280: loss [1.5208106830195998]\n",
            "Model Trained at Step 300: loss [1.5203127816574882]\n",
            "Model Trained at Step 320: loss [1.5258361759906047]\n",
            "Model Trained at Step 340: loss [1.5246877669892176]\n",
            "Model Trained at Step 360: loss [1.5173622309779091]\n",
            "Model Trained at Step 380: loss [1.5092659546654585]\n",
            "Model Trained at Step 400: loss [1.5012381716146883]\n",
            "Model Trained at Step 420: loss [1.5052433737680448]\n",
            "Model Trained at Step 440: loss [1.5183438938418135]\n",
            "Model Trained at Step 460: loss [1.5301565982914198]\n",
            "Model Trained at Step 480: loss [1.5421690388926959]\n",
            "Model Trained at Step 500: loss [1.5555703917469383]\n",
            "Model Trained at Step 520: loss [1.5694398148988737]\n",
            "Model Trained at Step 540: loss [1.5857220027194974]\n",
            "Model Trained at Step 560: loss [1.5897274368328793]\n",
            "Model Trained at Step 580: loss [1.5920783044046005]\n",
            "Model Trained at Step 600: loss [1.5952045562707986]\n",
            "Model Trained at Step 620: loss [1.6086625945210353]\n",
            "Model Trained at Step 640: loss [1.6119156809236703]\n",
            "Model Trained at Step 660: loss [1.6099806987775982]\n",
            "Model Trained at Step 680: loss [1.6223066061042295]\n",
            "Model Trained at Step 700: loss [1.642910261611603]\n",
            "Model Trained at Step 720: loss [1.6573932235523607]\n",
            "Model Trained at Step 740: loss [1.6757414860387805]\n",
            "Model Trained at Step 760: loss [1.6953781721787156]\n",
            "Model Trained at Step 780: loss [1.719396184136361]\n",
            "Model Trained at Step 800: loss [1.745605414116125]\n",
            "Model Trained at Step 820: loss [1.76977808781946]\n",
            "Model Trained at Step 840: loss [1.7885593074142239]\n",
            "Model Trained at Step 860: loss [1.8063533172728543]\n",
            "Model Trained at Step 880: loss [1.82189131716712]\n",
            "Model Trained at Step 900: loss [1.8354725375917567]\n",
            "Model Trained at Step 920: loss [1.8513733655439537]\n",
            "Model Trained at Step 940: loss [1.8626455934032176]\n",
            "Model Trained at Step 960: loss [1.8748212237144277]\n",
            "Model Trained at Step 980: loss [1.884565812464783]\n",
            "Model Trained at Step 1000: loss [1.8913937912022392]\n",
            "Model Trained at Step 1020: loss [1.9026315320998002]\n",
            "Model Trained at Step 1040: loss [1.9145123762497138]\n",
            "Model Trained at Step 1060: loss [1.9353329258182441]\n",
            "Model Trained at Step 1080: loss [1.9457681810296417]\n",
            "Model Trained at Step 1100: loss [1.9636845237852032]\n",
            "Model Trained at Step 1120: loss [1.982665810420475]\n",
            "Model Trained at Step 1140: loss [1.97648390216495]\n",
            "Model Trained at Step 1160: loss [1.9664087203835465]\n",
            "Model Trained at Step 1180: loss [1.9495733586043695]\n",
            "Model Trained at Step 1200: loss [1.93972779617904]\n",
            "Model Trained at Step 1220: loss [1.931215344261346]\n",
            "Model Trained at Step 1240: loss [1.9201872639223503]\n",
            "Model Trained at Step 1260: loss [1.9110443279060398]\n",
            "Model Trained at Step 1280: loss [1.904378265456837]\n",
            "Model Trained at Step 1300: loss [1.9012734924346195]\n",
            "Model Trained at Step 1320: loss [1.8972871028497533]\n",
            "Model Trained at Step 1340: loss [1.8945859986696862]\n",
            "Model Trained at Step 1360: loss [1.8875916834049682]\n",
            "Model Trained at Step 1380: loss [1.8816677338347403]\n",
            "Model Trained at Step 1400: loss [1.874628309302937]\n",
            "Model Trained at Step 1420: loss [1.8679637923184447]\n",
            "Model Trained at Step 1440: loss [1.862927857449188]\n",
            "Model Trained at Step 1460: loss [1.8541338349227927]\n",
            "Model Trained at Step 1480: loss [1.8449005927073432]\n",
            "Model Trained at Step 1500: loss [1.835592931827927]\n",
            "Model Trained at Step 1520: loss [1.8273897458127866]\n",
            "Model Trained at Step 1540: loss [1.8193809887741572]\n",
            "Model Trained at Step 1560: loss [1.8141894324337748]\n",
            "Model Trained at Step 1580: loss [1.810615529182574]\n",
            "Model Trained at Step 1600: loss [1.8090969802669925]\n",
            "Model Trained at Step 1620: loss [1.8066665920388658]\n",
            "Model Trained at Step 1640: loss [1.795106726639029]\n",
            "Model Trained at Step 1660: loss [1.8193548001277005]\n",
            "Model Trained at Step 1680: loss [1.9175825299874574]\n",
            "Model Trained at Step 1700: loss [1.9411158560508142]\n",
            "Model Trained at Step 1720: loss [1.9529041440598405]\n",
            "Model Trained at Step 1740: loss [1.9658496145428384]\n",
            "Model Trained at Step 1760: loss [1.9826456561380668]\n",
            "Model Trained at Step 1780: loss [1.997684485292774]\n",
            "Model Trained at Step 1800: loss [2.0153129647540324]\n",
            "Model Trained at Step 1820: loss [2.0331325532579387]\n",
            "Model Trained at Step 1840: loss [2.049292999468244]\n",
            "Model Trained at Step 1860: loss [2.0614228966569206]\n",
            "Model Trained at Step 1880: loss [2.0733595858615845]\n",
            "Model Trained at Step 1900: loss [2.083193389724042]\n",
            "Model Trained at Step 1920: loss [2.0909695152029175]\n",
            "Model Trained at Step 1940: loss [2.098489523773877]\n",
            "Model Trained at Step 1960: loss [2.104196992496701]\n",
            "Model Trained at Step 1980: loss [2.103621536439497]\n",
            "Model Trained at Step 2000: loss [2.1060235924787345]\n",
            "Model Trained at Step 2020: loss [2.110644054694275]\n",
            "Model Trained at Step 2040: loss [2.1079708494495035]\n",
            "Model Trained at Step 2060: loss [2.1084849762995725]\n",
            "Model Trained at Step 2080: loss [2.1080982582194743]\n",
            "Model Trained at Step 2100: loss [2.112269201149055]\n",
            "Model Trained at Step 2120: loss [2.107542669673517]\n",
            "Model Trained at Step 2140: loss [2.104272972281889]\n",
            "Model Trained at Step 2160: loss [2.1038572899983627]\n",
            "Model Trained at Step 2180: loss [2.1055016956116277]\n",
            "Model Trained at Step 2200: loss [2.112736477083826]\n",
            "Model Trained at Step 2220: loss [2.115792611038814]\n",
            "Model Trained at Step 2240: loss [2.114805744967211]\n",
            "Model Trained at Step 2260: loss [2.114995530993992]\n",
            "Model Trained at Step 2280: loss [2.1157505918437867]\n",
            "Model Trained at Step 2300: loss [2.1180005305316767]\n",
            "Model Trained at Step 2320: loss [2.120028872301743]\n",
            "Model Trained at Step 2340: loss [2.117909531134802]\n",
            "Model Trained at Step 2360: loss [2.10670312550497]\n",
            "Model Trained at Step 2380: loss [2.091435795325932]\n",
            "Model Trained at Step 2400: loss [2.0573622424900053]\n",
            "Model Trained at Step 2420: loss [2.0152531143125367]\n",
            "Model Trained at Step 2440: loss [2.016688215514309]\n",
            "Model Trained at Step 2460: loss [2.1510148549870944]\n",
            "Model Trained at Step 2480: loss [2.072760438370025]\n",
            "Model Trained at Step 2500: loss [2.1987353059327597]\n",
            "Model Trained at Step 2520: loss [2.258095986356343]\n",
            "Model Trained at Step 2540: loss [2.2706116052434804]\n",
            "Model Trained at Step 2560: loss [2.282071813234306]\n",
            "Model Trained at Step 2580: loss [2.2925573791009826]\n",
            "Model Trained at Step 2600: loss [2.3084552650741257]\n",
            "Model Trained at Step 2620: loss [2.3167206074571487]\n",
            "Model Trained at Step 2640: loss [2.3251947854718975]\n",
            "Model Trained at Step 2660: loss [2.331785007825267]\n",
            "Model Trained at Step 2680: loss [2.3365330061910887]\n",
            "Model Trained at Step 2700: loss [2.3386493501353534]\n",
            "Model Trained at Step 2720: loss [2.345255708988673]\n",
            "Model Trained at Step 2740: loss [2.3527258688867945]\n",
            "Model Trained at Step 2760: loss [2.3586764497732444]\n",
            "Model Trained at Step 2780: loss [2.3622221337863016]\n",
            "Model Trained at Step 2800: loss [2.3662342948182835]\n",
            "Model Trained at Step 2820: loss [2.371649366596225]\n",
            "Model Trained at Step 2840: loss [2.3715240780196947]\n",
            "Model Trained at Step 2860: loss [2.3610913224929786]\n",
            "Model Trained at Step 2880: loss [2.332400597799032]\n",
            "Model Trained at Step 2900: loss [2.2587764978383986]\n",
            "Model Trained at Step 2920: loss [2.243056098724529]\n",
            "Model Trained at Step 2940: loss [2.374912952765221]\n",
            "Model Trained at Step 2960: loss [2.524033594342216]\n",
            "Model Trained at Step 2980: loss [2.5426025314407226]\n",
            "Model Trained at Step 3000: loss [2.5596565218562213]\n",
            "Model Trained at Step 3020: loss [2.569943427650511]\n",
            "Model Trained at Step 3040: loss [2.5796993473807652]\n",
            "Model Trained at Step 3060: loss [2.5909381662457296]\n",
            "Model Trained at Step 3080: loss [2.5919784545275064]\n",
            "Model Trained at Step 3100: loss [2.5956853366912593]\n",
            "Model Trained at Step 3120: loss [2.602697060976576]\n",
            "Model Trained at Step 3140: loss [2.592097531617898]\n",
            "Model Trained at Step 3160: loss [2.580248050568303]\n",
            "Model Trained at Step 3180: loss [2.566480231108558]\n",
            "Model Trained at Step 3200: loss [2.549703996792617]\n",
            "Model Trained at Step 3220: loss [2.521412657086266]\n",
            "Model Trained at Step 3240: loss [2.4878110414536265]\n",
            "Model Trained at Step 3260: loss [2.442792990740665]\n",
            "Model Trained at Step 3280: loss [2.3572495482619615]\n",
            "Model Trained at Step 3300: loss [2.22380156206194]\n",
            "Model Trained at Step 3320: loss [2.3460108753689055]\n",
            "Model Trained at Step 3340: loss [2.4341858275922013]\n",
            "Model Trained at Step 3360: loss [2.3859492808062854]\n",
            "Model Trained at Step 3380: loss [2.2806873019278666]\n",
            "Model Trained at Step 3400: loss [2.197938616706304]\n",
            "Model Trained at Step 3420: loss [2.27595321376355]\n",
            "Model Trained at Step 3440: loss [2.272133551822221]\n",
            "Model Trained at Step 3460: loss [2.1562951787338167]\n",
            "Model Trained at Step 3480: loss [2.145032066643684]\n",
            "Model Trained at Step 3500: loss [2.2149248205540135]\n",
            "Model Trained at Step 3520: loss [2.1123617433144015]\n",
            "Model Trained at Step 3540: loss [2.0479656249000264]\n",
            "Model Trained at Step 3560: loss [2.1382536579022293]\n",
            "Model Trained at Step 3580: loss [2.2264701855959586]\n",
            "Model Trained at Step 3600: loss [2.1687900510320963]\n",
            "Model Trained at Step 3620: loss [2.0264029909880583]\n",
            "Model Trained at Step 3640: loss [1.9791049600693131]\n",
            "Model Trained at Step 3660: loss [2.100454900439713]\n",
            "Model Trained at Step 3680: loss [1.9670464670147414]\n",
            "Model Trained at Step 3700: loss [2.0615580783936758]\n",
            "Model Trained at Step 3720: loss [1.9418907201326778]\n",
            "Model Trained at Step 3740: loss [2.0245409237542376]\n",
            "Model Trained at Step 3760: loss [1.9280043173986965]\n",
            "Model Trained at Step 3780: loss [1.9003305531578065]\n",
            "Model Trained at Step 3800: loss [1.8639505936527474]\n",
            "Model Trained at Step 3820: loss [1.8557022774042864]\n",
            "Model Trained at Step 3840: loss [1.8543408132906216]\n",
            "Model Trained at Step 3860: loss [1.830124476553189]\n",
            "Model Trained at Step 3880: loss [1.820878728170372]\n",
            "Model Trained at Step 3900: loss [1.8157051042359602]\n",
            "Model Trained at Step 3920: loss [1.818017126730512]\n",
            "Model Trained at Step 3940: loss [1.8086799023974898]\n",
            "Model Trained at Step 3960: loss [1.8100404646760455]\n",
            "Model Trained at Step 3980: loss [1.8103630406865943]\n",
            "Model Trained at Step 4000: loss [1.8151363612894649]\n",
            "Model Trained at Step 4020: loss [1.817074719552346]\n",
            "Model Trained at Step 4040: loss [1.8204328654134414]\n",
            "Model Trained at Step 4060: loss [1.833422836906093]\n",
            "Model Trained at Step 4080: loss [1.8388710110708075]\n",
            "Model Trained at Step 4100: loss [1.8444249019732362]\n",
            "Model Trained at Step 4120: loss [1.8610457692999645]\n",
            "Model Trained at Step 4140: loss [1.8757085833297424]\n",
            "Model Trained at Step 4160: loss [1.8898945004933498]\n",
            "Model Trained at Step 4180: loss [1.9057404034850123]\n",
            "Model Trained at Step 4200: loss [1.9203120670542908]\n",
            "Model Trained at Step 4220: loss [1.9343100986767574]\n",
            "Model Trained at Step 4240: loss [1.9407232951887916]\n",
            "Model Trained at Step 4260: loss [1.9492701144003948]\n",
            "Model Trained at Step 4280: loss [1.966013028770409]\n",
            "Model Trained at Step 4300: loss [1.981199757520369]\n",
            "Model Trained at Step 4320: loss [1.9978158312186394]\n",
            "Model Trained at Step 4340: loss [2.004573355410312]\n",
            "Model Trained at Step 4360: loss [2.0095224167060324]\n",
            "Model Trained at Step 4380: loss [2.014638884744916]\n",
            "Model Trained at Step 4400: loss [2.0328568457390572]\n",
            "Model Trained at Step 4420: loss [2.0475053410391633]\n",
            "Model Trained at Step 4440: loss [2.0591061549931697]\n",
            "Model Trained at Step 4460: loss [2.07542984324328]\n",
            "Model Trained at Step 4480: loss [2.09110670409418]\n",
            "Model Trained at Step 4500: loss [2.1032749130106154]\n",
            "Model Trained at Step 4520: loss [2.1220116920318164]\n",
            "Model Trained at Step 4540: loss [2.135594563281278]\n",
            "Model Trained at Step 4560: loss [2.13758855379439]\n",
            "Model Trained at Step 4580: loss [2.1420464953824734]\n",
            "Model Trained at Step 4600: loss [2.1500806262477794]\n",
            "Model Trained at Step 4620: loss [2.1616910431674285]\n",
            "Model Trained at Step 4640: loss [2.1772258034784064]\n",
            "Model Trained at Step 4660: loss [2.190971273721696]\n",
            "Model Trained at Step 4680: loss [2.202799198354691]\n",
            "Model Trained at Step 4700: loss [2.304375783978881]\n",
            "Model Trained at Step 4720: loss [2.3085803302177506]\n",
            "Model Trained at Step 4740: loss [2.3257640938786137]\n",
            "Model Trained at Step 4760: loss [2.437588080290017]\n",
            "Model Trained at Step 4780: loss [2.438830970133761]\n",
            "Model Trained at Step 4800: loss [2.4183769929929104]\n",
            "Model Trained at Step 4820: loss [2.468407575864213]\n",
            "Model Trained at Step 4840: loss [2.366459766382944]\n",
            "Model Trained at Step 4860: loss [2.5753835521939674]\n",
            "Model Trained at Step 4880: loss [2.4676266271489866]\n",
            "Model Trained at Step 4900: loss [2.702894633105821]\n",
            "Model Trained at Step 4920: loss [2.6301289939815327]\n",
            "Model Trained at Step 4940: loss [2.621597679327656]\n",
            "Model Trained at Step 4960: loss [2.6753567301795798]\n",
            "Model Trained at Step 4980: loss [2.809701495431822]\n",
            "Model Trained at Step 5000: loss [2.6203846797369903]\n",
            "Simulation ev training 9 done\n",
            "Model Trained at Step 20: loss [2.0960552496345115]\n",
            "Model Trained at Step 40: loss [1.866811164660073]\n",
            "Model Trained at Step 60: loss [1.8445615460698164]\n",
            "Model Trained at Step 80: loss [1.8624600262191815]\n",
            "Model Trained at Step 100: loss [1.8535908855831729]\n",
            "Model Trained at Step 120: loss [1.8467358691361078]\n",
            "Model Trained at Step 140: loss [1.8517312605292169]\n",
            "Model Trained at Step 160: loss [1.8417715707658586]\n",
            "Model Trained at Step 180: loss [1.8392116534924376]\n",
            "Model Trained at Step 200: loss [1.8653909473158827]\n",
            "Model Trained at Step 220: loss [1.8624578217274663]\n",
            "Model Trained at Step 240: loss [1.8609881839174132]\n",
            "Model Trained at Step 260: loss [1.8683265885329934]\n",
            "Model Trained at Step 280: loss [1.872404498816287]\n",
            "Model Trained at Step 300: loss [1.8782996721197691]\n",
            "Model Trained at Step 320: loss [1.8815504764216127]\n",
            "Model Trained at Step 340: loss [1.884420644486537]\n",
            "Model Trained at Step 360: loss [1.8928910154793488]\n",
            "Model Trained at Step 380: loss [1.9033342176906807]\n",
            "Model Trained at Step 400: loss [1.911533391468328]\n",
            "Model Trained at Step 420: loss [1.9244145674143573]\n",
            "Model Trained at Step 440: loss [1.941654156262628]\n",
            "Model Trained at Step 460: loss [1.9579963229148922]\n",
            "Model Trained at Step 480: loss [1.977035144320147]\n",
            "Model Trained at Step 500: loss [1.9978801810191595]\n",
            "Model Trained at Step 520: loss [2.020281009811855]\n",
            "Model Trained at Step 540: loss [2.0437978637644645]\n",
            "Model Trained at Step 560: loss [2.074718933342385]\n",
            "Model Trained at Step 580: loss [2.1017510228310083]\n",
            "Model Trained at Step 600: loss [2.1300241159198037]\n",
            "Model Trained at Step 620: loss [2.1626918696945383]\n",
            "Model Trained at Step 640: loss [2.206005007305469]\n",
            "Model Trained at Step 660: loss [2.244564525785221]\n",
            "Model Trained at Step 680: loss [2.2798958454652323]\n",
            "Model Trained at Step 700: loss [2.3227149377915772]\n",
            "Model Trained at Step 720: loss [2.3722039261918524]\n",
            "Model Trained at Step 740: loss [2.417533525814332]\n",
            "Model Trained at Step 760: loss [2.4613267224043565]\n",
            "Model Trained at Step 780: loss [2.4974675395575665]\n",
            "Model Trained at Step 800: loss [2.523061572114435]\n",
            "Model Trained at Step 820: loss [2.631532356279206]\n",
            "Model Trained at Step 840: loss [2.698355704771656]\n",
            "Model Trained at Step 860: loss [2.7003235135652046]\n",
            "Model Trained at Step 880: loss [2.7345216388990234]\n",
            "Model Trained at Step 900: loss [2.861116586719684]\n",
            "Model Trained at Step 920: loss [2.9034300417960903]\n",
            "Model Trained at Step 940: loss [2.930696476717127]\n",
            "Model Trained at Step 960: loss [2.9392274320111307]\n",
            "Model Trained at Step 980: loss [2.941432139809585]\n",
            "Model Trained at Step 1000: loss [2.9401008921055065]\n",
            "Model Trained at Step 1020: loss [2.9485165350812723]\n",
            "Model Trained at Step 1040: loss [2.912050547091305]\n",
            "Model Trained at Step 1060: loss [2.889170211857233]\n",
            "Model Trained at Step 1080: loss [2.95383626393538]\n",
            "Model Trained at Step 1100: loss [2.9547706253646804]\n",
            "Model Trained at Step 1120: loss [2.926256010450655]\n",
            "Model Trained at Step 1140: loss [2.893470789865183]\n",
            "Model Trained at Step 1160: loss [2.8607110119409507]\n",
            "Model Trained at Step 1180: loss [2.83189789933311]\n",
            "Model Trained at Step 1200: loss [2.80214226195763]\n",
            "Model Trained at Step 1220: loss [2.7745212920737936]\n",
            "Model Trained at Step 1240: loss [2.7466900631850564]\n",
            "Model Trained at Step 1260: loss [2.7199234830590706]\n",
            "Model Trained at Step 1280: loss [2.6927913545647457]\n",
            "Model Trained at Step 1300: loss [2.665577040959568]\n",
            "Model Trained at Step 1320: loss [2.6361072604099998]\n",
            "Model Trained at Step 1340: loss [2.6088546574060563]\n",
            "Model Trained at Step 1360: loss [2.58510737243044]\n",
            "Model Trained at Step 1380: loss [2.558412145174891]\n",
            "Model Trained at Step 1400: loss [2.5337391298178895]\n",
            "Model Trained at Step 1420: loss [2.5078539314555925]\n",
            "Model Trained at Step 1440: loss [2.4853853409346995]\n",
            "Model Trained at Step 1460: loss [2.4679984957919876]\n",
            "Model Trained at Step 1480: loss [2.4503662706940275]\n",
            "Model Trained at Step 1500: loss [2.4324882328004533]\n",
            "Model Trained at Step 1520: loss [2.4164367637553337]\n",
            "Model Trained at Step 1540: loss [2.4033770445844977]\n",
            "Model Trained at Step 1560: loss [2.391585898171937]\n",
            "Model Trained at Step 1580: loss [2.3858354832267787]\n",
            "Model Trained at Step 1600: loss [2.381954507674258]\n",
            "Model Trained at Step 1620: loss [2.3779817363999474]\n",
            "Model Trained at Step 1640: loss [2.3703192390457692]\n",
            "Model Trained at Step 1660: loss [2.366014940217409]\n",
            "Model Trained at Step 1680: loss [2.347432142380425]\n",
            "Model Trained at Step 1700: loss [2.2981720304440154]\n",
            "Model Trained at Step 1720: loss [2.24096339745157]\n",
            "Model Trained at Step 1740: loss [2.1895064188895526]\n",
            "Model Trained at Step 1760: loss [2.1649178379220024]\n",
            "Model Trained at Step 1780: loss [2.240598663705793]\n",
            "Model Trained at Step 1800: loss [2.3742514462643207]\n",
            "Model Trained at Step 1820: loss [2.182223126638776]\n",
            "Model Trained at Step 1840: loss [2.303417549801984]\n",
            "Model Trained at Step 1860: loss [2.2393349404536287]\n",
            "Model Trained at Step 1880: loss [2.3589385284003255]\n",
            "Model Trained at Step 1900: loss [2.399975808912371]\n",
            "Model Trained at Step 1920: loss [2.254552473976122]\n",
            "Model Trained at Step 1940: loss [2.410002347795378]\n",
            "Model Trained at Step 1960: loss [2.321087524806358]\n",
            "Model Trained at Step 1980: loss [2.0133939651679875]\n",
            "Model Trained at Step 2000: loss [2.4591910112946227]\n",
            "Model Trained at Step 2020: loss [2.159172340492339]\n",
            "Model Trained at Step 2040: loss [2.2753081040201613]\n",
            "Model Trained at Step 2060: loss [2.1107302306810225]\n",
            "Model Trained at Step 2080: loss [2.2963063767068457]\n",
            "Model Trained at Step 2100: loss [2.0532699465500066]\n",
            "Model Trained at Step 2120: loss [2.236196630076664]\n",
            "Model Trained at Step 2140: loss [2.215287017184834]\n",
            "Model Trained at Step 2160: loss [2.1928377356897855]\n",
            "Model Trained at Step 2180: loss [2.2206381139775457]\n",
            "Model Trained at Step 2200: loss [2.1903544145044838]\n",
            "Model Trained at Step 2220: loss [2.2942966969873693]\n",
            "Model Trained at Step 2240: loss [2.1967332481038273]\n",
            "Model Trained at Step 2260: loss [2.2671901285851517]\n",
            "Model Trained at Step 2280: loss [2.148062940484173]\n",
            "Model Trained at Step 2300: loss [2.3393996639298766]\n",
            "Model Trained at Step 2320: loss [2.3048802243116002]\n",
            "Model Trained at Step 2340: loss [2.193214705049926]\n",
            "Model Trained at Step 2360: loss [2.372449197763163]\n",
            "Model Trained at Step 2380: loss [2.3910162243700226]\n",
            "Model Trained at Step 2400: loss [2.354898877365532]\n",
            "Model Trained at Step 2420: loss [2.4692690782778293]\n",
            "Model Trained at Step 2440: loss [2.34736775309383]\n",
            "Model Trained at Step 2460: loss [2.28800278090164]\n",
            "Model Trained at Step 2480: loss [2.3573211439781216]\n",
            "Model Trained at Step 2500: loss [2.525652367095432]\n",
            "Model Trained at Step 2520: loss [2.4630606621801054]\n",
            "Model Trained at Step 2540: loss [2.539192665434695]\n",
            "Model Trained at Step 2560: loss [2.6066566045679074]\n",
            "Model Trained at Step 2580: loss [2.528485328303234]\n",
            "Model Trained at Step 2600: loss [2.6347157256438942]\n",
            "Model Trained at Step 2620: loss [2.4094285190979114]\n",
            "Model Trained at Step 2640: loss [2.553844484624217]\n",
            "Model Trained at Step 2660: loss [2.513923015271801]\n",
            "Model Trained at Step 2680: loss [2.4539190517866443]\n",
            "Model Trained at Step 2700: loss [2.4981175684307484]\n",
            "Model Trained at Step 2720: loss [2.5753711167403788]\n",
            "Model Trained at Step 2740: loss [2.429643735695059]\n",
            "Model Trained at Step 2760: loss [2.45957987642682]\n",
            "Model Trained at Step 2780: loss [2.4563787615862536]\n",
            "Model Trained at Step 2800: loss [2.3325866900835726]\n",
            "Model Trained at Step 2820: loss [2.4798019708267454]\n",
            "Model Trained at Step 2840: loss [2.3810271289934577]\n",
            "Model Trained at Step 2860: loss [2.2726214613474345]\n",
            "Model Trained at Step 2880: loss [2.421213964089042]\n",
            "Model Trained at Step 2900: loss [2.3549370802565113]\n",
            "Model Trained at Step 2920: loss [2.3115992323815133]\n",
            "Model Trained at Step 2940: loss [2.2209272021711297]\n",
            "Model Trained at Step 2960: loss [2.343305705139298]\n",
            "Model Trained at Step 2980: loss [2.189820686251605]\n",
            "Model Trained at Step 3000: loss [2.150437782681058]\n",
            "Model Trained at Step 3020: loss [2.231942770291431]\n",
            "Model Trained at Step 3040: loss [2.182058727416192]\n",
            "Model Trained at Step 3060: loss [2.1988954429959002]\n",
            "Model Trained at Step 3080: loss [2.1885456235176295]\n",
            "Model Trained at Step 3100: loss [2.183463159079663]\n",
            "Model Trained at Step 3120: loss [2.2198097820126925]\n",
            "Model Trained at Step 3140: loss [2.04706513063197]\n",
            "Model Trained at Step 3160: loss [1.954174850986751]\n",
            "Model Trained at Step 3180: loss [2.088724601748347]\n",
            "Model Trained at Step 3200: loss [2.1405902321612236]\n",
            "Model Trained at Step 3220: loss [2.0829670262705284]\n",
            "Model Trained at Step 3240: loss [2.1228526578641365]\n",
            "Model Trained at Step 3260: loss [2.0224013234184737]\n",
            "Model Trained at Step 3280: loss [1.9674367322086899]\n",
            "Model Trained at Step 3300: loss [2.0326653640426517]\n",
            "Model Trained at Step 3320: loss [2.044847829332439]\n",
            "Model Trained at Step 3340: loss [2.06228981122128]\n",
            "Model Trained at Step 3360: loss [2.088472252793286]\n",
            "Model Trained at Step 3380: loss [2.030277682395866]\n",
            "Model Trained at Step 3400: loss [2.0059649527352486]\n",
            "Model Trained at Step 3420: loss [2.0037781791338696]\n",
            "Model Trained at Step 3440: loss [1.996568822028027]\n",
            "Model Trained at Step 3460: loss [1.9754802827148006]\n",
            "Model Trained at Step 3480: loss [2.0206266178666836]\n",
            "Model Trained at Step 3500: loss [1.9775703824595543]\n",
            "Model Trained at Step 3520: loss [2.0191460884432195]\n",
            "Model Trained at Step 3540: loss [2.0184954720374755]\n",
            "Model Trained at Step 3560: loss [1.9567143849381676]\n",
            "Model Trained at Step 3580: loss [2.0343450825868197]\n",
            "Model Trained at Step 3600: loss [1.9864570791065224]\n",
            "Model Trained at Step 3620: loss [2.0482228140602703]\n",
            "Model Trained at Step 3640: loss [2.005797632528292]\n",
            "Model Trained at Step 3660: loss [2.170717135940902]\n",
            "Model Trained at Step 3680: loss [2.079412944997476]\n",
            "Model Trained at Step 3700: loss [2.1731701495599802]\n",
            "Model Trained at Step 3720: loss [2.1485225224221454]\n",
            "Model Trained at Step 3740: loss [2.1736599725639043]\n",
            "Model Trained at Step 3760: loss [2.1860882282564864]\n",
            "Model Trained at Step 3780: loss [2.160612365739637]\n",
            "Model Trained at Step 3800: loss [2.284371735550814]\n",
            "Model Trained at Step 3820: loss [2.153374390817894]\n",
            "Model Trained at Step 3840: loss [2.1425580930436916]\n",
            "Model Trained at Step 3860: loss [2.2343161387036288]\n",
            "Model Trained at Step 3880: loss [2.321233989578981]\n",
            "Model Trained at Step 3900: loss [2.2629050037333256]\n",
            "Model Trained at Step 3920: loss [2.2664051431998247]\n",
            "Model Trained at Step 3940: loss [2.262947784084084]\n",
            "Model Trained at Step 3960: loss [2.271463006620801]\n",
            "Model Trained at Step 3980: loss [2.296599416755124]\n",
            "Model Trained at Step 4000: loss [2.398734919751999]\n",
            "Model Trained at Step 4020: loss [2.333611539558019]\n",
            "Model Trained at Step 4040: loss [2.3057194688724985]\n",
            "Model Trained at Step 4060: loss [2.362988334717193]\n",
            "Model Trained at Step 4080: loss [2.4314239796809147]\n",
            "Model Trained at Step 4100: loss [2.2942685561841265]\n",
            "Model Trained at Step 4120: loss [2.4806648764898336]\n",
            "Model Trained at Step 4140: loss [2.3299475118722457]\n",
            "Model Trained at Step 4160: loss [2.4432971394942298]\n",
            "Model Trained at Step 4180: loss [2.522805719992342]\n",
            "Model Trained at Step 4200: loss [2.4577788325086196]\n",
            "Model Trained at Step 4220: loss [2.3962323389611555]\n",
            "Model Trained at Step 4240: loss [2.478853552048779]\n",
            "Model Trained at Step 4260: loss [2.406548777615348]\n",
            "Model Trained at Step 4280: loss [2.398113279982256]\n",
            "Model Trained at Step 4300: loss [2.366041336935919]\n",
            "Model Trained at Step 4320: loss [2.528059757146715]\n",
            "Model Trained at Step 4340: loss [2.374084872258206]\n",
            "Model Trained at Step 4360: loss [2.439974001644736]\n",
            "Model Trained at Step 4380: loss [2.4315395385177516]\n",
            "Model Trained at Step 4400: loss [2.4033334386211296]\n",
            "Model Trained at Step 4420: loss [2.3915631555746435]\n",
            "Model Trained at Step 4440: loss [2.4347618133189064]\n",
            "Model Trained at Step 4460: loss [2.362805956932436]\n",
            "Model Trained at Step 4480: loss [2.3372553224412407]\n",
            "Model Trained at Step 4500: loss [2.406140610462548]\n",
            "Model Trained at Step 4520: loss [2.3457896660796096]\n",
            "Model Trained at Step 4540: loss [2.33440573127496]\n",
            "Model Trained at Step 4560: loss [2.3825632583737977]\n",
            "Model Trained at Step 4580: loss [2.329040125762004]\n",
            "Model Trained at Step 4600: loss [2.322607422641279]\n",
            "Model Trained at Step 4620: loss [2.348769661689116]\n",
            "Model Trained at Step 4640: loss [2.283889960390864]\n",
            "Model Trained at Step 4660: loss [2.3680014950515447]\n",
            "Model Trained at Step 4680: loss [2.3076453050199137]\n",
            "Model Trained at Step 4700: loss [2.3394382334662827]\n",
            "Model Trained at Step 4720: loss [2.3538516071690436]\n",
            "Model Trained at Step 4740: loss [2.3169456755383218]\n",
            "Model Trained at Step 4760: loss [2.277751650770459]\n",
            "Model Trained at Step 4780: loss [2.2960005435315827]\n",
            "Model Trained at Step 4800: loss [2.41292078065104]\n",
            "Model Trained at Step 4820: loss [2.3123889562628164]\n",
            "Model Trained at Step 4840: loss [2.3328845845014516]\n",
            "Model Trained at Step 4860: loss [2.320101261397582]\n",
            "Model Trained at Step 4880: loss [2.318142969744468]\n",
            "Model Trained at Step 4900: loss [2.304743775269623]\n",
            "Model Trained at Step 4920: loss [2.324865823833536]\n",
            "Model Trained at Step 4940: loss [2.342953017718292]\n",
            "Model Trained at Step 4960: loss [2.3334149728411147]\n",
            "Model Trained at Step 4980: loss [2.343678480083758]\n",
            "Model Trained at Step 5000: loss [2.35121271500456]\n",
            "Simulation ev training 10 done\n"
          ]
        }
      ]
    }
  ]
}